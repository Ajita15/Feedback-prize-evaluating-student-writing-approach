{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9cfb86e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:27.318514Z",
     "iopub.status.busy": "2022-01-30T17:44:27.317617Z",
     "iopub.status.idle": "2022-01-30T17:44:27.320997Z",
     "shell.execute_reply": "2022-01-30T17:44:27.321446Z",
     "shell.execute_reply.started": "2022-01-30T17:42:22.585880Z"
    },
    "papermill": {
     "duration": 0.034623,
     "end_time": "2022-01-30T17:44:27.321704",
     "exception": false,
     "start_time": "2022-01-30T17:44:27.287081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1f455",
   "metadata": {
    "papermill": {
     "duration": 0.01807,
     "end_time": "2022-01-30T17:44:27.358332",
     "exception": false,
     "start_time": "2022-01-30T17:44:27.340262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Training Notebook**\n",
    "1. Run this notebook - to create a trained model.\n",
    "2. Later used this trained model for inference and create a submission.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5948e64e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:27.399901Z",
     "iopub.status.busy": "2022-01-30T17:44:27.399244Z",
     "iopub.status.idle": "2022-01-30T17:44:29.419622Z",
     "shell.execute_reply": "2022-01-30T17:44:29.419173Z",
     "shell.execute_reply.started": "2022-01-30T17:42:25.765818Z"
    },
    "papermill": {
     "duration": 2.042956,
     "end_time": "2022-01-30T17:44:29.419749",
     "exception": false,
     "start_time": "2022-01-30T17:44:27.376793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15594 entries, 0 to 15593\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         15594 non-null  object\n",
      " 1   text       15594 non-null  object\n",
      " 2   label_str  15594 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 365.6+ KB\n"
     ]
    }
   ],
   "source": [
    "label_df_new=pd.read_csv(\"../input/feedback-nb1/train_file_labels2.csv\")\n",
    "label_df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fed0e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:29.505049Z",
     "iopub.status.busy": "2022-01-30T17:44:29.477535Z",
     "iopub.status.idle": "2022-01-30T17:44:30.579196Z",
     "shell.execute_reply": "2022-01-30T17:44:30.578730Z",
     "shell.execute_reply.started": "2022-01-30T17:42:28.002140Z"
    },
    "papermill": {
     "duration": 1.141782,
     "end_time": "2022-01-30T17:44:30.579321",
     "exception": false,
     "start_time": "2022-01-30T17:44:29.437539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'I-Evidence': 3489544,\n",
       " 'I-Claim': 824107,\n",
       " 'I-Concluding Statement': 814366,\n",
       " 'I-Lead': 474070,\n",
       " 'O': 305458,\n",
       " 'I-Position': 265856,\n",
       " 'I-Counterclaim': 133965,\n",
       " 'I-Rebuttal': 117445,\n",
       " 'B-Claim': 50202,\n",
       " 'B-Evidence': 45702,\n",
       " 'B-Position': 15419,\n",
       " 'B-Concluding Statement': 13505,\n",
       " 'B-Lead': 9305,\n",
       " 'B-Counterclaim': 5817,\n",
       " 'B-Rebuttal': 4337}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find out the number of unique tags and their count\n",
    "from collections import Counter\n",
    "\n",
    "labels_lst = []\n",
    "for label_str in label_df_new[\"label_str\"]:\n",
    "    labels_lst.extend(label_str.strip().split(\",\"))\n",
    "labels_count = Counter(labels_lst)\n",
    "labels_count = {k: v for k, v in sorted(labels_count.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(len(labels_count))\n",
    "labels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5965c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:30.620596Z",
     "iopub.status.busy": "2022-01-30T17:44:30.619845Z",
     "iopub.status.idle": "2022-01-30T17:44:41.389407Z",
     "shell.execute_reply": "2022-01-30T17:44:41.388904Z",
     "shell.execute_reply.started": "2022-01-30T17:42:31.800905Z"
    },
    "papermill": {
     "duration": 10.792047,
     "end_time": "2022-01-30T17:44:41.389543",
     "exception": false,
     "start_time": "2022-01-30T17:44:30.597496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\r\n",
      "Collecting seqeval[gpu]\r\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\r\n",
      "     |████████████████████████████████| 43 kB 247 kB/s            \r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.3.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval[gpu]) (0.23.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.0.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.7.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Building wheels for collected packages: seqeval\r\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=490034c403fccfef3a9badae33ad5c57cd659daf1875fd80f84bb84146e92345\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\r\n",
      "Successfully built seqeval\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers seqeval[gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bc30b",
   "metadata": {
    "papermill": {
     "duration": 0.0219,
     "end_time": "2022-01-30T17:44:41.434024",
     "exception": false,
     "start_time": "2022-01-30T17:44:41.412124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Create the dataset for the transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f014a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:41.492328Z",
     "iopub.status.busy": "2022-01-30T17:44:41.491411Z",
     "iopub.status.idle": "2022-01-30T17:44:48.696692Z",
     "shell.execute_reply": "2022-01-30T17:44:48.696206Z",
     "shell.execute_reply.started": "2022-01-30T17:42:42.610370Z"
    },
    "papermill": {
     "duration": 7.240508,
     "end_time": "2022-01-30T17:44:48.696823",
     "exception": false,
     "start_time": "2022-01-30T17:44:41.456315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc25bec4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:48.786559Z",
     "iopub.status.busy": "2022-01-30T17:44:48.785761Z",
     "iopub.status.idle": "2022-01-30T17:44:48.791424Z",
     "shell.execute_reply": "2022-01-30T17:44:48.790989Z",
     "shell.execute_reply.started": "2022-01-30T17:42:50.459842Z"
    },
    "papermill": {
     "duration": 0.072272,
     "end_time": "2022-01-30T17:44:48.791547",
     "exception": false,
     "start_time": "2022-01-30T17:44:48.719275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0043f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:48.841934Z",
     "iopub.status.busy": "2022-01-30T17:44:48.841246Z",
     "iopub.status.idle": "2022-01-30T17:44:48.843974Z",
     "shell.execute_reply": "2022-01-30T17:44:48.844429Z",
     "shell.execute_reply.started": "2022-01-30T17:42:50.512803Z"
    },
    "papermill": {
     "duration": 0.030573,
     "end_time": "2022-01-30T17:44:48.844557",
     "exception": false,
     "start_time": "2022-01-30T17:44:48.813984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-Evidence': 3489544,\n",
       " 'I-Claim': 824107,\n",
       " 'I-Concluding Statement': 814366,\n",
       " 'I-Lead': 474070,\n",
       " 'O': 305458,\n",
       " 'I-Position': 265856,\n",
       " 'I-Counterclaim': 133965,\n",
       " 'I-Rebuttal': 117445,\n",
       " 'B-Claim': 50202,\n",
       " 'B-Evidence': 45702,\n",
       " 'B-Position': 15419,\n",
       " 'B-Concluding Statement': 13505,\n",
       " 'B-Lead': 9305,\n",
       " 'B-Counterclaim': 5817,\n",
       " 'B-Rebuttal': 4337}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be397d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:48.897302Z",
     "iopub.status.busy": "2022-01-30T17:44:48.896583Z",
     "iopub.status.idle": "2022-01-30T17:44:48.899296Z",
     "shell.execute_reply": "2022-01-30T17:44:48.899712Z",
     "shell.execute_reply.started": "2022-01-30T17:42:50.527450Z"
    },
    "papermill": {
     "duration": 0.032538,
     "end_time": "2022-01-30T17:44:48.899834",
     "exception": false,
     "start_time": "2022-01-30T17:44:48.867296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-Claim',\n",
       " 1: 'I-Claim',\n",
       " 2: 'B-Evidence',\n",
       " 3: 'I-Evidence',\n",
       " 4: 'B-Position',\n",
       " 5: 'I-Position',\n",
       " 6: 'B-Concluding Statement',\n",
       " 7: 'I-Concluding Statement',\n",
       " 8: 'B-Lead',\n",
       " 9: 'I-Lead',\n",
       " 10: 'B-Counterclaim',\n",
       " 11: 'I-Counterclaim',\n",
       " 12: 'B-Rebuttal',\n",
       " 13: 'I-Rebuttal',\n",
       " 14: 'O'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint as pprint\n",
    "label_to_idx = {'B-Claim': 0, 'I-Claim': 1,\n",
    "                'B-Evidence': 2, 'I-Evidence': 3,\n",
    "                'B-Position': 4, 'I-Position': 5,\n",
    "                'B-Concluding Statement': 6, 'I-Concluding Statement': 7,\n",
    "                'B-Lead': 8, 'I-Lead': 9,\n",
    "                'B-Counterclaim': 10, 'I-Counterclaim': 11,\n",
    "                'B-Rebuttal': 12, 'I-Rebuttal': 13,\n",
    "                'O': 14}\n",
    "idx_to_label = {v:k for k,v in label_to_idx.items()}\n",
    "idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649bef2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:48.950774Z",
     "iopub.status.busy": "2022-01-30T17:44:48.950009Z",
     "iopub.status.idle": "2022-01-30T17:44:53.399049Z",
     "shell.execute_reply": "2022-01-30T17:44:53.398575Z",
     "shell.execute_reply.started": "2022-01-30T17:42:51.061275Z"
    },
    "papermill": {
     "duration": 4.476239,
     "end_time": "2022-01-30T17:44:53.399232",
     "exception": false,
     "start_time": "2022-01-30T17:44:48.922993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b26ec6df244262a2898a0334e1a82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d254a7400c143c3abf4a19b80c7eb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524af31a833b498091bd11f072f83dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aff4a52fbb4b5385c77897bc995d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46982622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:53.460758Z",
     "iopub.status.busy": "2022-01-30T17:44:53.460035Z",
     "iopub.status.idle": "2022-01-30T17:44:53.503363Z",
     "shell.execute_reply": "2022-01-30T17:44:53.503787Z",
     "shell.execute_reply.started": "2022-01-30T17:43:24.359887Z"
    },
    "papermill": {
     "duration": 0.073009,
     "end_time": "2022-01-30T17:44:53.503930",
     "exception": false,
     "start_time": "2022-01-30T17:44:53.430921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert-base-uncased-tokenizer/tokenizer_config.json',\n",
       " './bert-base-uncased-tokenizer/special_tokens_map.json',\n",
       " './bert-base-uncased-tokenizer/vocab.txt',\n",
       " './bert-base-uncased-tokenizer/added_tokens.json',\n",
       " './bert-base-uncased-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./bert-base-uncased-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "481c9784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:53.563701Z",
     "iopub.status.busy": "2022-01-30T17:44:53.562942Z",
     "iopub.status.idle": "2022-01-30T17:44:53.565314Z",
     "shell.execute_reply": "2022-01-30T17:44:53.564872Z",
     "shell.execute_reply.started": "2022-01-25T17:56:03.072923Z"
    },
    "papermill": {
     "duration": 0.03664,
     "end_time": "2022-01-30T17:44:53.565427",
     "exception": false,
     "start_time": "2022-01-30T17:44:53.528787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data.text[index].strip().split()  \n",
    "        word_labels = self.data.label_str[index].split(\",\") \n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                            is_split_into_words=True,\n",
    "#                              is_pretokenized=True, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [label_to_idx[label] for label in word_labels] \n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "          if mapping[0] == 0 and mapping[1] != 0:\n",
    "            # overwrite label\n",
    "            encoded_labels[idx] = labels[i]\n",
    "            i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c5da5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:53.620550Z",
     "iopub.status.busy": "2022-01-30T17:44:53.619965Z",
     "iopub.status.idle": "2022-01-30T17:44:53.655543Z",
     "shell.execute_reply": "2022-01-30T17:44:53.655901Z",
     "shell.execute_reply.started": "2022-01-25T17:56:13.813875Z"
    },
    "papermill": {
     "duration": 0.0657,
     "end_time": "2022-01-30T17:44:53.656026",
     "exception": false,
     "start_time": "2022-01-30T17:44:53.590326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (12475, 3)\n",
      "TEST Dataset: (3119, 3)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = label_df_new.sample(frac=train_size,random_state=200)\n",
    "test_dataset = label_df_new.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(label_df_new.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f01a930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:53.710776Z",
     "iopub.status.busy": "2022-01-30T17:44:53.709258Z",
     "iopub.status.idle": "2022-01-30T17:44:53.711393Z",
     "shell.execute_reply": "2022-01-30T17:44:53.711786Z",
     "shell.execute_reply.started": "2022-01-25T17:56:21.660496Z"
    },
    "papermill": {
     "duration": 0.031131,
     "end_time": "2022-01-30T17:44:53.711909",
     "exception": false,
     "start_time": "2022-01-30T17:44:53.680778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b298ec88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:53.768653Z",
     "iopub.status.busy": "2022-01-30T17:44:53.768067Z",
     "iopub.status.idle": "2022-01-30T17:44:53.771900Z",
     "shell.execute_reply": "2022-01-30T17:44:53.771498Z",
     "shell.execute_reply.started": "2022-01-25T17:56:34.872936Z"
    },
    "papermill": {
     "duration": 0.034981,
     "end_time": "2022-01-30T17:44:53.772014",
     "exception": false,
     "start_time": "2022-01-30T17:44:53.737033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f22cf8a2",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-30T17:44:53.828242Z",
     "iopub.status.busy": "2022-01-30T17:44:53.827408Z",
     "iopub.status.idle": "2022-01-30T17:45:09.668788Z",
     "shell.execute_reply": "2022-01-30T17:45:09.669182Z",
     "shell.execute_reply.started": "2022-01-25T17:56:45.420904Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 15.871595,
     "end_time": "2022-01-30T17:45:09.669337",
     "exception": false,
     "start_time": "2022-01-30T17:44:53.797742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b4f7e0838e46618bb5e9b967306f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_to_idx))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f52f8",
   "metadata": {
    "papermill": {
     "duration": 0.026769,
     "end_time": "2022-01-30T17:45:09.723007",
     "exception": false,
     "start_time": "2022-01-30T17:45:09.696238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bb8341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:45:09.782777Z",
     "iopub.status.busy": "2022-01-30T17:45:09.782065Z",
     "iopub.status.idle": "2022-01-30T17:45:10.608350Z",
     "shell.execute_reply": "2022-01-30T17:45:10.607867Z",
     "shell.execute_reply.started": "2022-01-25T17:58:00.106939Z"
    },
    "papermill": {
     "duration": 0.858744,
     "end_time": "2022-01-30T17:45:10.608479",
     "exception": false,
     "start_time": "2022-01-30T17:45:09.749735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6993, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sanity check\n",
    "inputs = training_set[2]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699d8d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:45:10.668451Z",
     "iopub.status.busy": "2022-01-30T17:45:10.667656Z",
     "iopub.status.idle": "2022-01-30T17:45:10.669954Z",
     "shell.execute_reply": "2022-01-30T17:45:10.669547Z",
     "shell.execute_reply.started": "2022-01-25T17:58:11.693036Z"
    },
    "papermill": {
     "duration": 0.034312,
     "end_time": "2022-01-30T17:45:10.670063",
     "exception": false,
     "start_time": "2022-01-30T17:45:10.635751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0a6827c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:45:10.749568Z",
     "iopub.status.busy": "2022-01-30T17:45:10.741935Z",
     "iopub.status.idle": "2022-01-30T17:45:10.751662Z",
     "shell.execute_reply": "2022-01-30T17:45:10.751249Z",
     "shell.execute_reply.started": "2022-01-25T17:58:36.485969Z"
    },
    "papermill": {
     "duration": 0.054767,
     "end_time": "2022-01-30T17:45:10.751774",
     "exception": false,
     "start_time": "2022-01-30T17:45:10.697007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = output[0]\n",
    "        tr_logits = output[1]\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        # if flattened_target is [1024,] i.e [2*512,] where 2 is batch_size and 512 is seq_length\n",
    "        # and out of 1024 i.e 50 tokens have -100 i.e they don't have a label assigned then\n",
    "        # labels is [974,] and predictions is [974,]\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0cfca69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T17:45:10.811213Z",
     "iopub.status.busy": "2022-01-30T17:45:10.809932Z",
     "iopub.status.idle": "2022-01-31T04:26:11.793265Z",
     "shell.execute_reply": "2022-01-31T04:26:11.790174Z"
    },
    "papermill": {
     "duration": 38461.014592,
     "end_time": "2022-01-31T04:26:11.793428",
     "exception": false,
     "start_time": "2022-01-30T17:45:10.778836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 2.753129005432129\n",
      "Training loss per 100 training steps: 1.578703756969754\n",
      "Training loss per 100 training steps: 1.4377106828476065\n",
      "Training loss per 100 training steps: 1.369864355488077\n",
      "Training loss per 100 training steps: 1.3116734482700985\n",
      "Training loss per 100 training steps: 1.2644969548531872\n",
      "Training loss per 100 training steps: 1.2401785113648844\n",
      "Training loss per 100 training steps: 1.2196291163032302\n",
      "Training loss per 100 training steps: 1.2030350683482547\n",
      "Training loss per 100 training steps: 1.1826867650404622\n",
      "Training loss per 100 training steps: 1.166116451258426\n",
      "Training loss per 100 training steps: 1.147037460547377\n",
      "Training loss per 100 training steps: 1.1289966313120328\n",
      "Training loss per 100 training steps: 1.1108486611370303\n",
      "Training loss per 100 training steps: 1.0966670158758578\n",
      "Training loss per 100 training steps: 1.0878434659916905\n",
      "Training loss per 100 training steps: 1.0786816136193678\n",
      "Training loss per 100 training steps: 1.0667311856846189\n",
      "Training loss per 100 training steps: 1.0589670125707396\n",
      "Training loss per 100 training steps: 1.045969587777678\n",
      "Training loss per 100 training steps: 1.036163620222574\n",
      "Training loss per 100 training steps: 1.0301864723090273\n",
      "Training loss per 100 training steps: 1.022078483849967\n",
      "Training loss per 100 training steps: 1.0139463094331451\n",
      "Training loss per 100 training steps: 1.0090900577142405\n",
      "Training loss per 100 training steps: 1.0027881324029073\n",
      "Training loss per 100 training steps: 1.0001077425108649\n",
      "Training loss per 100 training steps: 0.9932384962772537\n",
      "Training loss per 100 training steps: 0.9888911015942283\n",
      "Training loss per 100 training steps: 0.9848615697686814\n",
      "Training loss per 100 training steps: 0.9798849500921797\n",
      "Training loss per 100 training steps: 0.9742334047197257\n",
      "Training loss per 100 training steps: 0.9714654763018254\n",
      "Training loss per 100 training steps: 0.9670336194397355\n",
      "Training loss per 100 training steps: 0.9630224992031211\n",
      "Training loss per 100 training steps: 0.960173964249308\n",
      "Training loss per 100 training steps: 0.9565586826720857\n",
      "Training loss per 100 training steps: 0.9523880721563843\n",
      "Training loss per 100 training steps: 0.9491218798013462\n",
      "Training loss per 100 training steps: 0.9466142510755097\n",
      "Training loss per 100 training steps: 0.9433539422660493\n",
      "Training loss per 100 training steps: 0.939872537395978\n",
      "Training loss per 100 training steps: 0.9369504218462166\n",
      "Training loss per 100 training steps: 0.9326506361341066\n",
      "Training loss per 100 training steps: 0.929466455971699\n",
      "Training loss per 100 training steps: 0.9266186199719788\n",
      "Training loss per 100 training steps: 0.9238180446186887\n",
      "Training loss per 100 training steps: 0.9220368616113357\n",
      "Training loss per 100 training steps: 0.9177569849426312\n",
      "Training loss per 100 training steps: 0.91564943815793\n",
      "Training loss per 100 training steps: 0.9136565906736666\n",
      "Training loss per 100 training steps: 0.9103967844066936\n",
      "Training loss per 100 training steps: 0.9077063196640798\n",
      "Training loss per 100 training steps: 0.904331026294168\n",
      "Training loss per 100 training steps: 0.9013752510615619\n",
      "Training loss per 100 training steps: 0.8989003052864913\n",
      "Training loss per 100 training steps: 0.8966143777239133\n",
      "Training loss per 100 training steps: 0.8945877272697775\n",
      "Training loss per 100 training steps: 0.8930206192508686\n",
      "Training loss per 100 training steps: 0.8907810142738822\n",
      "Training loss per 100 training steps: 0.8880208203036116\n",
      "Training loss per 100 training steps: 0.886912179640953\n",
      "Training loss per 100 training steps: 0.884752975661146\n",
      "Training loss epoch: 0.884276325843987\n",
      "Training accuracy epoch: 0.7102481406962691\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.4221396744251251\n",
      "Training loss per 100 training steps: 0.7394590628619241\n",
      "Training loss per 100 training steps: 0.705513052086332\n",
      "Training loss per 100 training steps: 0.6942174043568266\n",
      "Training loss per 100 training steps: 0.6906905242629776\n",
      "Training loss per 100 training steps: 0.6951948433460113\n",
      "Training loss per 100 training steps: 0.7066872065852764\n",
      "Training loss per 100 training steps: 0.698009814604713\n",
      "Training loss per 100 training steps: 0.6931127261616913\n",
      "Training loss per 100 training steps: 0.6889564326646986\n",
      "Training loss per 100 training steps: 0.6921021672306242\n",
      "Training loss per 100 training steps: 0.6933492945877667\n",
      "Training loss per 100 training steps: 0.6936216369382944\n",
      "Training loss per 100 training steps: 0.693708677655179\n",
      "Training loss per 100 training steps: 0.698460857424457\n",
      "Training loss per 100 training steps: 0.6947362321186351\n",
      "Training loss per 100 training steps: 0.6943017775885095\n",
      "Training loss per 100 training steps: 0.6953414847733763\n",
      "Training loss per 100 training steps: 0.6961241385329108\n",
      "Training loss per 100 training steps: 0.6970046122236543\n",
      "Training loss per 100 training steps: 0.7013694577175995\n",
      "Training loss per 100 training steps: 0.7017950630511403\n",
      "Training loss per 100 training steps: 0.7008041388565495\n",
      "Training loss per 100 training steps: 0.699443355442285\n",
      "Training loss per 100 training steps: 0.6989597745335931\n",
      "Training loss per 100 training steps: 0.6991618016585023\n",
      "Training loss per 100 training steps: 0.6990706371096912\n",
      "Training loss per 100 training steps: 0.7001514902427346\n",
      "Training loss per 100 training steps: 0.7011893908707681\n",
      "Training loss per 100 training steps: 0.7001169017525387\n",
      "Training loss per 100 training steps: 0.701117423470717\n",
      "Training loss per 100 training steps: 0.700769642674896\n",
      "Training loss per 100 training steps: 0.7003247066517615\n",
      "Training loss per 100 training steps: 0.7000010241724729\n",
      "Training loss per 100 training steps: 0.6992952916174207\n",
      "Training loss per 100 training steps: 0.6989263379337174\n",
      "Training loss per 100 training steps: 0.6988843185715926\n",
      "Training loss per 100 training steps: 0.698323339739222\n",
      "Training loss per 100 training steps: 0.699154462250745\n",
      "Training loss per 100 training steps: 0.6983718171475087\n",
      "Training loss per 100 training steps: 0.6973766650230817\n",
      "Training loss per 100 training steps: 0.6971452760568511\n",
      "Training loss per 100 training steps: 0.6981742469023251\n",
      "Training loss per 100 training steps: 0.6978029571239739\n",
      "Training loss per 100 training steps: 0.6974555146955571\n",
      "Training loss per 100 training steps: 0.698358907739445\n",
      "Training loss per 100 training steps: 0.6981231187432\n",
      "Training loss per 100 training steps: 0.6974529020394753\n",
      "Training loss per 100 training steps: 0.6970577190919758\n",
      "Training loss per 100 training steps: 0.6960565779001657\n",
      "Training loss per 100 training steps: 0.695928970930982\n",
      "Training loss per 100 training steps: 0.6954981008742842\n",
      "Training loss per 100 training steps: 0.6958847077557316\n",
      "Training loss per 100 training steps: 0.6961948225052261\n",
      "Training loss per 100 training steps: 0.6958459113603652\n",
      "Training loss per 100 training steps: 0.6961764972105046\n",
      "Training loss per 100 training steps: 0.6956230271976774\n",
      "Training loss per 100 training steps: 0.6960311616367257\n",
      "Training loss per 100 training steps: 0.6957958560784384\n",
      "Training loss per 100 training steps: 0.6955266464940372\n",
      "Training loss per 100 training steps: 0.695539674633663\n",
      "Training loss per 100 training steps: 0.6958433853600029\n",
      "Training loss per 100 training steps: 0.6957492109191665\n",
      "Training loss epoch: 0.6960320421931987\n",
      "Training accuracy epoch: 0.7623770351388929\n",
      "Training epoch: 3\n",
      "Training loss per 100 training steps: 0.6272588968276978\n",
      "Training loss per 100 training steps: 0.6087654299370133\n",
      "Training loss per 100 training steps: 0.5867355196778454\n",
      "Training loss per 100 training steps: 0.5902021427784251\n",
      "Training loss per 100 training steps: 0.5983976679772808\n",
      "Training loss per 100 training steps: 0.6064129786160654\n",
      "Training loss per 100 training steps: 0.6048641820715589\n",
      "Training loss per 100 training steps: 0.610835110962646\n",
      "Training loss per 100 training steps: 0.6091909258069468\n",
      "Training loss per 100 training steps: 0.6008255384473769\n",
      "Training loss per 100 training steps: 0.6014459245614\n",
      "Training loss per 100 training steps: 0.6065074989908831\n",
      "Training loss per 100 training steps: 0.6026322390453007\n",
      "Training loss per 100 training steps: 0.6029483183393746\n",
      "Training loss per 100 training steps: 0.6070179679696258\n",
      "Training loss per 100 training steps: 0.603481578492348\n",
      "Training loss per 100 training steps: 0.6043650574242228\n",
      "Training loss per 100 training steps: 0.6026724794510531\n",
      "Training loss per 100 training steps: 0.6026754472311637\n",
      "Training loss per 100 training steps: 0.6022422434169327\n",
      "Training loss per 100 training steps: 0.6016541986629881\n",
      "Training loss per 100 training steps: 0.6034298699946701\n",
      "Training loss per 100 training steps: 0.6037237530321167\n",
      "Training loss per 100 training steps: 0.6023931754757041\n",
      "Training loss per 100 training steps: 0.6038774659501072\n",
      "Training loss per 100 training steps: 0.6037072289566763\n",
      "Training loss per 100 training steps: 0.603119881444828\n",
      "Training loss per 100 training steps: 0.6031108537964007\n",
      "Training loss per 100 training steps: 0.6047963300152618\n",
      "Training loss per 100 training steps: 0.6049535215178344\n",
      "Training loss per 100 training steps: 0.6056449867981111\n",
      "Training loss per 100 training steps: 0.6055835932589315\n",
      "Training loss per 100 training steps: 0.6057697168991589\n",
      "Training loss per 100 training steps: 0.6048132855353735\n",
      "Training loss per 100 training steps: 0.6059149612892519\n",
      "Training loss per 100 training steps: 0.6049414495593035\n",
      "Training loss per 100 training steps: 0.6055095953842529\n",
      "Training loss per 100 training steps: 0.6049074885938432\n",
      "Training loss per 100 training steps: 0.6045709190083096\n",
      "Training loss per 100 training steps: 0.6050760198868687\n",
      "Training loss per 100 training steps: 0.6045366850116198\n",
      "Training loss per 100 training steps: 0.6038367013921391\n",
      "Training loss per 100 training steps: 0.6038230535966872\n",
      "Training loss per 100 training steps: 0.6036730046706459\n",
      "Training loss per 100 training steps: 0.6030348107909458\n",
      "Training loss per 100 training steps: 0.6029832980263845\n",
      "Training loss per 100 training steps: 0.603840617790504\n",
      "Training loss per 100 training steps: 0.6041355166846553\n",
      "Training loss per 100 training steps: 0.6049280849546722\n",
      "Training loss per 100 training steps: 0.6058685544667695\n",
      "Training loss per 100 training steps: 0.6058811113622707\n",
      "Training loss per 100 training steps: 0.6058284983059351\n",
      "Training loss per 100 training steps: 0.6053737974200105\n",
      "Training loss per 100 training steps: 0.6058664917580431\n",
      "Training loss per 100 training steps: 0.605715281139853\n",
      "Training loss per 100 training steps: 0.6059518758735101\n",
      "Training loss per 100 training steps: 0.6066263057098625\n",
      "Training loss per 100 training steps: 0.6062542533842101\n",
      "Training loss per 100 training steps: 0.6062059748010417\n",
      "Training loss per 100 training steps: 0.6063875273953372\n",
      "Training loss per 100 training steps: 0.605808325204362\n",
      "Training loss per 100 training steps: 0.6066641506192929\n",
      "Training loss per 100 training steps: 0.6062118342391687\n",
      "Training loss epoch: 0.6065728006584068\n",
      "Training accuracy epoch: 0.7889339995222996\n",
      "Training epoch: 4\n",
      "Training loss per 100 training steps: 0.41485127806663513\n",
      "Training loss per 100 training steps: 0.5132184974332847\n",
      "Training loss per 100 training steps: 0.5054355473512441\n",
      "Training loss per 100 training steps: 0.5048837241936364\n",
      "Training loss per 100 training steps: 0.503268151880797\n",
      "Training loss per 100 training steps: 0.5060953332367771\n",
      "Training loss per 100 training steps: 0.504730853706152\n",
      "Training loss per 100 training steps: 0.508250522481732\n",
      "Training loss per 100 training steps: 0.5030332772100761\n",
      "Training loss per 100 training steps: 0.5036420715470425\n",
      "Training loss per 100 training steps: 0.5048985278660005\n",
      "Training loss per 100 training steps: 0.5048250396288492\n",
      "Training loss per 100 training steps: 0.508166346757934\n",
      "Training loss per 100 training steps: 0.5104655503631096\n",
      "Training loss per 100 training steps: 0.509348684303638\n",
      "Training loss per 100 training steps: 0.5108314081133007\n",
      "Training loss per 100 training steps: 0.5136843651933345\n",
      "Training loss per 100 training steps: 0.5137916120017787\n",
      "Training loss per 100 training steps: 0.513432470830297\n",
      "Training loss per 100 training steps: 0.5131150480518524\n",
      "Training loss per 100 training steps: 0.5147362337901495\n",
      "Training loss per 100 training steps: 0.515003695612377\n",
      "Training loss per 100 training steps: 0.5151519345078128\n",
      "Training loss per 100 training steps: 0.5158892507619932\n",
      "Training loss per 100 training steps: 0.51721273878995\n",
      "Training loss per 100 training steps: 0.5178946925670517\n",
      "Training loss per 100 training steps: 0.5188137264503327\n",
      "Training loss per 100 training steps: 0.5190727063701225\n",
      "Training loss per 100 training steps: 0.5172800427300723\n",
      "Training loss per 100 training steps: 0.5179858426006446\n",
      "Training loss per 100 training steps: 0.5179044264295783\n",
      "Training loss per 100 training steps: 0.5187697873451717\n",
      "Training loss per 100 training steps: 0.5197426204507666\n",
      "Training loss per 100 training steps: 0.5197373156573186\n",
      "Training loss per 100 training steps: 0.5207422654424054\n",
      "Training loss per 100 training steps: 0.5217076652571325\n",
      "Training loss per 100 training steps: 0.521871697431768\n",
      "Training loss per 100 training steps: 0.5234690168698843\n",
      "Training loss per 100 training steps: 0.5244628573618133\n",
      "Training loss per 100 training steps: 0.5250827142559574\n",
      "Training loss per 100 training steps: 0.5250051191890874\n",
      "Training loss per 100 training steps: 0.5250036272242342\n",
      "Training loss per 100 training steps: 0.5255726968173015\n",
      "Training loss per 100 training steps: 0.5259876029747388\n",
      "Training loss per 100 training steps: 0.5262744344349369\n",
      "Training loss per 100 training steps: 0.5256174950914445\n",
      "Training loss per 100 training steps: 0.524710690068683\n",
      "Training loss per 100 training steps: 0.5248963004384262\n",
      "Training loss per 100 training steps: 0.5254035220729563\n",
      "Training loss per 100 training steps: 0.5257338128757463\n",
      "Training loss per 100 training steps: 0.5251061319053424\n",
      "Training loss per 100 training steps: 0.5250993141051111\n",
      "Training loss per 100 training steps: 0.5253586817408864\n",
      "Training loss per 100 training steps: 0.5257195368102757\n",
      "Training loss per 100 training steps: 0.5251511136534435\n",
      "Training loss per 100 training steps: 0.52587379569889\n",
      "Training loss per 100 training steps: 0.5259446955710474\n",
      "Training loss per 100 training steps: 0.5267984368020747\n",
      "Training loss per 100 training steps: 0.526902013628971\n",
      "Training loss per 100 training steps: 0.5270185151939634\n",
      "Training loss per 100 training steps: 0.5271618422198406\n",
      "Training loss per 100 training steps: 0.5270881390260431\n",
      "Training loss per 100 training steps: 0.5268126457972866\n",
      "Training loss epoch: 0.5272807025551586\n",
      "Training accuracy epoch: 0.8155315820868453\n",
      "Training epoch: 5\n",
      "Training loss per 100 training steps: 0.4756675064563751\n",
      "Training loss per 100 training steps: 0.424719970267598\n",
      "Training loss per 100 training steps: 0.43505546784223015\n",
      "Training loss per 100 training steps: 0.4397882239218566\n",
      "Training loss per 100 training steps: 0.4346037460235586\n",
      "Training loss per 100 training steps: 0.43810663858573595\n",
      "Training loss per 100 training steps: 0.4369588538000171\n",
      "Training loss per 100 training steps: 0.437614785393065\n",
      "Training loss per 100 training steps: 0.4382164474023266\n",
      "Training loss per 100 training steps: 0.4385078226719527\n",
      "Training loss per 100 training steps: 0.43897628934531063\n",
      "Training loss per 100 training steps: 0.4414006949459066\n",
      "Training loss per 100 training steps: 0.4433009527456155\n",
      "Training loss per 100 training steps: 0.4446503725948011\n",
      "Training loss per 100 training steps: 0.44725426829963133\n",
      "Training loss per 100 training steps: 0.4493049932699613\n",
      "Training loss per 100 training steps: 0.4495160728879157\n",
      "Training loss per 100 training steps: 0.450022572930738\n",
      "Training loss per 100 training steps: 0.44778039229869315\n",
      "Training loss per 100 training steps: 0.4510055490853221\n",
      "Training loss per 100 training steps: 0.4518227696173016\n",
      "Training loss per 100 training steps: 0.45077879848521646\n",
      "Training loss per 100 training steps: 0.4503660273925655\n",
      "Training loss per 100 training steps: 0.4497839921955076\n",
      "Training loss per 100 training steps: 0.4493811766538259\n",
      "Training loss per 100 training steps: 0.4503529492269655\n",
      "Training loss per 100 training steps: 0.45077322981750356\n",
      "Training loss per 100 training steps: 0.45129342349049606\n",
      "Training loss per 100 training steps: 0.45296112668488714\n",
      "Training loss per 100 training steps: 0.4527598111686399\n",
      "Training loss per 100 training steps: 0.45194194122021775\n",
      "Training loss per 100 training steps: 0.45105765405796067\n",
      "Training loss per 100 training steps: 0.4511069300741898\n",
      "Training loss per 100 training steps: 0.4503992953668107\n",
      "Training loss per 100 training steps: 0.451008992036873\n",
      "Training loss per 100 training steps: 0.45196206056451976\n",
      "Training loss per 100 training steps: 0.4522140875305609\n",
      "Training loss per 100 training steps: 0.45303355965242936\n",
      "Training loss per 100 training steps: 0.4528122757181968\n",
      "Training loss per 100 training steps: 0.4531346232432397\n",
      "Training loss per 100 training steps: 0.45306684458160423\n",
      "Training loss per 100 training steps: 0.45283702898464445\n",
      "Training loss per 100 training steps: 0.4529271155003422\n",
      "Training loss per 100 training steps: 0.45369376484161755\n",
      "Training loss per 100 training steps: 0.4534049773471541\n",
      "Training loss per 100 training steps: 0.4534562636622851\n",
      "Training loss per 100 training steps: 0.454305345426951\n",
      "Training loss per 100 training steps: 0.4549080345924675\n",
      "Training loss per 100 training steps: 0.45579843377507095\n",
      "Training loss per 100 training steps: 0.4554783419470864\n",
      "Training loss per 100 training steps: 0.45559454661319315\n",
      "Training loss per 100 training steps: 0.4556701833218295\n",
      "Training loss per 100 training steps: 0.4559646713271826\n",
      "Training loss per 100 training steps: 0.45652958163364366\n",
      "Training loss per 100 training steps: 0.4565883476112622\n",
      "Training loss per 100 training steps: 0.4564059793927457\n",
      "Training loss per 100 training steps: 0.4567418962722377\n",
      "Training loss per 100 training steps: 0.457569475955\n",
      "Training loss per 100 training steps: 0.457975104808232\n",
      "Training loss per 100 training steps: 0.45813080395602346\n",
      "Training loss per 100 training steps: 0.4581111347840699\n",
      "Training loss per 100 training steps: 0.45818295081904276\n",
      "Training loss per 100 training steps: 0.4587229603823107\n",
      "Training loss epoch: 0.4585248714037605\n",
      "Training accuracy epoch: 0.8385230901323022\n",
      "Training epoch: 6\n",
      "Training loss per 100 training steps: 0.2767373025417328\n",
      "Training loss per 100 training steps: 0.3789360779936951\n",
      "Training loss per 100 training steps: 0.3788844960839001\n",
      "Training loss per 100 training steps: 0.3752638572746535\n",
      "Training loss per 100 training steps: 0.37131501330141714\n",
      "Training loss per 100 training steps: 0.37477739068294713\n",
      "Training loss per 100 training steps: 0.3746855862550251\n",
      "Training loss per 100 training steps: 0.3777458494616813\n",
      "Training loss per 100 training steps: 0.3769761115014925\n",
      "Training loss per 100 training steps: 0.379282542995356\n",
      "Training loss per 100 training steps: 0.3797277234546788\n",
      "Training loss per 100 training steps: 0.3808262982861656\n",
      "Training loss per 100 training steps: 0.3786283646470601\n",
      "Training loss per 100 training steps: 0.3796694720025066\n",
      "Training loss per 100 training steps: 0.37880793262238505\n",
      "Training loss per 100 training steps: 0.38029385368270846\n",
      "Training loss per 100 training steps: 0.3776137991881069\n",
      "Training loss per 100 training steps: 0.37723416090340106\n",
      "Training loss per 100 training steps: 0.3778507485396246\n",
      "Training loss per 100 training steps: 0.37751918772371706\n",
      "Training loss per 100 training steps: 0.37727243947456296\n",
      "Training loss per 100 training steps: 0.3787051879733035\n",
      "Training loss per 100 training steps: 0.3789169909216731\n",
      "Training loss per 100 training steps: 0.37932284914365105\n",
      "Training loss per 100 training steps: 0.3812816689409213\n",
      "Training loss per 100 training steps: 0.38180550552386255\n",
      "Training loss per 100 training steps: 0.3825424553812845\n",
      "Training loss per 100 training steps: 0.38341742619990704\n",
      "Training loss per 100 training steps: 0.3835067823775291\n",
      "Training loss per 100 training steps: 0.3846870518274418\n",
      "Training loss per 100 training steps: 0.3852489335379901\n",
      "Training loss per 100 training steps: 0.38538968638823\n",
      "Training loss per 100 training steps: 0.3862622421274415\n",
      "Training loss per 100 training steps: 0.3856021107305084\n",
      "Training loss per 100 training steps: 0.38529187891116407\n",
      "Training loss per 100 training steps: 0.38560602296344876\n",
      "Training loss per 100 training steps: 0.3858216724761184\n",
      "Training loss per 100 training steps: 0.3865588354180831\n",
      "Training loss per 100 training steps: 0.3874083687437385\n",
      "Training loss per 100 training steps: 0.3882065505984005\n",
      "Training loss per 100 training steps: 0.3886198025872762\n",
      "Training loss per 100 training steps: 0.38943908271697475\n",
      "Training loss per 100 training steps: 0.3897977263942251\n",
      "Training loss per 100 training steps: 0.39044947153829096\n",
      "Training loss per 100 training steps: 0.3913590162133316\n",
      "Training loss per 100 training steps: 0.3915801443671901\n",
      "Training loss per 100 training steps: 0.3917395504269651\n",
      "Training loss per 100 training steps: 0.3922749638111287\n",
      "Training loss per 100 training steps: 0.3923870661431546\n",
      "Training loss per 100 training steps: 0.39295712412445005\n",
      "Training loss per 100 training steps: 0.39336571978489665\n",
      "Training loss per 100 training steps: 0.3939916207320905\n",
      "Training loss per 100 training steps: 0.39388684971572124\n",
      "Training loss per 100 training steps: 0.39385276380873663\n",
      "Training loss per 100 training steps: 0.3940139281032533\n",
      "Training loss per 100 training steps: 0.3940862221160822\n",
      "Training loss per 100 training steps: 0.39450265599690393\n",
      "Training loss per 100 training steps: 0.3949552373361078\n",
      "Training loss per 100 training steps: 0.3951530490928535\n",
      "Training loss per 100 training steps: 0.3956374271197216\n",
      "Training loss per 100 training steps: 0.3956088702051769\n",
      "Training loss per 100 training steps: 0.39569885894891876\n",
      "Training loss per 100 training steps: 0.39574233063978304\n",
      "Training loss epoch: 0.3954075273689517\n",
      "Training accuracy epoch: 0.8595772438417512\n",
      "Training epoch: 7\n",
      "Training loss per 100 training steps: 0.4896973669528961\n",
      "Training loss per 100 training steps: 0.33141571969384015\n",
      "Training loss per 100 training steps: 0.33111902566357926\n",
      "Training loss per 100 training steps: 0.3327289309006098\n",
      "Training loss per 100 training steps: 0.33294367635543654\n",
      "Training loss per 100 training steps: 0.33084296532897295\n",
      "Training loss per 100 training steps: 0.331554947055244\n",
      "Training loss per 100 training steps: 0.335172984289396\n",
      "Training loss per 100 training steps: 0.33856156757903677\n",
      "Training loss per 100 training steps: 0.3410505502899234\n",
      "Training loss per 100 training steps: 0.3395070928417809\n",
      "Training loss per 100 training steps: 0.33893313551056015\n",
      "Training loss per 100 training steps: 0.3385371785598507\n",
      "Training loss per 100 training steps: 0.33776352300146145\n",
      "Training loss per 100 training steps: 0.33658656316924357\n",
      "Training loss per 100 training steps: 0.3351153572915813\n",
      "Training loss per 100 training steps: 0.336734393176429\n",
      "Training loss per 100 training steps: 0.33786514668951556\n",
      "Training loss per 100 training steps: 0.3385188557251022\n",
      "Training loss per 100 training steps: 0.3378710845325618\n",
      "Training loss per 100 training steps: 0.33759822662098743\n",
      "Training loss per 100 training steps: 0.33790113887120604\n",
      "Training loss per 100 training steps: 0.3376699289952704\n",
      "Training loss per 100 training steps: 0.33627064388548095\n",
      "Training loss per 100 training steps: 0.3352195895946073\n",
      "Training loss per 100 training steps: 0.33444590582439704\n",
      "Training loss per 100 training steps: 0.3344531989821456\n",
      "Training loss per 100 training steps: 0.3362196410210684\n",
      "Training loss per 100 training steps: 0.3360677182146404\n",
      "Training loss per 100 training steps: 0.33602094550811906\n",
      "Training loss per 100 training steps: 0.3364720444887956\n",
      "Training loss per 100 training steps: 0.33680705833724817\n",
      "Training loss per 100 training steps: 0.33637386651506596\n",
      "Training loss per 100 training steps: 0.3370151971307434\n",
      "Training loss per 100 training steps: 0.33731835245948105\n",
      "Training loss per 100 training steps: 0.3371523912342718\n",
      "Training loss per 100 training steps: 0.3385509053119493\n",
      "Training loss per 100 training steps: 0.33921160804240147\n",
      "Training loss per 100 training steps: 0.3396814534662625\n",
      "Training loss per 100 training steps: 0.34012582560705623\n",
      "Training loss per 100 training steps: 0.34062318940956765\n",
      "Training loss per 100 training steps: 0.3412251530142019\n",
      "Training loss per 100 training steps: 0.3415366868490486\n",
      "Training loss per 100 training steps: 0.34218640797449956\n",
      "Training loss per 100 training steps: 0.3419312444583032\n",
      "Training loss per 100 training steps: 0.342153183870486\n",
      "Training loss per 100 training steps: 0.34215940872174494\n",
      "Training loss per 100 training steps: 0.3424231895396969\n",
      "Training loss per 100 training steps: 0.34186252444593396\n",
      "Training loss per 100 training steps: 0.34168028213789503\n",
      "Training loss per 100 training steps: 0.3417295927350003\n",
      "Training loss per 100 training steps: 0.3415647341504253\n",
      "Training loss per 100 training steps: 0.34151397331149874\n",
      "Training loss per 100 training steps: 0.34149290431932827\n",
      "Training loss per 100 training steps: 0.34220003087093914\n",
      "Training loss per 100 training steps: 0.34200614149873704\n",
      "Training loss per 100 training steps: 0.34231714889717435\n",
      "Training loss per 100 training steps: 0.34273001497629046\n",
      "Training loss per 100 training steps: 0.3427899484204761\n",
      "Training loss per 100 training steps: 0.3429535626643065\n",
      "Training loss per 100 training steps: 0.3430702234492658\n",
      "Training loss per 100 training steps: 0.34333429924193926\n",
      "Training loss per 100 training steps: 0.3433069148674034\n",
      "Training loss epoch: 0.3432668396844324\n",
      "Training accuracy epoch: 0.8783197918992146\n",
      "Training epoch: 8\n",
      "Training loss per 100 training steps: 0.16587704420089722\n",
      "Training loss per 100 training steps: 0.31619429739542526\n",
      "Training loss per 100 training steps: 0.3058980876459411\n",
      "Training loss per 100 training steps: 0.3006851473866507\n",
      "Training loss per 100 training steps: 0.29360946840107294\n",
      "Training loss per 100 training steps: 0.29180191911593406\n",
      "Training loss per 100 training steps: 0.2880409271169621\n",
      "Training loss per 100 training steps: 0.2859839209688883\n",
      "Training loss per 100 training steps: 0.28582551890749314\n",
      "Training loss per 100 training steps: 0.2837142052953767\n",
      "Training loss per 100 training steps: 0.28427080015515116\n",
      "Training loss per 100 training steps: 0.2847635595840178\n",
      "Training loss per 100 training steps: 0.28584042606839627\n",
      "Training loss per 100 training steps: 0.2833739711740537\n",
      "Training loss per 100 training steps: 0.2843768260894419\n",
      "Training loss per 100 training steps: 0.28452071792469036\n",
      "Training loss per 100 training steps: 0.28479566373196263\n",
      "Training loss per 100 training steps: 0.28477436683191615\n",
      "Training loss per 100 training steps: 0.2853010842790708\n",
      "Training loss per 100 training steps: 0.28662436578750045\n",
      "Training loss per 100 training steps: 0.2883793020277456\n",
      "Training loss per 100 training steps: 0.28828143254135236\n",
      "Training loss per 100 training steps: 0.2881984751500606\n",
      "Training loss per 100 training steps: 0.2891180003252122\n",
      "Training loss per 100 training steps: 0.2885798796598661\n",
      "Training loss per 100 training steps: 0.28834416942208446\n",
      "Training loss per 100 training steps: 0.2886964755160723\n",
      "Training loss per 100 training steps: 0.2892441535185814\n",
      "Training loss per 100 training steps: 0.2892709964715405\n",
      "Training loss per 100 training steps: 0.29060641607614435\n",
      "Training loss per 100 training steps: 0.290320187087721\n",
      "Training loss per 100 training steps: 0.29072243121255315\n",
      "Training loss per 100 training steps: 0.2901664483548607\n",
      "Training loss per 100 training steps: 0.29101877973968787\n",
      "Training loss per 100 training steps: 0.29164324817706366\n",
      "Training loss per 100 training steps: 0.2922556066895699\n",
      "Training loss per 100 training steps: 0.29221559440785894\n",
      "Training loss per 100 training steps: 0.2924553840402267\n",
      "Training loss per 100 training steps: 0.29264380039940757\n",
      "Training loss per 100 training steps: 0.29220970750038394\n",
      "Training loss per 100 training steps: 0.2933114888134881\n",
      "Training loss per 100 training steps: 0.29395014493095095\n",
      "Training loss per 100 training steps: 0.2940619969756028\n",
      "Training loss per 100 training steps: 0.2941046548035915\n",
      "Training loss per 100 training steps: 0.2946091691297419\n",
      "Training loss per 100 training steps: 0.2954071388190699\n",
      "Training loss per 100 training steps: 0.29504726276690674\n",
      "Training loss per 100 training steps: 0.2956093266789386\n",
      "Training loss per 100 training steps: 0.2956851664798672\n",
      "Training loss per 100 training steps: 0.29649848362457154\n",
      "Training loss per 100 training steps: 0.2966736061437068\n",
      "Training loss per 100 training steps: 0.2964876239235303\n",
      "Training loss per 100 training steps: 0.2971790719718612\n",
      "Training loss per 100 training steps: 0.29759554957053536\n",
      "Training loss per 100 training steps: 0.2977729800831988\n",
      "Training loss per 100 training steps: 0.2974118657333621\n",
      "Training loss per 100 training steps: 0.2978845688690167\n",
      "Training loss per 100 training steps: 0.29801953579152946\n",
      "Training loss per 100 training steps: 0.29783845320954044\n",
      "Training loss per 100 training steps: 0.29797000917666816\n",
      "Training loss per 100 training steps: 0.2980133360628907\n",
      "Training loss per 100 training steps: 0.2980052038225492\n",
      "Training loss per 100 training steps: 0.29806499119467733\n",
      "Training loss epoch: 0.29812206152071696\n",
      "Training accuracy epoch: 0.894682028457561\n",
      "Training epoch: 9\n",
      "Training loss per 100 training steps: 0.3931213915348053\n",
      "Training loss per 100 training steps: 0.2558435074210462\n",
      "Training loss per 100 training steps: 0.25328783148816275\n",
      "Training loss per 100 training steps: 0.2482974555813692\n",
      "Training loss per 100 training steps: 0.24668320280005063\n",
      "Training loss per 100 training steps: 0.24557516260998336\n",
      "Training loss per 100 training steps: 0.2457113965707343\n",
      "Training loss per 100 training steps: 0.24741344765285708\n",
      "Training loss per 100 training steps: 0.24861745145892755\n",
      "Training loss per 100 training steps: 0.2512530406273397\n",
      "Training loss per 100 training steps: 0.25152826837108627\n",
      "Training loss per 100 training steps: 0.25175679405198703\n",
      "Training loss per 100 training steps: 0.253103024896225\n",
      "Training loss per 100 training steps: 0.2528523272440284\n",
      "Training loss per 100 training steps: 0.2521050534783454\n",
      "Training loss per 100 training steps: 0.2511613273926124\n",
      "Training loss per 100 training steps: 0.2524992543721175\n",
      "Training loss per 100 training steps: 0.2524391905886988\n",
      "Training loss per 100 training steps: 0.2519728820134928\n",
      "Training loss per 100 training steps: 0.2528841150748195\n",
      "Training loss per 100 training steps: 0.2515634197833902\n",
      "Training loss per 100 training steps: 0.2519956300467756\n",
      "Training loss per 100 training steps: 0.2514167097957804\n",
      "Training loss per 100 training steps: 0.25157891740671956\n",
      "Training loss per 100 training steps: 0.2526736481980991\n",
      "Training loss per 100 training steps: 0.2540443628301761\n",
      "Training loss per 100 training steps: 0.2550624090013041\n",
      "Training loss per 100 training steps: 0.25543497131067827\n",
      "Training loss per 100 training steps: 0.2548986818945326\n",
      "Training loss per 100 training steps: 0.25419539773803984\n",
      "Training loss per 100 training steps: 0.253516831042376\n",
      "Training loss per 100 training steps: 0.2536826190114939\n",
      "Training loss per 100 training steps: 0.253607332089352\n",
      "Training loss per 100 training steps: 0.2543773451144024\n",
      "Training loss per 100 training steps: 0.25479356354796395\n",
      "Training loss per 100 training steps: 0.2552549125029743\n",
      "Training loss per 100 training steps: 0.25507441281385757\n",
      "Training loss per 100 training steps: 0.25588596156317417\n",
      "Training loss per 100 training steps: 0.25635644992050477\n",
      "Training loss per 100 training steps: 0.25670543507527455\n",
      "Training loss per 100 training steps: 0.2569365396579391\n",
      "Training loss per 100 training steps: 0.2568523880120179\n",
      "Training loss per 100 training steps: 0.2571730171273742\n",
      "Training loss per 100 training steps: 0.2573510429940354\n",
      "Training loss per 100 training steps: 0.2572745780698181\n",
      "Training loss per 100 training steps: 0.2575056389712047\n",
      "Training loss per 100 training steps: 0.25742168115240205\n",
      "Training loss per 100 training steps: 0.2573782969212686\n",
      "Training loss per 100 training steps: 0.25771600273595896\n",
      "Training loss per 100 training steps: 0.2576963731360053\n",
      "Training loss per 100 training steps: 0.2575243071089984\n",
      "Training loss per 100 training steps: 0.2578764485861318\n",
      "Training loss per 100 training steps: 0.2582568256814771\n",
      "Training loss per 100 training steps: 0.25799773237742646\n",
      "Training loss per 100 training steps: 0.25809473972602054\n",
      "Training loss per 100 training steps: 0.25811916895307585\n",
      "Training loss per 100 training steps: 0.2584562703610927\n",
      "Training loss per 100 training steps: 0.2589085761614835\n",
      "Training loss per 100 training steps: 0.2587657378131735\n",
      "Training loss per 100 training steps: 0.25897297119824986\n",
      "Training loss per 100 training steps: 0.25867254340500034\n",
      "Training loss per 100 training steps: 0.25903891825993847\n",
      "Training loss per 100 training steps: 0.2595445669674127\n",
      "Training loss epoch: 0.25939188384964257\n",
      "Training accuracy epoch: 0.9089566770064206\n",
      "Training epoch: 10\n",
      "Training loss per 100 training steps: 0.15778985619544983\n",
      "Training loss per 100 training steps: 0.18546113287015717\n",
      "Training loss per 100 training steps: 0.19729406101193595\n",
      "Training loss per 100 training steps: 0.20896347050254924\n",
      "Training loss per 100 training steps: 0.21209622719824464\n",
      "Training loss per 100 training steps: 0.21542578335933224\n",
      "Training loss per 100 training steps: 0.2124940752828131\n",
      "Training loss per 100 training steps: 0.2173775017511275\n",
      "Training loss per 100 training steps: 0.21860090025308798\n",
      "Training loss per 100 training steps: 0.2193791666393738\n",
      "Training loss per 100 training steps: 0.21744650078157743\n",
      "Training loss per 100 training steps: 0.2163433253200053\n",
      "Training loss per 100 training steps: 0.2166017496060397\n",
      "Training loss per 100 training steps: 0.21639674235218356\n",
      "Training loss per 100 training steps: 0.21650816258348837\n",
      "Training loss per 100 training steps: 0.2175748330006732\n",
      "Training loss per 100 training steps: 0.21695533506735126\n",
      "Training loss per 100 training steps: 0.21819450783881106\n",
      "Training loss per 100 training steps: 0.21822806767838024\n",
      "Training loss per 100 training steps: 0.21761859213734444\n",
      "Training loss per 100 training steps: 0.21706262549877853\n",
      "Training loss per 100 training steps: 0.21798612756910835\n",
      "Training loss per 100 training steps: 0.21804056714023584\n",
      "Training loss per 100 training steps: 0.21784265669809896\n",
      "Training loss per 100 training steps: 0.21955637365691316\n",
      "Training loss per 100 training steps: 0.22039836752073902\n",
      "Training loss per 100 training steps: 0.22173090028744485\n",
      "Training loss per 100 training steps: 0.22293108252974647\n",
      "Training loss per 100 training steps: 0.2235574579396362\n",
      "Training loss per 100 training steps: 0.22348997920946762\n",
      "Training loss per 100 training steps: 0.22342295215894326\n",
      "Training loss per 100 training steps: 0.22311686405354028\n",
      "Training loss per 100 training steps: 0.22327119374914825\n",
      "Training loss per 100 training steps: 0.22344794228528528\n",
      "Training loss per 100 training steps: 0.22350922540285875\n",
      "Training loss per 100 training steps: 0.22319357038093307\n",
      "Training loss per 100 training steps: 0.22314307783021425\n",
      "Training loss per 100 training steps: 0.22381031331730772\n",
      "Training loss per 100 training steps: 0.224208029790717\n",
      "Training loss per 100 training steps: 0.22485233710348698\n",
      "Training loss per 100 training steps: 0.22516933646238363\n",
      "Training loss per 100 training steps: 0.2249220352805197\n",
      "Training loss per 100 training steps: 0.2247757474601205\n",
      "Training loss per 100 training steps: 0.2245262016552433\n",
      "Training loss per 100 training steps: 0.22483767062202148\n",
      "Training loss per 100 training steps: 0.2243668067603608\n",
      "Training loss per 100 training steps: 0.22486619684716472\n",
      "Training loss per 100 training steps: 0.224921683653067\n",
      "Training loss per 100 training steps: 0.22533488665470552\n",
      "Training loss per 100 training steps: 0.22619856414311978\n",
      "Training loss per 100 training steps: 0.2261474423603615\n",
      "Training loss per 100 training steps: 0.22642088689091122\n",
      "Training loss per 100 training steps: 0.22682950026295698\n",
      "Training loss per 100 training steps: 0.2265857696829062\n",
      "Training loss per 100 training steps: 0.22668773937336528\n",
      "Training loss per 100 training steps: 0.2271186119950109\n",
      "Training loss per 100 training steps: 0.22701290949023137\n",
      "Training loss per 100 training steps: 0.2272462208272255\n",
      "Training loss per 100 training steps: 0.2277894461393572\n",
      "Training loss per 100 training steps: 0.2280216361482029\n",
      "Training loss per 100 training steps: 0.22799551008473115\n",
      "Training loss per 100 training steps: 0.22752231081779464\n",
      "Training loss per 100 training steps: 0.22792743392489337\n",
      "Training loss epoch: 0.22775709324729254\n",
      "Training accuracy epoch: 0.9204726717584442\n",
      "Training epoch: 11\n",
      "Training loss per 100 training steps: 0.14168110489845276\n",
      "Training loss per 100 training steps: 0.17349548995642378\n",
      "Training loss per 100 training steps: 0.17480627350991046\n",
      "Training loss per 100 training steps: 0.18505648034083289\n",
      "Training loss per 100 training steps: 0.18175334118752556\n",
      "Training loss per 100 training steps: 0.18219104883244294\n",
      "Training loss per 100 training steps: 0.18119941085589408\n",
      "Training loss per 100 training steps: 0.18234634522438983\n",
      "Training loss per 100 training steps: 0.1817909012828836\n",
      "Training loss per 100 training steps: 0.17944100934321622\n",
      "Training loss per 100 training steps: 0.1799273820673967\n",
      "Training loss per 100 training steps: 0.1830957301779837\n",
      "Training loss per 100 training steps: 0.184917945337236\n",
      "Training loss per 100 training steps: 0.18407217380687377\n",
      "Training loss per 100 training steps: 0.1858598016478988\n",
      "Training loss per 100 training steps: 0.18644312913247263\n",
      "Training loss per 100 training steps: 0.1863819206404563\n",
      "Training loss per 100 training steps: 0.18684962374880376\n",
      "Training loss per 100 training steps: 0.18802981661102097\n",
      "Training loss per 100 training steps: 0.18854862501622344\n",
      "Training loss per 100 training steps: 0.18835810028102207\n",
      "Training loss per 100 training steps: 0.18946923073762403\n",
      "Training loss per 100 training steps: 0.1902610528537126\n",
      "Training loss per 100 training steps: 0.18973261832151528\n",
      "Training loss per 100 training steps: 0.19048143595516037\n",
      "Training loss per 100 training steps: 0.19098757798882163\n",
      "Training loss per 100 training steps: 0.19062352333660426\n",
      "Training loss per 100 training steps: 0.19083248465075422\n",
      "Training loss per 100 training steps: 0.19170373158382437\n",
      "Training loss per 100 training steps: 0.19177956363364093\n",
      "Training loss per 100 training steps: 0.19240911555268872\n",
      "Training loss per 100 training steps: 0.1924610099665429\n",
      "Training loss per 100 training steps: 0.1924796129173318\n",
      "Training loss per 100 training steps: 0.19188926124962774\n",
      "Training loss per 100 training steps: 0.19218781744577682\n",
      "Training loss per 100 training steps: 0.1927197036444843\n",
      "Training loss per 100 training steps: 0.19268001272904065\n",
      "Training loss per 100 training steps: 0.19290172328059738\n",
      "Training loss per 100 training steps: 0.1929035003536416\n",
      "Training loss per 100 training steps: 0.19332470653124556\n",
      "Training loss per 100 training steps: 0.1940247784123119\n",
      "Training loss per 100 training steps: 0.19439714396827992\n",
      "Training loss per 100 training steps: 0.1943285619523895\n",
      "Training loss per 100 training steps: 0.19498571951329444\n",
      "Training loss per 100 training steps: 0.195091627375683\n",
      "Training loss per 100 training steps: 0.1954377360552569\n",
      "Training loss per 100 training steps: 0.19603949135675025\n",
      "Training loss per 100 training steps: 0.1960496687043242\n",
      "Training loss per 100 training steps: 0.19629924192867895\n",
      "Training loss per 100 training steps: 0.19649497595446752\n",
      "Training loss per 100 training steps: 0.19676989772284603\n",
      "Training loss per 100 training steps: 0.19675537327443607\n",
      "Training loss per 100 training steps: 0.19719296268967654\n",
      "Training loss per 100 training steps: 0.19722992931544456\n",
      "Training loss per 100 training steps: 0.19722956264558114\n",
      "Training loss per 100 training steps: 0.1975720967986387\n",
      "Training loss per 100 training steps: 0.19792161047374845\n",
      "Training loss per 100 training steps: 0.19809671743843005\n",
      "Training loss per 100 training steps: 0.19819733632127698\n",
      "Training loss per 100 training steps: 0.19847666518091953\n",
      "Training loss per 100 training steps: 0.19881756140555149\n",
      "Training loss per 100 training steps: 0.1990164301976857\n",
      "Training loss per 100 training steps: 0.19901113225565556\n",
      "Training loss epoch: 0.1989835987402632\n",
      "Training accuracy epoch: 0.931091737399515\n",
      "Training epoch: 12\n",
      "Training loss per 100 training steps: 0.11919977515935898\n",
      "Training loss per 100 training steps: 0.19618511334577998\n",
      "Training loss per 100 training steps: 0.17858026430603877\n",
      "Training loss per 100 training steps: 0.1744240734020737\n",
      "Training loss per 100 training steps: 0.1746852858818863\n",
      "Training loss per 100 training steps: 0.1679559927563854\n",
      "Training loss per 100 training steps: 0.16696404012543042\n",
      "Training loss per 100 training steps: 0.1669784337561179\n",
      "Training loss per 100 training steps: 0.164683096574738\n",
      "Training loss per 100 training steps: 0.16589581691758185\n",
      "Training loss per 100 training steps: 0.1659313198597683\n",
      "Training loss per 100 training steps: 0.16679222440578848\n",
      "Training loss per 100 training steps: 0.16675888251925786\n",
      "Training loss per 100 training steps: 0.16918772935787135\n",
      "Training loss per 100 training steps: 0.1700363928368375\n",
      "Training loss per 100 training steps: 0.17038727289861158\n",
      "Training loss per 100 training steps: 0.17048348249317258\n",
      "Training loss per 100 training steps: 0.17013866354058169\n",
      "Training loss per 100 training steps: 0.1698310069071707\n",
      "Training loss per 100 training steps: 0.17101572088795328\n",
      "Training loss per 100 training steps: 0.17205346916904157\n",
      "Training loss per 100 training steps: 0.17290872962669296\n",
      "Training loss per 100 training steps: 0.1732015820462993\n",
      "Training loss per 100 training steps: 0.1729859745195512\n",
      "Training loss per 100 training steps: 0.17240028873610402\n",
      "Training loss per 100 training steps: 0.17333260629443778\n",
      "Training loss per 100 training steps: 0.17333234092940372\n",
      "Training loss per 100 training steps: 0.17349714364598567\n",
      "Training loss per 100 training steps: 0.17320335196827893\n",
      "Training loss per 100 training steps: 0.1737704775573847\n",
      "Training loss per 100 training steps: 0.17356806632562086\n",
      "Training loss per 100 training steps: 0.1730112693128673\n",
      "Training loss per 100 training steps: 0.17280930644602413\n",
      "Training loss per 100 training steps: 0.1729768032405014\n",
      "Training loss per 100 training steps: 0.1730507922514077\n",
      "Training loss per 100 training steps: 0.17293566266291313\n",
      "Training loss per 100 training steps: 0.17343788008161368\n",
      "Training loss per 100 training steps: 0.17366236496854637\n",
      "Training loss per 100 training steps: 0.17337296919280237\n",
      "Training loss per 100 training steps: 0.17301257893894803\n",
      "Training loss per 100 training steps: 0.17306842920467\n",
      "Training loss per 100 training steps: 0.1737090574556558\n",
      "Training loss per 100 training steps: 0.17348725603854825\n",
      "Training loss per 100 training steps: 0.1737908575122137\n",
      "Training loss per 100 training steps: 0.1737779589891315\n",
      "Training loss per 100 training steps: 0.17386604887920634\n",
      "Training loss per 100 training steps: 0.17380609482521112\n",
      "Training loss per 100 training steps: 0.17402747874337388\n",
      "Training loss per 100 training steps: 0.1740838239870312\n",
      "Training loss per 100 training steps: 0.17384433506884805\n",
      "Training loss per 100 training steps: 0.1740293259626462\n",
      "Training loss per 100 training steps: 0.17428315056689117\n",
      "Training loss per 100 training steps: 0.17439434923309674\n",
      "Training loss per 100 training steps: 0.17445624121400663\n",
      "Training loss per 100 training steps: 0.17466524137324385\n",
      "Training loss per 100 training steps: 0.17519428842314844\n",
      "Training loss per 100 training steps: 0.17523242269506803\n",
      "Training loss per 100 training steps: 0.17536127257182568\n",
      "Training loss per 100 training steps: 0.17513719765709287\n",
      "Training loss per 100 training steps: 0.17542620699813566\n",
      "Training loss per 100 training steps: 0.17545064592042123\n",
      "Training loss per 100 training steps: 0.17554625346914723\n",
      "Training loss per 100 training steps: 0.17587716310586762\n",
      "Training loss epoch: 0.1759095831119353\n",
      "Training accuracy epoch: 0.9394545577710487\n",
      "Training epoch: 13\n",
      "Training loss per 100 training steps: 0.050167862325906754\n",
      "Training loss per 100 training steps: 0.1435369203826136\n",
      "Training loss per 100 training steps: 0.15684397973639158\n",
      "Training loss per 100 training steps: 0.15212523106770262\n",
      "Training loss per 100 training steps: 0.1497946050380382\n",
      "Training loss per 100 training steps: 0.14895141486100807\n",
      "Training loss per 100 training steps: 0.14902430367245403\n",
      "Training loss per 100 training steps: 0.14741442430088197\n",
      "Training loss per 100 training steps: 0.14952026729613832\n",
      "Training loss per 100 training steps: 0.14954834269617087\n",
      "Training loss per 100 training steps: 0.14918127807520598\n",
      "Training loss per 100 training steps: 0.14746305778462754\n",
      "Training loss per 100 training steps: 0.14877057024068374\n",
      "Training loss per 100 training steps: 0.14923531221541547\n",
      "Training loss per 100 training steps: 0.14963338290159056\n",
      "Training loss per 100 training steps: 0.15039143506124203\n",
      "Training loss per 100 training steps: 0.15089136298247344\n",
      "Training loss per 100 training steps: 0.15049099376568048\n",
      "Training loss per 100 training steps: 0.15101665895013888\n",
      "Training loss per 100 training steps: 0.15047754306012548\n",
      "Training loss per 100 training steps: 0.15023928481667978\n",
      "Training loss per 100 training steps: 0.150534298687066\n",
      "Training loss per 100 training steps: 0.15076244723884094\n",
      "Training loss per 100 training steps: 0.1505985857759186\n",
      "Training loss per 100 training steps: 0.15185903615794566\n",
      "Training loss per 100 training steps: 0.15173743701143722\n",
      "Training loss per 100 training steps: 0.15231435723996284\n",
      "Training loss per 100 training steps: 0.15316124483541285\n",
      "Training loss per 100 training steps: 0.15258486627605078\n",
      "Training loss per 100 training steps: 0.15348157767193268\n",
      "Training loss per 100 training steps: 0.15404386386690833\n",
      "Training loss per 100 training steps: 0.15428567007182528\n",
      "Training loss per 100 training steps: 0.15422768340988963\n",
      "Training loss per 100 training steps: 0.15461851103476484\n",
      "Training loss per 100 training steps: 0.15470513682756315\n",
      "Training loss per 100 training steps: 0.1545859734284761\n",
      "Training loss per 100 training steps: 0.15457154212627375\n",
      "Training loss per 100 training steps: 0.15420169544004114\n",
      "Training loss per 100 training steps: 0.15469152617201617\n",
      "Training loss per 100 training steps: 0.1549131369090624\n",
      "Training loss per 100 training steps: 0.15485261029762165\n",
      "Training loss per 100 training steps: 0.15545717560696617\n",
      "Training loss per 100 training steps: 0.15565631163057797\n",
      "Training loss per 100 training steps: 0.15584571691390392\n",
      "Training loss per 100 training steps: 0.15556505244745628\n",
      "Training loss per 100 training steps: 0.1556971570452784\n",
      "Training loss per 100 training steps: 0.156467092669138\n",
      "Training loss per 100 training steps: 0.1564914995672784\n",
      "Training loss per 100 training steps: 0.1561878999355999\n",
      "Training loss per 100 training steps: 0.1562494122698486\n",
      "Training loss per 100 training steps: 0.1564233184919574\n",
      "Training loss per 100 training steps: 0.15642283487694555\n",
      "Training loss per 100 training steps: 0.15672527805612335\n",
      "Training loss per 100 training steps: 0.1568391754712637\n",
      "Training loss per 100 training steps: 0.15703234139296532\n",
      "Training loss per 100 training steps: 0.15707783430331052\n",
      "Training loss per 100 training steps: 0.15710748050313153\n",
      "Training loss per 100 training steps: 0.15698269035429818\n",
      "Training loss per 100 training steps: 0.1567270962469337\n",
      "Training loss per 100 training steps: 0.1569425288939047\n",
      "Training loss per 100 training steps: 0.15703250631239807\n",
      "Training loss per 100 training steps: 0.15777613506998933\n",
      "Training loss per 100 training steps: 0.15841657337586929\n",
      "Training loss epoch: 0.15854461736431116\n",
      "Training accuracy epoch: 0.9455108710955082\n",
      "Training epoch: 14\n",
      "Training loss per 100 training steps: 0.05531810224056244\n",
      "Training loss per 100 training steps: 0.14860244231796502\n",
      "Training loss per 100 training steps: 0.14324670430476688\n",
      "Training loss per 100 training steps: 0.14171367764869006\n",
      "Training loss per 100 training steps: 0.1397524733454026\n",
      "Training loss per 100 training steps: 0.14019955158374922\n",
      "Training loss per 100 training steps: 0.14012059731449245\n",
      "Training loss per 100 training steps: 0.14074479022625297\n",
      "Training loss per 100 training steps: 0.14021073523141472\n",
      "Training loss per 100 training steps: 0.13915552570588977\n",
      "Training loss per 100 training steps: 0.13949212287696866\n",
      "Training loss per 100 training steps: 0.13927552315299074\n",
      "Training loss per 100 training steps: 0.13936261831783534\n",
      "Training loss per 100 training steps: 0.14012978328313166\n",
      "Training loss per 100 training steps: 0.13972575059140646\n",
      "Training loss per 100 training steps: 0.1395858560197383\n",
      "Training loss per 100 training steps: 0.13869875677070725\n",
      "Training loss per 100 training steps: 0.1389965386554787\n",
      "Training loss per 100 training steps: 0.13931499358120766\n",
      "Training loss per 100 training steps: 0.1390913262123586\n",
      "Training loss per 100 training steps: 0.139280180385367\n",
      "Training loss per 100 training steps: 0.13921363001378134\n",
      "Training loss per 100 training steps: 0.13858294392724788\n",
      "Training loss per 100 training steps: 0.13888291314103418\n",
      "Training loss per 100 training steps: 0.13912094871543412\n",
      "Training loss per 100 training steps: 0.13965374308048797\n",
      "Training loss per 100 training steps: 0.13911038739943318\n",
      "Training loss per 100 training steps: 0.13850017552140248\n",
      "Training loss per 100 training steps: 0.13831443534645305\n",
      "Training loss per 100 training steps: 0.13919909871611919\n",
      "Training loss per 100 training steps: 0.13982568632780895\n",
      "Training loss per 100 training steps: 0.13934931047612148\n",
      "Training loss per 100 training steps: 0.14012139840621787\n",
      "Training loss per 100 training steps: 0.13956503167743373\n",
      "Training loss per 100 training steps: 0.13963888953920478\n",
      "Training loss per 100 training steps: 0.1395839555610649\n",
      "Training loss per 100 training steps: 0.13965841225689543\n",
      "Training loss per 100 training steps: 0.1400303792169932\n",
      "Training loss per 100 training steps: 0.14011380165632567\n",
      "Training loss per 100 training steps: 0.14016881048852678\n",
      "Training loss per 100 training steps: 0.1403299023029952\n",
      "Training loss per 100 training steps: 0.14039695093211108\n",
      "Training loss per 100 training steps: 0.14038688895432025\n",
      "Training loss per 100 training steps: 0.14070538833351506\n",
      "Training loss per 100 training steps: 0.14065925848177874\n",
      "Training loss per 100 training steps: 0.14131816387413637\n",
      "Training loss per 100 training steps: 0.14118138449776646\n",
      "Training loss per 100 training steps: 0.14070992627312065\n",
      "Training loss per 100 training steps: 0.14084953509635448\n",
      "Training loss per 100 training steps: 0.14089770431506227\n",
      "Training loss per 100 training steps: 0.14093310963386665\n",
      "Training loss per 100 training steps: 0.14118496949927414\n",
      "Training loss per 100 training steps: 0.1412882167974713\n",
      "Training loss per 100 training steps: 0.1420012663661066\n",
      "Training loss per 100 training steps: 0.14219621357055706\n",
      "Training loss per 100 training steps: 0.1427156276030981\n",
      "Training loss per 100 training steps: 0.142470150186576\n",
      "Training loss per 100 training steps: 0.14236029555728352\n",
      "Training loss per 100 training steps: 0.14213176892952822\n",
      "Training loss per 100 training steps: 0.14232412189612323\n",
      "Training loss per 100 training steps: 0.142720440852924\n",
      "Training loss per 100 training steps: 0.1426259287691925\n",
      "Training loss per 100 training steps: 0.14278067204684627\n",
      "Training loss epoch: 0.14278479584307016\n",
      "Training accuracy epoch: 0.9510861557848116\n",
      "Training epoch: 15\n",
      "Training loss per 100 training steps: 0.13533209264278412\n",
      "Training loss per 100 training steps: 0.10519010836424508\n",
      "Training loss per 100 training steps: 0.11417378048268866\n",
      "Training loss per 100 training steps: 0.10633187429929938\n",
      "Training loss per 100 training steps: 0.11407981090308016\n",
      "Training loss per 100 training steps: 0.11399316307094075\n",
      "Training loss per 100 training steps: 0.11775011626365106\n",
      "Training loss per 100 training steps: 0.12049238339927414\n",
      "Training loss per 100 training steps: 0.12191118250895157\n",
      "Training loss per 100 training steps: 0.12317999042611937\n",
      "Training loss per 100 training steps: 0.12446663534542272\n",
      "Training loss per 100 training steps: 0.12430397071854485\n",
      "Training loss per 100 training steps: 0.12393108768455319\n",
      "Training loss per 100 training steps: 0.12290482166553235\n",
      "Training loss per 100 training steps: 0.12251718850509874\n",
      "Training loss per 100 training steps: 0.12234934189261931\n",
      "Training loss per 100 training steps: 0.12364494500112842\n",
      "Training loss per 100 training steps: 0.12383545995380582\n",
      "Training loss per 100 training steps: 0.12325213009337463\n",
      "Training loss per 100 training steps: 0.12324421630505039\n",
      "Training loss per 100 training steps: 0.12395880322462169\n",
      "Training loss per 100 training steps: 0.12365262405986746\n",
      "Training loss per 100 training steps: 0.12357514938115328\n",
      "Training loss per 100 training steps: 0.12422430107466349\n",
      "Training loss per 100 training steps: 0.12404553238023189\n",
      "Training loss per 100 training steps: 0.12444390885245915\n",
      "Training loss per 100 training steps: 0.1244079698298251\n",
      "Training loss per 100 training steps: 0.1251359760012341\n",
      "Training loss per 100 training steps: 0.12588691765589455\n",
      "Training loss per 100 training steps: 0.12631660413898474\n",
      "Training loss per 100 training steps: 0.12641855084170642\n",
      "Training loss per 100 training steps: 0.12647574377702403\n",
      "Training loss per 100 training steps: 0.12708815536352666\n",
      "Training loss per 100 training steps: 0.12713164386701462\n",
      "Training loss per 100 training steps: 0.1270972154593663\n",
      "Training loss per 100 training steps: 0.12723340335711986\n",
      "Training loss per 100 training steps: 0.12750009129610626\n",
      "Training loss per 100 training steps: 0.12736741305419244\n",
      "Training loss per 100 training steps: 0.1272028377157129\n",
      "Training loss per 100 training steps: 0.12729726038644962\n",
      "Training loss per 100 training steps: 0.12740407215761404\n",
      "Training loss per 100 training steps: 0.1278183224965936\n",
      "Training loss per 100 training steps: 0.1277205392687722\n",
      "Training loss per 100 training steps: 0.12768786130730697\n",
      "Training loss per 100 training steps: 0.1278916639907523\n",
      "Training loss per 100 training steps: 0.12803966518023052\n",
      "Training loss per 100 training steps: 0.12799868804852624\n",
      "Training loss per 100 training steps: 0.12779000707935892\n",
      "Training loss per 100 training steps: 0.12801508538147957\n",
      "Training loss per 100 training steps: 0.127834048890827\n",
      "Training loss per 100 training steps: 0.1278267553101475\n",
      "Training loss per 100 training steps: 0.12791710677857032\n",
      "Training loss per 100 training steps: 0.12801957432194253\n",
      "Training loss per 100 training steps: 0.12803526444195817\n",
      "Training loss per 100 training steps: 0.12862988543717221\n",
      "Training loss per 100 training steps: 0.12888267864804145\n",
      "Training loss per 100 training steps: 0.12910908183681205\n",
      "Training loss per 100 training steps: 0.12917451357520912\n",
      "Training loss per 100 training steps: 0.12919514149557776\n",
      "Training loss per 100 training steps: 0.12913026112871814\n",
      "Training loss per 100 training steps: 0.12956582560841665\n",
      "Training loss per 100 training steps: 0.12956493774221664\n",
      "Training loss per 100 training steps: 0.129507002721402\n",
      "Training loss epoch: 0.1294012271803477\n",
      "Training accuracy epoch: 0.9558985390537741\n",
      "Training epoch: 16\n",
      "Training loss per 100 training steps: 0.03500618413090706\n",
      "Training loss per 100 training steps: 0.11068903048070941\n",
      "Training loss per 100 training steps: 0.12485142816228802\n",
      "Training loss per 100 training steps: 0.11715234714278648\n",
      "Training loss per 100 training steps: 0.11513474020187248\n",
      "Training loss per 100 training steps: 0.11317097775809125\n",
      "Training loss per 100 training steps: 0.11257055395371134\n",
      "Training loss per 100 training steps: 0.11123276413810279\n",
      "Training loss per 100 training steps: 0.11184756348628118\n",
      "Training loss per 100 training steps: 0.1122825208882885\n",
      "Training loss per 100 training steps: 0.11311405507352698\n",
      "Training loss per 100 training steps: 0.11339892526719716\n",
      "Training loss per 100 training steps: 0.11464566173695716\n",
      "Training loss per 100 training steps: 0.11611987833151283\n",
      "Training loss per 100 training steps: 0.11653402403028926\n",
      "Training loss per 100 training steps: 0.11597999602340306\n",
      "Training loss per 100 training steps: 0.1158417884055653\n",
      "Training loss per 100 training steps: 0.11513564970069452\n",
      "Training loss per 100 training steps: 0.1145462215400256\n",
      "Training loss per 100 training steps: 0.11455677945028671\n",
      "Training loss per 100 training steps: 0.11443621246935404\n",
      "Training loss per 100 training steps: 0.11505864713645085\n",
      "Training loss per 100 training steps: 0.11556660905920446\n",
      "Training loss per 100 training steps: 0.11597756658401867\n",
      "Training loss per 100 training steps: 0.11593853349664382\n",
      "Training loss per 100 training steps: 0.11537544241548181\n",
      "Training loss per 100 training steps: 0.11484986077260079\n",
      "Training loss per 100 training steps: 0.11511204447662779\n",
      "Training loss per 100 training steps: 0.11511180779631677\n",
      "Training loss per 100 training steps: 0.11483993212349253\n",
      "Training loss per 100 training steps: 0.11524146536653145\n",
      "Training loss per 100 training steps: 0.11563705558423404\n",
      "Training loss per 100 training steps: 0.11592503696326859\n",
      "Training loss per 100 training steps: 0.11595921971282497\n",
      "Training loss per 100 training steps: 0.11610814409661284\n",
      "Training loss per 100 training steps: 0.11676682064616167\n",
      "Training loss per 100 training steps: 0.11646353342200266\n",
      "Training loss per 100 training steps: 0.11652288228432932\n",
      "Training loss per 100 training steps: 0.11638634113062706\n",
      "Training loss per 100 training steps: 0.11658771939289662\n",
      "Training loss per 100 training steps: 0.11666252089432033\n",
      "Training loss per 100 training steps: 0.11630006861491222\n",
      "Training loss per 100 training steps: 0.11623019678507092\n",
      "Training loss per 100 training steps: 0.1162303824844287\n",
      "Training loss per 100 training steps: 0.11617053215749777\n",
      "Training loss per 100 training steps: 0.11640359332979096\n",
      "Training loss per 100 training steps: 0.11655992500170648\n",
      "Training loss per 100 training steps: 0.1165507250320633\n",
      "Training loss per 100 training steps: 0.11694786568916984\n",
      "Training loss per 100 training steps: 0.11726290054966682\n",
      "Training loss per 100 training steps: 0.11734270560354966\n",
      "Training loss per 100 training steps: 0.11718590156217372\n",
      "Training loss per 100 training steps: 0.11704307591850237\n",
      "Training loss per 100 training steps: 0.11719729600941921\n",
      "Training loss per 100 training steps: 0.11743983185871293\n",
      "Training loss per 100 training steps: 0.11763423553124\n",
      "Training loss per 100 training steps: 0.11758727489446351\n",
      "Training loss per 100 training steps: 0.1174904002891022\n",
      "Training loss per 100 training steps: 0.11755349176981286\n",
      "Training loss per 100 training steps: 0.11766464045300318\n",
      "Training loss per 100 training steps: 0.11779367329566427\n",
      "Training loss per 100 training steps: 0.11773501332185844\n",
      "Training loss per 100 training steps: 0.11787159991447754\n",
      "Training loss epoch: 0.11784453759274173\n",
      "Training accuracy epoch: 0.9601581840452024\n",
      "Training epoch: 17\n",
      "Training loss per 100 training steps: 0.057667359709739685\n",
      "Training loss per 100 training steps: 0.10448785604779968\n",
      "Training loss per 100 training steps: 0.10456067941322997\n",
      "Training loss per 100 training steps: 0.10564796330462956\n",
      "Training loss per 100 training steps: 0.10619102035906919\n",
      "Training loss per 100 training steps: 0.10582275610201491\n",
      "Training loss per 100 training steps: 0.10364540069427546\n",
      "Training loss per 100 training steps: 0.10190858066576226\n",
      "Training loss per 100 training steps: 0.10255869158593642\n",
      "Training loss per 100 training steps: 0.1032190190795623\n",
      "Training loss per 100 training steps: 0.10291895588216084\n",
      "Training loss per 100 training steps: 0.10340495770424943\n",
      "Training loss per 100 training steps: 0.10328206410910702\n",
      "Training loss per 100 training steps: 0.10203696212922433\n",
      "Training loss per 100 training steps: 0.10310529473455068\n",
      "Training loss per 100 training steps: 0.1030643909644065\n",
      "Training loss per 100 training steps: 0.10340154781061539\n",
      "Training loss per 100 training steps: 0.10360379044381549\n",
      "Training loss per 100 training steps: 0.10375841582133985\n",
      "Training loss per 100 training steps: 0.10354113456489875\n",
      "Training loss per 100 training steps: 0.10370641131617728\n",
      "Training loss per 100 training steps: 0.10385033172018461\n",
      "Training loss per 100 training steps: 0.10393295443084925\n",
      "Training loss per 100 training steps: 0.10368467283466332\n",
      "Training loss per 100 training steps: 0.1037411249102307\n",
      "Training loss per 100 training steps: 0.10384727541788495\n",
      "Training loss per 100 training steps: 0.10400536010451457\n",
      "Training loss per 100 training steps: 0.10415724909618519\n",
      "Training loss per 100 training steps: 0.10456828133115642\n",
      "Training loss per 100 training steps: 0.10527941115410476\n",
      "Training loss per 100 training steps: 0.10586077371092066\n",
      "Training loss per 100 training steps: 0.10558795322484185\n",
      "Training loss per 100 training steps: 0.10563371637027265\n",
      "Training loss per 100 training steps: 0.10559787457908727\n",
      "Training loss per 100 training steps: 0.10601128756718464\n",
      "Training loss per 100 training steps: 0.10607788883180481\n",
      "Training loss per 100 training steps: 0.10626577768036183\n",
      "Training loss per 100 training steps: 0.10623515552673282\n",
      "Training loss per 100 training steps: 0.10653636356905373\n",
      "Training loss per 100 training steps: 0.10642973256269476\n",
      "Training loss per 100 training steps: 0.10616399332977808\n",
      "Training loss per 100 training steps: 0.1059098585132465\n",
      "Training loss per 100 training steps: 0.10625800096013771\n",
      "Training loss per 100 training steps: 0.10641390269795868\n",
      "Training loss per 100 training steps: 0.10639682156176655\n",
      "Training loss per 100 training steps: 0.1062705899946519\n",
      "Training loss per 100 training steps: 0.10649899673425317\n",
      "Training loss per 100 training steps: 0.10651479318573155\n",
      "Training loss per 100 training steps: 0.10661870026501225\n",
      "Training loss per 100 training steps: 0.10688972968050982\n",
      "Training loss per 100 training steps: 0.10671865534677229\n",
      "Training loss per 100 training steps: 0.10672747576727928\n",
      "Training loss per 100 training steps: 0.10700688321269451\n",
      "Training loss per 100 training steps: 0.10708650491356574\n",
      "Training loss per 100 training steps: 0.10713115302480963\n",
      "Training loss per 100 training steps: 0.10734543593182558\n",
      "Training loss per 100 training steps: 0.10724932425463374\n",
      "Training loss per 100 training steps: 0.10732509042645241\n",
      "Training loss per 100 training steps: 0.10741386832117102\n",
      "Training loss per 100 training steps: 0.10717815871072223\n",
      "Training loss per 100 training steps: 0.10693353456127666\n",
      "Training loss per 100 training steps: 0.1070209110190506\n",
      "Training loss per 100 training steps: 0.10720667558085546\n",
      "Training loss epoch: 0.10730059286722816\n",
      "Training accuracy epoch: 0.9638858004568266\n",
      "Training epoch: 18\n",
      "Training loss per 100 training steps: 0.020994272083044052\n",
      "Training loss per 100 training steps: 0.09051315233314244\n",
      "Training loss per 100 training steps: 0.08802208689676218\n",
      "Training loss per 100 training steps: 0.0859160764634832\n",
      "Training loss per 100 training steps: 0.0863872249285572\n",
      "Training loss per 100 training steps: 0.08834785652791192\n",
      "Training loss per 100 training steps: 0.08732971558641772\n",
      "Training loss per 100 training steps: 0.08753782829878248\n",
      "Training loss per 100 training steps: 0.08885943940834848\n",
      "Training loss per 100 training steps: 0.09096293381030789\n",
      "Training loss per 100 training steps: 0.09030866582801642\n",
      "Training loss per 100 training steps: 0.09056231938915786\n",
      "Training loss per 100 training steps: 0.09095788665009596\n",
      "Training loss per 100 training steps: 0.09117406921630881\n",
      "Training loss per 100 training steps: 0.09102520303216383\n",
      "Training loss per 100 training steps: 0.09063109513424923\n",
      "Training loss per 100 training steps: 0.09230973088935246\n",
      "Training loss per 100 training steps: 0.09349418868697294\n",
      "Training loss per 100 training steps: 0.09448118607988318\n",
      "Training loss per 100 training steps: 0.09475845036288916\n",
      "Training loss per 100 training steps: 0.0945581141655668\n",
      "Training loss per 100 training steps: 0.09385224354722897\n",
      "Training loss per 100 training steps: 0.0941069222419061\n",
      "Training loss per 100 training steps: 0.09420839888739867\n",
      "Training loss per 100 training steps: 0.09397324188596885\n",
      "Training loss per 100 training steps: 0.09409596161443556\n",
      "Training loss per 100 training steps: 0.09364370595238049\n",
      "Training loss per 100 training steps: 0.09375409729617053\n",
      "Training loss per 100 training steps: 0.09419716678345985\n",
      "Training loss per 100 training steps: 0.09440740535312783\n",
      "Training loss per 100 training steps: 0.09444085976472935\n",
      "Training loss per 100 training steps: 0.09480519141208696\n",
      "Training loss per 100 training steps: 0.0950994608278326\n",
      "Training loss per 100 training steps: 0.09505684739459781\n",
      "Training loss per 100 training steps: 0.09501874081247427\n",
      "Training loss per 100 training steps: 0.09510190940982913\n",
      "Training loss per 100 training steps: 0.09461640804558341\n",
      "Training loss per 100 training steps: 0.09476110533915835\n",
      "Training loss per 100 training steps: 0.09490745259718163\n",
      "Training loss per 100 training steps: 0.09495718340387999\n",
      "Training loss per 100 training steps: 0.09511730927370872\n",
      "Training loss per 100 training steps: 0.09555226395021778\n",
      "Training loss per 100 training steps: 0.09550647098890075\n",
      "Training loss per 100 training steps: 0.0955142932353397\n",
      "Training loss per 100 training steps: 0.09552047389789686\n",
      "Training loss per 100 training steps: 0.09594588030920706\n",
      "Training loss per 100 training steps: 0.09603964170293064\n",
      "Training loss per 100 training steps: 0.09590527290242398\n",
      "Training loss per 100 training steps: 0.09610836333564178\n",
      "Training loss per 100 training steps: 0.09626269859569274\n",
      "Training loss per 100 training steps: 0.09629518925428347\n",
      "Training loss per 100 training steps: 0.0964768744664717\n",
      "Training loss per 100 training steps: 0.09657455164214127\n",
      "Training loss per 100 training steps: 0.09672921638733939\n",
      "Training loss per 100 training steps: 0.09690404607355009\n",
      "Training loss per 100 training steps: 0.09714483934348976\n",
      "Training loss per 100 training steps: 0.09721109773853115\n",
      "Training loss per 100 training steps: 0.09718271930660537\n",
      "Training loss per 100 training steps: 0.09718496146450682\n",
      "Training loss per 100 training steps: 0.0973279316268607\n",
      "Training loss per 100 training steps: 0.09759670496960784\n",
      "Training loss per 100 training steps: 0.09769614549220759\n",
      "Training loss per 100 training steps: 0.09795625248195931\n",
      "Training loss epoch: 0.09798854601850304\n",
      "Training accuracy epoch: 0.9669783356261169\n",
      "Training epoch: 19\n",
      "Training loss per 100 training steps: 0.08757888525724411\n",
      "Training loss per 100 training steps: 0.0773819362154544\n",
      "Training loss per 100 training steps: 0.07901992513543338\n",
      "Training loss per 100 training steps: 0.08100888963039977\n",
      "Training loss per 100 training steps: 0.08297737265479498\n",
      "Training loss per 100 training steps: 0.0824189471804022\n",
      "Training loss per 100 training steps: 0.08370870903456668\n",
      "Training loss per 100 training steps: 0.08234918158243645\n",
      "Training loss per 100 training steps: 0.08332662449884559\n",
      "Training loss per 100 training steps: 0.08360008846324446\n",
      "Training loss per 100 training steps: 0.08487239722315999\n",
      "Training loss per 100 training steps: 0.08508528079511894\n",
      "Training loss per 100 training steps: 0.0850514701804492\n",
      "Training loss per 100 training steps: 0.08594178056060196\n",
      "Training loss per 100 training steps: 0.08601508650720667\n",
      "Training loss per 100 training steps: 0.08791582903708099\n",
      "Training loss per 100 training steps: 0.0882523091145495\n",
      "Training loss per 100 training steps: 0.08870339430176019\n",
      "Training loss per 100 training steps: 0.08896821101432949\n",
      "Training loss per 100 training steps: 0.08869055262129893\n",
      "Training loss per 100 training steps: 0.08857984358837166\n",
      "Training loss per 100 training steps: 0.08851727914352808\n",
      "Training loss per 100 training steps: 0.08879802592011124\n",
      "Training loss per 100 training steps: 0.08850384815389623\n",
      "Training loss per 100 training steps: 0.08886841731853876\n",
      "Training loss per 100 training steps: 0.08886561132246916\n",
      "Training loss per 100 training steps: 0.08934373271892185\n",
      "Training loss per 100 training steps: 0.08993301926477598\n",
      "Training loss per 100 training steps: 0.089907699062345\n",
      "Training loss per 100 training steps: 0.09006882075777985\n",
      "Training loss per 100 training steps: 0.08960650741418726\n",
      "Training loss per 100 training steps: 0.0897319331903777\n",
      "Training loss per 100 training steps: 0.0897931894190727\n",
      "Training loss per 100 training steps: 0.09037831631617187\n",
      "Training loss per 100 training steps: 0.09108522350289765\n",
      "Training loss per 100 training steps: 0.09119951939541486\n",
      "Training loss per 100 training steps: 0.09093479565975805\n",
      "Training loss per 100 training steps: 0.09088625318345092\n",
      "Training loss per 100 training steps: 0.09074908365021143\n",
      "Training loss per 100 training steps: 0.09095346096763901\n",
      "Training loss per 100 training steps: 0.09065912264848248\n",
      "Training loss per 100 training steps: 0.09052144688497561\n",
      "Training loss per 100 training steps: 0.09060343797446109\n",
      "Training loss per 100 training steps: 0.09079116698327633\n",
      "Training loss per 100 training steps: 0.09107184954350732\n",
      "Training loss per 100 training steps: 0.09123868446828584\n",
      "Training loss per 100 training steps: 0.09179390389172669\n",
      "Training loss per 100 training steps: 0.09203364050114467\n",
      "Training loss per 100 training steps: 0.09175816020107572\n",
      "Training loss per 100 training steps: 0.09139408152155203\n",
      "Training loss per 100 training steps: 0.09167704666735464\n",
      "Training loss per 100 training steps: 0.09174059543077985\n",
      "Training loss per 100 training steps: 0.09162335412659524\n",
      "Training loss per 100 training steps: 0.09160668605487386\n",
      "Training loss per 100 training steps: 0.09184420888228186\n",
      "Training loss per 100 training steps: 0.09201723336081473\n",
      "Training loss per 100 training steps: 0.0921159691620938\n",
      "Training loss per 100 training steps: 0.09200197078133715\n",
      "Training loss per 100 training steps: 0.09199742645434965\n",
      "Training loss per 100 training steps: 0.09197205215565629\n",
      "Training loss per 100 training steps: 0.09232153314252835\n",
      "Training loss per 100 training steps: 0.09228717564638607\n",
      "Training loss per 100 training steps: 0.09246362886173656\n",
      "Training loss epoch: 0.09251651805216395\n",
      "Training accuracy epoch: 0.9690331127159127\n",
      "Training epoch: 20\n",
      "Training loss per 100 training steps: 0.0359296090900898\n",
      "Training loss per 100 training steps: 0.07309873385059804\n",
      "Training loss per 100 training steps: 0.07721280756485255\n",
      "Training loss per 100 training steps: 0.08037520869112144\n",
      "Training loss per 100 training steps: 0.08131409810967465\n",
      "Training loss per 100 training steps: 0.08086687790884259\n",
      "Training loss per 100 training steps: 0.07888586391011461\n",
      "Training loss per 100 training steps: 0.07853893245093407\n",
      "Training loss per 100 training steps: 0.07762484491794613\n",
      "Training loss per 100 training steps: 0.07816344931397119\n",
      "Training loss per 100 training steps: 0.07767435626362215\n",
      "Training loss per 100 training steps: 0.07828507238159889\n",
      "Training loss per 100 training steps: 0.07877535807518607\n",
      "Training loss per 100 training steps: 0.07951755596641208\n",
      "Training loss per 100 training steps: 0.08036939293507761\n",
      "Training loss per 100 training steps: 0.08115501341215775\n",
      "Training loss per 100 training steps: 0.08113437393118281\n",
      "Training loss per 100 training steps: 0.08191135963496217\n",
      "Training loss per 100 training steps: 0.08321077426350461\n",
      "Training loss per 100 training steps: 0.08313184694094541\n",
      "Training loss per 100 training steps: 0.08281356945488674\n",
      "Training loss per 100 training steps: 0.08280454563470765\n",
      "Training loss per 100 training steps: 0.08349840825331355\n",
      "Training loss per 100 training steps: 0.08310187584852095\n",
      "Training loss per 100 training steps: 0.08331455989352511\n",
      "Training loss per 100 training steps: 0.0828921750710323\n",
      "Training loss per 100 training steps: 0.08291912678219536\n",
      "Training loss per 100 training steps: 0.08280710347787426\n",
      "Training loss per 100 training steps: 0.08267168016724889\n",
      "Training loss per 100 training steps: 0.08273719430012486\n",
      "Training loss per 100 training steps: 0.08276277524512231\n",
      "Training loss per 100 training steps: 0.0832143952660545\n",
      "Training loss per 100 training steps: 0.08337139333490665\n",
      "Training loss per 100 training steps: 0.08351936386704899\n",
      "Training loss per 100 training steps: 0.08349181702602858\n",
      "Training loss per 100 training steps: 0.08340911496630414\n",
      "Training loss per 100 training steps: 0.08400180737872812\n",
      "Training loss per 100 training steps: 0.08400203667094659\n",
      "Training loss per 100 training steps: 0.08371245749465046\n",
      "Training loss per 100 training steps: 0.083794558347544\n",
      "Training loss per 100 training steps: 0.08331105255783004\n",
      "Training loss per 100 training steps: 0.08336054048477337\n",
      "Training loss per 100 training steps: 0.08365055711761914\n",
      "Training loss per 100 training steps: 0.08366201696442117\n",
      "Training loss per 100 training steps: 0.08386291762477668\n",
      "Training loss per 100 training steps: 0.08381712561905909\n",
      "Training loss per 100 training steps: 0.08368553599629934\n",
      "Training loss per 100 training steps: 0.08370348528786103\n",
      "Training loss per 100 training steps: 0.08369406409235933\n",
      "Training loss per 100 training steps: 0.08391508822820752\n",
      "Training loss per 100 training steps: 0.08403342266775998\n",
      "Training loss per 100 training steps: 0.08415155553095002\n",
      "Training loss per 100 training steps: 0.08418024997094385\n",
      "Training loss per 100 training steps: 0.08401217774273341\n",
      "Training loss per 100 training steps: 0.0839381657788329\n",
      "Training loss per 100 training steps: 0.08401605363682467\n",
      "Training loss per 100 training steps: 0.08415755183295305\n",
      "Training loss per 100 training steps: 0.08416424902196672\n",
      "Training loss per 100 training steps: 0.08441812319635614\n",
      "Training loss per 100 training steps: 0.08459737600071031\n",
      "Training loss per 100 training steps: 0.08503009590614849\n",
      "Training loss per 100 training steps: 0.08506637030130612\n",
      "Training loss per 100 training steps: 0.08522322815426903\n",
      "Training loss epoch: 0.08518779074468474\n",
      "Training accuracy epoch: 0.9715503854058333\n",
      "Training epoch: 21\n",
      "Training loss per 100 training steps: 0.059049706906080246\n",
      "Training loss per 100 training steps: 0.07468106218217181\n",
      "Training loss per 100 training steps: 0.07882626748305574\n",
      "Training loss per 100 training steps: 0.07587578349384755\n",
      "Training loss per 100 training steps: 0.07835414704107249\n",
      "Training loss per 100 training steps: 0.07972742052481113\n",
      "Training loss per 100 training steps: 0.07795301417257643\n",
      "Training loss per 100 training steps: 0.07626392030005692\n",
      "Training loss per 100 training steps: 0.0757620686536699\n",
      "Training loss per 100 training steps: 0.07590981665456029\n",
      "Training loss per 100 training steps: 0.07583810946974832\n",
      "Training loss per 100 training steps: 0.07491724339159854\n",
      "Training loss per 100 training steps: 0.07450831035360675\n",
      "Training loss per 100 training steps: 0.07500924621021474\n",
      "Training loss per 100 training steps: 0.0746327449140012\n",
      "Training loss per 100 training steps: 0.0742947588547373\n",
      "Training loss per 100 training steps: 0.07473175890409538\n",
      "Training loss per 100 training steps: 0.07485522393996541\n",
      "Training loss per 100 training steps: 0.07501989128402921\n",
      "Training loss per 100 training steps: 0.07516250607190406\n",
      "Training loss per 100 training steps: 0.07509846304098133\n",
      "Training loss per 100 training steps: 0.07490374184113177\n",
      "Training loss per 100 training steps: 0.0750956552673655\n",
      "Training loss per 100 training steps: 0.07549107404226807\n",
      "Training loss per 100 training steps: 0.07556049240784474\n",
      "Training loss per 100 training steps: 0.07551747542598926\n",
      "Training loss per 100 training steps: 0.07546133065522104\n",
      "Training loss per 100 training steps: 0.07568018982900318\n",
      "Training loss per 100 training steps: 0.07588851717879257\n",
      "Training loss per 100 training steps: 0.0759176735161723\n",
      "Training loss per 100 training steps: 0.07603492903472252\n",
      "Training loss per 100 training steps: 0.07626448604065032\n",
      "Training loss per 100 training steps: 0.07650141855834185\n",
      "Training loss per 100 training steps: 0.07637098974447276\n",
      "Training loss per 100 training steps: 0.07650365344628252\n",
      "Training loss per 100 training steps: 0.07667420163115668\n",
      "Training loss per 100 training steps: 0.07654538985024449\n",
      "Training loss per 100 training steps: 0.07655944707511525\n",
      "Training loss per 100 training steps: 0.07659245127748568\n",
      "Training loss per 100 training steps: 0.07704361081828827\n",
      "Training loss per 100 training steps: 0.07712777405440709\n",
      "Training loss per 100 training steps: 0.07729808049521962\n",
      "Training loss per 100 training steps: 0.0772564667975918\n",
      "Training loss per 100 training steps: 0.07753606481046495\n",
      "Training loss per 100 training steps: 0.07756710769112377\n",
      "Training loss per 100 training steps: 0.07750372439381824\n",
      "Training loss per 100 training steps: 0.07725999395988907\n",
      "Training loss per 100 training steps: 0.07728911273693953\n",
      "Training loss per 100 training steps: 0.07703395295880232\n",
      "Training loss per 100 training steps: 0.07704212072685815\n",
      "Training loss per 100 training steps: 0.07700558577405713\n",
      "Training loss per 100 training steps: 0.07693456510099962\n",
      "Training loss per 100 training steps: 0.07722635742659571\n",
      "Training loss per 100 training steps: 0.07752491039620321\n",
      "Training loss per 100 training steps: 0.077721991189008\n",
      "Training loss per 100 training steps: 0.07776369320135072\n",
      "Training loss per 100 training steps: 0.07782797221417115\n",
      "Training loss per 100 training steps: 0.07779124875527141\n",
      "Training loss per 100 training steps: 0.07803630641641676\n",
      "Training loss per 100 training steps: 0.07836884955868158\n",
      "Training loss per 100 training steps: 0.07830278892723425\n",
      "Training loss per 100 training steps: 0.07819433701033049\n",
      "Training loss per 100 training steps: 0.07836176333600531\n",
      "Training loss epoch: 0.07846173818379046\n",
      "Training accuracy epoch: 0.9739006470606435\n",
      "Training epoch: 22\n",
      "Training loss per 100 training steps: 0.056974902749061584\n",
      "Training loss per 100 training steps: 0.07480362067828969\n",
      "Training loss per 100 training steps: 0.06436586124238683\n",
      "Training loss per 100 training steps: 0.06557206264650604\n",
      "Training loss per 100 training steps: 0.06756832984338823\n",
      "Training loss per 100 training steps: 0.068329856515362\n",
      "Training loss per 100 training steps: 0.06966669231919749\n",
      "Training loss per 100 training steps: 0.07130100033823905\n",
      "Training loss per 100 training steps: 0.07167548827446774\n",
      "Training loss per 100 training steps: 0.07217450813776902\n",
      "Training loss per 100 training steps: 0.07313946786115168\n",
      "Training loss per 100 training steps: 0.07358230378197085\n",
      "Training loss per 100 training steps: 0.07329364743166689\n",
      "Training loss per 100 training steps: 0.07354141032086295\n",
      "Training loss per 100 training steps: 0.07333641761244487\n",
      "Training loss per 100 training steps: 0.07302572752341634\n",
      "Training loss per 100 training steps: 0.07321351131806493\n",
      "Training loss per 100 training steps: 0.07283847306735665\n",
      "Training loss per 100 training steps: 0.07298876479154826\n",
      "Training loss per 100 training steps: 0.07333436510687315\n",
      "Training loss per 100 training steps: 0.07324908006489966\n",
      "Training loss per 100 training steps: 0.07315699821606239\n",
      "Training loss per 100 training steps: 0.07343784452548022\n",
      "Training loss per 100 training steps: 0.07341292478164002\n",
      "Training loss per 100 training steps: 0.07339923959394148\n",
      "Training loss per 100 training steps: 0.07337732624462912\n",
      "Training loss per 100 training steps: 0.07366589134915666\n",
      "Training loss per 100 training steps: 0.07373173850023362\n",
      "Training loss per 100 training steps: 0.07371126151929973\n",
      "Training loss per 100 training steps: 0.07351368393313033\n",
      "Training loss per 100 training steps: 0.07318400932951982\n",
      "Training loss per 100 training steps: 0.07310265822048362\n",
      "Training loss per 100 training steps: 0.07298417575861031\n",
      "Training loss per 100 training steps: 0.07300262050607242\n",
      "Training loss per 100 training steps: 0.07331353878400841\n",
      "Training loss per 100 training steps: 0.07396543548918326\n",
      "Training loss per 100 training steps: 0.07444812790124727\n",
      "Training loss per 100 training steps: 0.07442275241761695\n",
      "Training loss per 100 training steps: 0.07470832337958308\n",
      "Training loss per 100 training steps: 0.07466298875232294\n",
      "Training loss per 100 training steps: 0.0747784771554389\n",
      "Training loss per 100 training steps: 0.07505444210197325\n",
      "Training loss per 100 training steps: 0.07569946472309583\n",
      "Training loss per 100 training steps: 0.07574003058889743\n",
      "Training loss per 100 training steps: 0.07599724631912505\n",
      "Training loss per 100 training steps: 0.0758243160305445\n",
      "Training loss per 100 training steps: 0.07560358169631111\n",
      "Training loss per 100 training steps: 0.0757894381789926\n",
      "Training loss per 100 training steps: 0.07577990139566883\n",
      "Training loss per 100 training steps: 0.07570256217835303\n",
      "Training loss per 100 training steps: 0.07573639203864509\n",
      "Training loss per 100 training steps: 0.07574461965729117\n",
      "Training loss per 100 training steps: 0.07579514547918186\n",
      "Training loss per 100 training steps: 0.07578260912156723\n",
      "Training loss per 100 training steps: 0.07573208867873492\n",
      "Training loss per 100 training steps: 0.07572805373293717\n",
      "Training loss per 100 training steps: 0.07571699150462669\n",
      "Training loss per 100 training steps: 0.07573329044915664\n",
      "Training loss per 100 training steps: 0.07590466641973967\n",
      "Training loss per 100 training steps: 0.07599932615233465\n",
      "Training loss per 100 training steps: 0.07598824087505388\n",
      "Training loss per 100 training steps: 0.07607978500244619\n",
      "Training loss per 100 training steps: 0.07624273582647298\n",
      "Training loss epoch: 0.07629403078848118\n",
      "Training accuracy epoch: 0.9742621323293587\n",
      "Training epoch: 23\n",
      "Training loss per 100 training steps: 0.0427730418741703\n",
      "Training loss per 100 training steps: 0.07047756457710547\n",
      "Training loss per 100 training steps: 0.06722149976983259\n",
      "Training loss per 100 training steps: 0.06830162572538313\n",
      "Training loss per 100 training steps: 0.06601397358891357\n",
      "Training loss per 100 training steps: 0.06794845780307528\n",
      "Training loss per 100 training steps: 0.0684504334976782\n",
      "Training loss per 100 training steps: 0.06957869907507618\n",
      "Training loss per 100 training steps: 0.06843627884794497\n",
      "Training loss per 100 training steps: 0.06861703301217618\n",
      "Training loss per 100 training steps: 0.06838057095210043\n",
      "Training loss per 100 training steps: 0.06687960227706946\n",
      "Training loss per 100 training steps: 0.06623727301283772\n",
      "Training loss per 100 training steps: 0.06598628077532188\n",
      "Training loss per 100 training steps: 0.06649851569221843\n",
      "Training loss per 100 training steps: 0.06608615867588186\n",
      "Training loss per 100 training steps: 0.06610285268871371\n",
      "Training loss per 100 training steps: 0.06561761451181548\n",
      "Training loss per 100 training steps: 0.06614723039011697\n",
      "Training loss per 100 training steps: 0.06585082776052315\n",
      "Training loss per 100 training steps: 0.0661966592062503\n",
      "Training loss per 100 training steps: 0.06683251705873529\n",
      "Training loss per 100 training steps: 0.06667236448847425\n",
      "Training loss per 100 training steps: 0.06661783790022381\n",
      "Training loss per 100 training steps: 0.06643919409380707\n",
      "Training loss per 100 training steps: 0.06629826530048186\n",
      "Training loss per 100 training steps: 0.06674870660251758\n",
      "Training loss per 100 training steps: 0.06690650548941433\n",
      "Training loss per 100 training steps: 0.06707970378563002\n",
      "Training loss per 100 training steps: 0.0673798894420907\n",
      "Training loss per 100 training steps: 0.06772770079249074\n",
      "Training loss per 100 training steps: 0.06783353151277222\n",
      "Training loss per 100 training steps: 0.06804101406857\n",
      "Training loss per 100 training steps: 0.06844612290649382\n",
      "Training loss per 100 training steps: 0.06844513401853551\n",
      "Training loss per 100 training steps: 0.06845057113864905\n",
      "Training loss per 100 training steps: 0.06847904807034755\n",
      "Training loss per 100 training steps: 0.0690990392729173\n",
      "Training loss per 100 training steps: 0.0690025252181526\n",
      "Training loss per 100 training steps: 0.06902770460250228\n",
      "Training loss per 100 training steps: 0.06918263358583776\n",
      "Training loss per 100 training steps: 0.06940870703829571\n",
      "Training loss per 100 training steps: 0.06945534065617189\n",
      "Training loss per 100 training steps: 0.06930888651962652\n",
      "Training loss per 100 training steps: 0.06953939339715265\n",
      "Training loss per 100 training steps: 0.0699722057588521\n",
      "Training loss per 100 training steps: 0.0702251280394602\n",
      "Training loss per 100 training steps: 0.07055591853728553\n",
      "Training loss per 100 training steps: 0.07065662691719601\n",
      "Training loss per 100 training steps: 0.07077770026315554\n",
      "Training loss per 100 training steps: 0.07094605613439915\n",
      "Training loss per 100 training steps: 0.07102903247735314\n",
      "Training loss per 100 training steps: 0.07086692378332295\n",
      "Training loss per 100 training steps: 0.07120800163354243\n",
      "Training loss per 100 training steps: 0.07117139049721208\n",
      "Training loss per 100 training steps: 0.07139273317449656\n",
      "Training loss per 100 training steps: 0.07129209053219117\n",
      "Training loss per 100 training steps: 0.07133523767977891\n",
      "Training loss per 100 training steps: 0.07125384577917963\n",
      "Training loss per 100 training steps: 0.07111149789470188\n",
      "Training loss per 100 training steps: 0.07130212405498794\n",
      "Training loss per 100 training steps: 0.0711630634698129\n",
      "Training loss per 100 training steps: 0.07128827916155618\n",
      "Training loss epoch: 0.07126401276941285\n",
      "Training accuracy epoch: 0.9763244676042894\n",
      "Training epoch: 24\n",
      "Training loss per 100 training steps: 0.020537249743938446\n",
      "Training loss per 100 training steps: 0.0625078606253287\n",
      "Training loss per 100 training steps: 0.06046076641482908\n",
      "Training loss per 100 training steps: 0.05961183868534244\n",
      "Training loss per 100 training steps: 0.06250773102423626\n",
      "Training loss per 100 training steps: 0.06271178883773659\n",
      "Training loss per 100 training steps: 0.06338382981401464\n",
      "Training loss per 100 training steps: 0.06243130259452316\n",
      "Training loss per 100 training steps: 0.06346989651042015\n",
      "Training loss per 100 training steps: 0.06497311427544278\n",
      "Training loss per 100 training steps: 0.06486101539574794\n",
      "Training loss per 100 training steps: 0.06396221852422329\n",
      "Training loss per 100 training steps: 0.06297894296171198\n",
      "Training loss per 100 training steps: 0.06359470837111002\n",
      "Training loss per 100 training steps: 0.06385696045924735\n",
      "Training loss per 100 training steps: 0.06363797233859418\n",
      "Training loss per 100 training steps: 0.06404342692735711\n",
      "Training loss per 100 training steps: 0.06381026262799054\n",
      "Training loss per 100 training steps: 0.06332367854691869\n",
      "Training loss per 100 training steps: 0.06381811138545249\n",
      "Training loss per 100 training steps: 0.0643095812808698\n",
      "Training loss per 100 training steps: 0.06481523994291676\n",
      "Training loss per 100 training steps: 0.06481841372906559\n",
      "Training loss per 100 training steps: 0.06473909294506335\n",
      "Training loss per 100 training steps: 0.06457190907146022\n",
      "Training loss per 100 training steps: 0.06484178496153802\n",
      "Training loss per 100 training steps: 0.06456360319546599\n",
      "Training loss per 100 training steps: 0.06447731621619639\n",
      "Training loss per 100 training steps: 0.06425465429082855\n",
      "Training loss per 100 training steps: 0.06426892810373867\n",
      "Training loss per 100 training steps: 0.06405551077104249\n",
      "Training loss per 100 training steps: 0.06427718186064003\n",
      "Training loss per 100 training steps: 0.06446745011465108\n",
      "Training loss per 100 training steps: 0.06418598720342428\n",
      "Training loss per 100 training steps: 0.06437305506834033\n",
      "Training loss per 100 training steps: 0.06445560020637896\n",
      "Training loss per 100 training steps: 0.06462514155738441\n",
      "Training loss per 100 training steps: 0.06473956285084184\n",
      "Training loss per 100 training steps: 0.06491294540059982\n",
      "Training loss per 100 training steps: 0.06499336879682875\n",
      "Training loss per 100 training steps: 0.0648894552369836\n",
      "Training loss per 100 training steps: 0.0648329782165313\n",
      "Training loss per 100 training steps: 0.06481128965920248\n",
      "Training loss per 100 training steps: 0.06525537980808856\n",
      "Training loss per 100 training steps: 0.06545564393388369\n",
      "Training loss per 100 training steps: 0.06557468555787907\n",
      "Training loss per 100 training steps: 0.06532062812267757\n",
      "Training loss per 100 training steps: 0.06554823754939443\n",
      "Training loss per 100 training steps: 0.06569796527766053\n",
      "Training loss per 100 training steps: 0.06607928293773008\n",
      "Training loss per 100 training steps: 0.0665208861351036\n",
      "Training loss per 100 training steps: 0.06649226010815397\n",
      "Training loss per 100 training steps: 0.06662184047901999\n",
      "Training loss per 100 training steps: 0.06646501502264421\n",
      "Training loss per 100 training steps: 0.06631720143196515\n",
      "Training loss per 100 training steps: 0.06647780515041748\n",
      "Training loss per 100 training steps: 0.06653742939972886\n",
      "Training loss per 100 training steps: 0.06669742238385692\n",
      "Training loss per 100 training steps: 0.06674454946552064\n",
      "Training loss per 100 training steps: 0.06654201197223931\n",
      "Training loss per 100 training steps: 0.06642702962326963\n",
      "Training loss per 100 training steps: 0.06651819980975165\n",
      "Training loss per 100 training steps: 0.06641183300068462\n",
      "Training loss epoch: 0.06643485524516962\n",
      "Training accuracy epoch: 0.9781107698759798\n",
      "Training epoch: 25\n",
      "Training loss per 100 training steps: 0.02776559256017208\n",
      "Training loss per 100 training steps: 0.05611734776895973\n",
      "Training loss per 100 training steps: 0.05968876805073067\n",
      "Training loss per 100 training steps: 0.05964459035626034\n",
      "Training loss per 100 training steps: 0.059241074054940904\n",
      "Training loss per 100 training steps: 0.05902166960402178\n",
      "Training loss per 100 training steps: 0.060079786324092554\n",
      "Training loss per 100 training steps: 0.05906000130274433\n",
      "Training loss per 100 training steps: 0.05775109585839674\n",
      "Training loss per 100 training steps: 0.05761833345418285\n",
      "Training loss per 100 training steps: 0.05748273331121161\n",
      "Training loss per 100 training steps: 0.057477416637286775\n",
      "Training loss per 100 training steps: 0.05696369981204007\n",
      "Training loss per 100 training steps: 0.05679585025835493\n",
      "Training loss per 100 training steps: 0.057569126850894804\n",
      "Training loss per 100 training steps: 0.059001260923285614\n",
      "Training loss per 100 training steps: 0.06002134718339737\n",
      "Training loss per 100 training steps: 0.06187470722704007\n",
      "Training loss per 100 training steps: 0.06310015857440245\n",
      "Training loss per 100 training steps: 0.06266808960698732\n",
      "Training loss per 100 training steps: 0.06270806095673415\n",
      "Training loss per 100 training steps: 0.06310077840162008\n",
      "Training loss per 100 training steps: 0.0625772087032192\n",
      "Training loss per 100 training steps: 0.06284396294189713\n",
      "Training loss per 100 training steps: 0.06225117368016373\n",
      "Training loss per 100 training steps: 0.06192975402812472\n",
      "Training loss per 100 training steps: 0.06229037505957473\n",
      "Training loss per 100 training steps: 0.0627446080305345\n",
      "Training loss per 100 training steps: 0.0626996491902439\n",
      "Training loss per 100 training steps: 0.06280092053457752\n",
      "Training loss per 100 training steps: 0.06304177037878282\n",
      "Training loss per 100 training steps: 0.06309330383930908\n",
      "Training loss per 100 training steps: 0.06273493175269726\n",
      "Training loss per 100 training steps: 0.06261597816074553\n",
      "Training loss per 100 training steps: 0.06223556652857646\n",
      "Training loss per 100 training steps: 0.06185833253284293\n",
      "Training loss per 100 training steps: 0.06174013044309338\n",
      "Training loss per 100 training steps: 0.06191114977137275\n",
      "Training loss per 100 training steps: 0.06198573877839668\n",
      "Training loss per 100 training steps: 0.06198862662604062\n",
      "Training loss per 100 training steps: 0.06192011031172276\n",
      "Training loss per 100 training steps: 0.062112705585560646\n",
      "Training loss per 100 training steps: 0.06213696106932029\n",
      "Training loss per 100 training steps: 0.06196160025149108\n",
      "Training loss per 100 training steps: 0.06199059009879372\n",
      "Training loss per 100 training steps: 0.062161774397770785\n",
      "Training loss per 100 training steps: 0.06222793211768993\n",
      "Training loss per 100 training steps: 0.062350151966979905\n",
      "Training loss per 100 training steps: 0.06266286078511883\n",
      "Training loss per 100 training steps: 0.06261567389860466\n",
      "Training loss per 100 training steps: 0.06278781049825821\n",
      "Training loss per 100 training steps: 0.06286165170479983\n",
      "Training loss per 100 training steps: 0.06293962630328673\n",
      "Training loss per 100 training steps: 0.06305831174139187\n",
      "Training loss per 100 training steps: 0.06306010963103995\n",
      "Training loss per 100 training steps: 0.06299645603729903\n",
      "Training loss per 100 training steps: 0.06285171870901612\n",
      "Training loss per 100 training steps: 0.06280202437233859\n",
      "Training loss per 100 training steps: 0.062759512416234\n",
      "Training loss per 100 training steps: 0.06271478009944911\n",
      "Training loss per 100 training steps: 0.06274958831201317\n",
      "Training loss per 100 training steps: 0.06285364986406484\n",
      "Training loss per 100 training steps: 0.06291927050828719\n",
      "Training loss epoch: 0.06287642049885488\n",
      "Training accuracy epoch: 0.9790197643424338\n",
      "Training epoch: 26\n",
      "Training loss per 100 training steps: 0.010939649306237698\n",
      "Training loss per 100 training steps: 0.07461297824185821\n",
      "Training loss per 100 training steps: 0.06265313813546255\n",
      "Training loss per 100 training steps: 0.06626465150690405\n",
      "Training loss per 100 training steps: 0.07363477750493218\n",
      "Training loss per 100 training steps: 0.07096339853019444\n",
      "Training loss per 100 training steps: 0.06780745708360375\n",
      "Training loss per 100 training steps: 0.06693277109185826\n",
      "Training loss per 100 training steps: 0.06531449710048\n",
      "Training loss per 100 training steps: 0.06409100012090904\n",
      "Training loss per 100 training steps: 0.06333858435173388\n",
      "Training loss per 100 training steps: 0.06219172792730821\n",
      "Training loss per 100 training steps: 0.06264221210414225\n",
      "Training loss per 100 training steps: 0.06254641908535205\n",
      "Training loss per 100 training steps: 0.06227710726155735\n",
      "Training loss per 100 training steps: 0.061792857970249436\n",
      "Training loss per 100 training steps: 0.06134926731643534\n",
      "Training loss per 100 training steps: 0.0611367637981693\n",
      "Training loss per 100 training steps: 0.061792572855024805\n",
      "Training loss per 100 training steps: 0.06201049043467001\n",
      "Training loss per 100 training steps: 0.06185460877461663\n",
      "Training loss per 100 training steps: 0.06245718634750974\n",
      "Training loss per 100 training steps: 0.06269307040328245\n",
      "Training loss per 100 training steps: 0.06217242852949223\n",
      "Training loss per 100 training steps: 0.061803289086682825\n",
      "Training loss per 100 training steps: 0.06171297424671906\n",
      "Training loss per 100 training steps: 0.061635046844604836\n",
      "Training loss per 100 training steps: 0.061828222216582095\n",
      "Training loss per 100 training steps: 0.061456408850362895\n",
      "Training loss per 100 training steps: 0.0614355609981216\n",
      "Training loss per 100 training steps: 0.06128701692967967\n",
      "Training loss per 100 training steps: 0.061330246415671884\n",
      "Training loss per 100 training steps: 0.061804709239410144\n",
      "Training loss per 100 training steps: 0.06168953283911814\n",
      "Training loss per 100 training steps: 0.06170825772491984\n",
      "Training loss per 100 training steps: 0.06176258429225324\n",
      "Training loss per 100 training steps: 0.06186760407570549\n",
      "Training loss per 100 training steps: 0.06175469361346666\n",
      "Training loss per 100 training steps: 0.06155754683686406\n",
      "Training loss per 100 training steps: 0.061344346733231664\n",
      "Training loss per 100 training steps: 0.061114060908327404\n",
      "Training loss per 100 training steps: 0.06123660030322623\n",
      "Training loss per 100 training steps: 0.06145266759511056\n",
      "Training loss per 100 training steps: 0.061753404794022235\n",
      "Training loss per 100 training steps: 0.061516969534658426\n",
      "Training loss per 100 training steps: 0.06130674345666281\n",
      "Training loss per 100 training steps: 0.06142224552136354\n",
      "Training loss per 100 training steps: 0.061369341122912875\n",
      "Training loss per 100 training steps: 0.061338451770360264\n",
      "Training loss per 100 training steps: 0.06142840752309887\n",
      "Training loss per 100 training steps: 0.061546638981690396\n",
      "Training loss per 100 training steps: 0.06148991694069933\n",
      "Training loss per 100 training steps: 0.061450173278904875\n",
      "Training loss per 100 training steps: 0.061398015719559274\n",
      "Training loss per 100 training steps: 0.06128389753695799\n",
      "Training loss per 100 training steps: 0.06133868924285888\n",
      "Training loss per 100 training steps: 0.06123631250663961\n",
      "Training loss per 100 training steps: 0.0613154643599138\n",
      "Training loss per 100 training steps: 0.06129730676044433\n",
      "Training loss per 100 training steps: 0.06142656621424561\n",
      "Training loss per 100 training steps: 0.06141563034493031\n",
      "Training loss per 100 training steps: 0.061323073809397666\n",
      "Training loss per 100 training steps: 0.06126526008606248\n",
      "Training loss epoch: 0.0614037248083522\n",
      "Training accuracy epoch: 0.9796123682787661\n",
      "Training epoch: 27\n",
      "Training loss per 100 training steps: 0.05289750546216965\n",
      "Training loss per 100 training steps: 0.0509624864265212\n",
      "Training loss per 100 training steps: 0.053682852380061104\n",
      "Training loss per 100 training steps: 0.05231151481134279\n",
      "Training loss per 100 training steps: 0.05213990577199308\n",
      "Training loss per 100 training steps: 0.05338147264847647\n",
      "Training loss per 100 training steps: 0.05453776822234822\n",
      "Training loss per 100 training steps: 0.05398227903558826\n",
      "Training loss per 100 training steps: 0.053753118970188384\n",
      "Training loss per 100 training steps: 0.05374852725233341\n",
      "Training loss per 100 training steps: 0.053632349470401415\n",
      "Training loss per 100 training steps: 0.05375248073096083\n",
      "Training loss per 100 training steps: 0.053760693649134036\n",
      "Training loss per 100 training steps: 0.05403626852201371\n",
      "Training loss per 100 training steps: 0.054714651565286175\n",
      "Training loss per 100 training steps: 0.05501384475453944\n",
      "Training loss per 100 training steps: 0.055949884309182685\n",
      "Training loss per 100 training steps: 0.05562127972317905\n",
      "Training loss per 100 training steps: 0.05534891835786195\n",
      "Training loss per 100 training steps: 0.055464140388027994\n",
      "Training loss per 100 training steps: 0.05535751422655409\n",
      "Training loss per 100 training steps: 0.055609122148516814\n",
      "Training loss per 100 training steps: 0.05549989723774356\n",
      "Training loss per 100 training steps: 0.055411968929454304\n",
      "Training loss per 100 training steps: 0.05516180456416221\n",
      "Training loss per 100 training steps: 0.0549343950830114\n",
      "Training loss per 100 training steps: 0.05473268553557326\n",
      "Training loss per 100 training steps: 0.05448293297764318\n",
      "Training loss per 100 training steps: 0.05492000056623379\n",
      "Training loss per 100 training steps: 0.05500583793024601\n",
      "Training loss per 100 training steps: 0.05561377780827123\n",
      "Training loss per 100 training steps: 0.05550291668845179\n",
      "Training loss per 100 training steps: 0.055365543901361405\n",
      "Training loss per 100 training steps: 0.055298130520942776\n",
      "Training loss per 100 training steps: 0.05528487909460778\n",
      "Training loss per 100 training steps: 0.055246956392496736\n",
      "Training loss per 100 training steps: 0.055123227651249614\n",
      "Training loss per 100 training steps: 0.055106838901057414\n",
      "Training loss per 100 training steps: 0.05521485651316456\n",
      "Training loss per 100 training steps: 0.055207717913529776\n",
      "Training loss per 100 training steps: 0.055298368441698886\n",
      "Training loss per 100 training steps: 0.055219335938385766\n",
      "Training loss per 100 training steps: 0.055578539504617955\n",
      "Training loss per 100 training steps: 0.055451217582176485\n",
      "Training loss per 100 training steps: 0.05535824163275132\n",
      "Training loss per 100 training steps: 0.05525746266688664\n",
      "Training loss per 100 training steps: 0.055639239789792974\n",
      "Training loss per 100 training steps: 0.05586736896572569\n",
      "Training loss per 100 training steps: 0.056026653773701286\n",
      "Training loss per 100 training steps: 0.05620423746757039\n",
      "Training loss per 100 training steps: 0.05618607466978964\n",
      "Training loss per 100 training steps: 0.056314814483231654\n",
      "Training loss per 100 training steps: 0.05642444008639538\n",
      "Training loss per 100 training steps: 0.05633680149978717\n",
      "Training loss per 100 training steps: 0.05627672873832109\n",
      "Training loss per 100 training steps: 0.056335327332775714\n",
      "Training loss per 100 training steps: 0.05627521818301377\n",
      "Training loss per 100 training steps: 0.056611586017717104\n",
      "Training loss per 100 training steps: 0.05659075420303439\n",
      "Training loss per 100 training steps: 0.05665773811026962\n",
      "Training loss per 100 training steps: 0.05683009398945478\n",
      "Training loss per 100 training steps: 0.05682952065999139\n",
      "Training loss per 100 training steps: 0.05686699535441326\n",
      "Training loss epoch: 0.05682843638521014\n",
      "Training accuracy epoch: 0.9812158383712358\n",
      "Training epoch: 28\n",
      "Training loss per 100 training steps: 0.013686569407582283\n",
      "Training loss per 100 training steps: 0.05495445939509364\n",
      "Training loss per 100 training steps: 0.05469746558593391\n",
      "Training loss per 100 training steps: 0.05425672870024147\n",
      "Training loss per 100 training steps: 0.05251720490426457\n",
      "Training loss per 100 training steps: 0.05462613139913126\n",
      "Training loss per 100 training steps: 0.053836229781445095\n",
      "Training loss per 100 training steps: 0.05405875154739025\n",
      "Training loss per 100 training steps: 0.05349657145013728\n",
      "Training loss per 100 training steps: 0.05393141365613164\n",
      "Training loss per 100 training steps: 0.0550963508000562\n",
      "Training loss per 100 training steps: 0.05487976300523369\n",
      "Training loss per 100 training steps: 0.05402944492080019\n",
      "Training loss per 100 training steps: 0.054770037034515016\n",
      "Training loss per 100 training steps: 0.055328238854513136\n",
      "Training loss per 100 training steps: 0.05479165294991284\n",
      "Training loss per 100 training steps: 0.055052749501241095\n",
      "Training loss per 100 training steps: 0.055085322760929215\n",
      "Training loss per 100 training steps: 0.055895884339669216\n",
      "Training loss per 100 training steps: 0.055823396089614646\n",
      "Training loss per 100 training steps: 0.055508442058077624\n",
      "Training loss per 100 training steps: 0.055752269118919666\n",
      "Training loss per 100 training steps: 0.05512429447389753\n",
      "Training loss per 100 training steps: 0.055256571979207296\n",
      "Training loss per 100 training steps: 0.054935002415656256\n",
      "Training loss per 100 training steps: 0.05512797125944364\n",
      "Training loss per 100 training steps: 0.05495230029662946\n",
      "Training loss per 100 training steps: 0.054976227382330385\n",
      "Training loss per 100 training steps: 0.05491930286917717\n",
      "Training loss per 100 training steps: 0.054653969369945876\n",
      "Training loss per 100 training steps: 0.0544182084535625\n",
      "Training loss per 100 training steps: 0.054742622714688216\n",
      "Training loss per 100 training steps: 0.05495513493735029\n",
      "Training loss per 100 training steps: 0.05496023091086534\n",
      "Training loss per 100 training steps: 0.05495345733617699\n",
      "Training loss per 100 training steps: 0.05516146254711522\n",
      "Training loss per 100 training steps: 0.05539250145810487\n",
      "Training loss per 100 training steps: 0.05546895188796697\n",
      "Training loss per 100 training steps: 0.05524242808984142\n",
      "Training loss per 100 training steps: 0.05518193768351216\n",
      "Training loss per 100 training steps: 0.055299291335408486\n",
      "Training loss per 100 training steps: 0.05533931643325791\n",
      "Training loss per 100 training steps: 0.055446479785059004\n",
      "Training loss per 100 training steps: 0.05531072714013065\n",
      "Training loss per 100 training steps: 0.055574910597510886\n",
      "Training loss per 100 training steps: 0.055641327523573626\n",
      "Training loss per 100 training steps: 0.055490461096385266\n",
      "Training loss per 100 training steps: 0.0555075094655133\n",
      "Training loss per 100 training steps: 0.05550302063805092\n",
      "Training loss per 100 training steps: 0.05558397901200328\n",
      "Training loss per 100 training steps: 0.055583561057495554\n",
      "Training loss per 100 training steps: 0.055459075081619055\n",
      "Training loss per 100 training steps: 0.05528154503920072\n",
      "Training loss per 100 training steps: 0.05531188300596195\n",
      "Training loss per 100 training steps: 0.05521031881303347\n",
      "Training loss per 100 training steps: 0.05519781098272518\n",
      "Training loss per 100 training steps: 0.05508788256962965\n",
      "Training loss per 100 training steps: 0.05499288541368526\n",
      "Training loss per 100 training steps: 0.05497311228678782\n",
      "Training loss per 100 training steps: 0.05507075231731794\n",
      "Training loss per 100 training steps: 0.05521568666549072\n",
      "Training loss per 100 training steps: 0.05516788693275014\n",
      "Training loss per 100 training steps: 0.05527473833707842\n",
      "Training loss epoch: 0.05534388718538683\n",
      "Training accuracy epoch: 0.9817019679659683\n",
      "Training epoch: 29\n",
      "Training loss per 100 training steps: 0.023564618080854416\n",
      "Training loss per 100 training steps: 0.05405705347011733\n",
      "Training loss per 100 training steps: 0.0565484987701572\n",
      "Training loss per 100 training steps: 0.05704026257749214\n",
      "Training loss per 100 training steps: 0.05404149020901252\n",
      "Training loss per 100 training steps: 0.05270885115446609\n",
      "Training loss per 100 training steps: 0.05264838198904946\n",
      "Training loss per 100 training steps: 0.05274688190788551\n",
      "Training loss per 100 training steps: 0.05224039120054321\n",
      "Training loss per 100 training steps: 0.052647739610056934\n",
      "Training loss per 100 training steps: 0.05287339301016014\n",
      "Training loss per 100 training steps: 0.05280236692419893\n",
      "Training loss per 100 training steps: 0.053002123908216535\n",
      "Training loss per 100 training steps: 0.052194884167891066\n",
      "Training loss per 100 training steps: 0.05199285412189297\n",
      "Training loss per 100 training steps: 0.05184317321609954\n",
      "Training loss per 100 training steps: 0.05196723106457124\n",
      "Training loss per 100 training steps: 0.05161740511513727\n",
      "Training loss per 100 training steps: 0.051356071375710154\n",
      "Training loss per 100 training steps: 0.05123999517594466\n",
      "Training loss per 100 training steps: 0.051124435595538316\n",
      "Training loss per 100 training steps: 0.051404648746305306\n",
      "Training loss per 100 training steps: 0.05114429210202014\n",
      "Training loss per 100 training steps: 0.050962320199814656\n",
      "Training loss per 100 training steps: 0.05115264094779318\n",
      "Training loss per 100 training steps: 0.051096883329625406\n",
      "Training loss per 100 training steps: 0.05161025142346382\n",
      "Training loss per 100 training steps: 0.05201353613945446\n",
      "Training loss per 100 training steps: 0.05189627433594374\n",
      "Training loss per 100 training steps: 0.051784493876409245\n",
      "Training loss per 100 training steps: 0.051727939224393865\n",
      "Training loss per 100 training steps: 0.051996314479773924\n",
      "Training loss per 100 training steps: 0.05164300260205033\n",
      "Training loss per 100 training steps: 0.05152777582618598\n",
      "Training loss per 100 training steps: 0.05178753552448757\n",
      "Training loss per 100 training steps: 0.05179845787960131\n",
      "Training loss per 100 training steps: 0.052029806339955\n",
      "Training loss per 100 training steps: 0.05219971986156097\n",
      "Training loss per 100 training steps: 0.052184468312816236\n",
      "Training loss per 100 training steps: 0.052033759755032535\n",
      "Training loss per 100 training steps: 0.0520176206122068\n",
      "Training loss per 100 training steps: 0.05198090050530285\n",
      "Training loss per 100 training steps: 0.05175094422446446\n",
      "Training loss per 100 training steps: 0.0517173117506098\n",
      "Training loss per 100 training steps: 0.05189358307436676\n",
      "Training loss per 100 training steps: 0.051945951805726046\n",
      "Training loss per 100 training steps: 0.05186541908748235\n",
      "Training loss per 100 training steps: 0.05192303454999785\n",
      "Training loss per 100 training steps: 0.051989664162693044\n",
      "Training loss per 100 training steps: 0.052008074551201836\n",
      "Training loss per 100 training steps: 0.05215078256025179\n",
      "Training loss per 100 training steps: 0.05211658105134586\n",
      "Training loss per 100 training steps: 0.052097469196108526\n",
      "Training loss per 100 training steps: 0.05204472411926829\n",
      "Training loss per 100 training steps: 0.05192064943823307\n",
      "Training loss per 100 training steps: 0.051975549194539214\n",
      "Training loss per 100 training steps: 0.05193623997827427\n",
      "Training loss per 100 training steps: 0.05200125959773021\n",
      "Training loss per 100 training steps: 0.05214942967895602\n",
      "Training loss per 100 training steps: 0.05205043498342832\n",
      "Training loss per 100 training steps: 0.0521838769911825\n",
      "Training loss per 100 training steps: 0.05202907969209593\n",
      "Training loss per 100 training steps: 0.05211769170726125\n",
      "Training loss epoch: 0.052151418695583956\n",
      "Training accuracy epoch: 0.982799499207112\n",
      "Training epoch: 30\n",
      "Training loss per 100 training steps: 0.009588627144694328\n",
      "Training loss per 100 training steps: 0.05325771187487437\n",
      "Training loss per 100 training steps: 0.04969884001705518\n",
      "Training loss per 100 training steps: 0.04678349220039218\n",
      "Training loss per 100 training steps: 0.0471930932944543\n",
      "Training loss per 100 training steps: 0.04899573335823735\n",
      "Training loss per 100 training steps: 0.05018128539410772\n",
      "Training loss per 100 training steps: 0.048855478669411784\n",
      "Training loss per 100 training steps: 0.048466419305805856\n",
      "Training loss per 100 training steps: 0.04752832547230201\n",
      "Training loss per 100 training steps: 0.048375974416492745\n",
      "Training loss per 100 training steps: 0.04944828698508015\n",
      "Training loss per 100 training steps: 0.04930503754376385\n",
      "Training loss per 100 training steps: 0.04893706721249398\n",
      "Training loss per 100 training steps: 0.04871917211402109\n",
      "Training loss per 100 training steps: 0.048291590393456985\n",
      "Training loss per 100 training steps: 0.04859721015770568\n",
      "Training loss per 100 training steps: 0.04819564162923252\n",
      "Training loss per 100 training steps: 0.04881874300337302\n",
      "Training loss per 100 training steps: 0.04887271933328617\n",
      "Training loss per 100 training steps: 0.049171750050753464\n",
      "Training loss per 100 training steps: 0.048908247655359095\n",
      "Training loss per 100 training steps: 0.04855839741957466\n",
      "Training loss per 100 training steps: 0.048572347800764235\n",
      "Training loss per 100 training steps: 0.04858640921567844\n",
      "Training loss per 100 training steps: 0.04864276319936585\n",
      "Training loss per 100 training steps: 0.04876866324341748\n",
      "Training loss per 100 training steps: 0.04911468798521891\n",
      "Training loss per 100 training steps: 0.04900262969438291\n",
      "Training loss per 100 training steps: 0.049012576582687796\n",
      "Training loss per 100 training steps: 0.049071238074086726\n",
      "Training loss per 100 training steps: 0.04909319887826714\n",
      "Training loss per 100 training steps: 0.04877521231728619\n",
      "Training loss per 100 training steps: 0.04873535959560721\n",
      "Training loss per 100 training steps: 0.04899786836048711\n",
      "Training loss per 100 training steps: 0.04915192130862894\n",
      "Training loss per 100 training steps: 0.049253664811673786\n",
      "Training loss per 100 training steps: 0.04914205395637981\n",
      "Training loss per 100 training steps: 0.04935111394339486\n",
      "Training loss per 100 training steps: 0.049124140735566174\n",
      "Training loss per 100 training steps: 0.049233639808293685\n",
      "Training loss per 100 training steps: 0.0490518844545592\n",
      "Training loss per 100 training steps: 0.04911800661753115\n",
      "Training loss per 100 training steps: 0.049030700790387495\n",
      "Training loss per 100 training steps: 0.04917705199524944\n",
      "Training loss per 100 training steps: 0.04933182005256839\n",
      "Training loss per 100 training steps: 0.04934259284580893\n",
      "Training loss per 100 training steps: 0.049470678517364926\n",
      "Training loss per 100 training steps: 0.04957679121015883\n",
      "Training loss per 100 training steps: 0.049570751221791796\n",
      "Training loss per 100 training steps: 0.0497263354991228\n",
      "Training loss per 100 training steps: 0.04998089054333208\n",
      "Training loss per 100 training steps: 0.049874510600485325\n",
      "Training loss per 100 training steps: 0.04970957364170543\n",
      "Training loss per 100 training steps: 0.04983301533928765\n",
      "Training loss per 100 training steps: 0.049785437729342394\n",
      "Training loss per 100 training steps: 0.04964212400335243\n",
      "Training loss per 100 training steps: 0.049776410431269645\n",
      "Training loss per 100 training steps: 0.04979326625434379\n",
      "Training loss per 100 training steps: 0.04986636892932981\n",
      "Training loss per 100 training steps: 0.04976296111361065\n",
      "Training loss per 100 training steps: 0.049742577347647685\n",
      "Training loss per 100 training steps: 0.049716266689018115\n",
      "Training loss epoch: 0.04975292869747171\n",
      "Training accuracy epoch: 0.9834141293859541\n",
      "Training epoch: 31\n",
      "Training loss per 100 training steps: 0.02991231344640255\n",
      "Training loss per 100 training steps: 0.039651711178611555\n",
      "Training loss per 100 training steps: 0.04208522260105654\n",
      "Training loss per 100 training steps: 0.04662464485168024\n",
      "Training loss per 100 training steps: 0.04549483506867146\n",
      "Training loss per 100 training steps: 0.044523921147585584\n",
      "Training loss per 100 training steps: 0.04404393456815843\n",
      "Training loss per 100 training steps: 0.044417747191768595\n",
      "Training loss per 100 training steps: 0.045894297112777045\n",
      "Training loss per 100 training steps: 0.045706216417476284\n",
      "Training loss per 100 training steps: 0.046159940136300696\n",
      "Training loss per 100 training steps: 0.04552054040359805\n",
      "Training loss per 100 training steps: 0.0448638500390244\n",
      "Training loss per 100 training steps: 0.04452236867645794\n",
      "Training loss per 100 training steps: 0.044677466879513045\n",
      "Training loss per 100 training steps: 0.04534549036371627\n",
      "Training loss per 100 training steps: 0.04525323773572555\n",
      "Training loss per 100 training steps: 0.045282818581925365\n",
      "Training loss per 100 training steps: 0.04545119047285922\n",
      "Training loss per 100 training steps: 0.04591473599962953\n",
      "Training loss per 100 training steps: 0.04610238132927602\n",
      "Training loss per 100 training steps: 0.04621008221327776\n",
      "Training loss per 100 training steps: 0.04627083899493057\n",
      "Training loss per 100 training steps: 0.046048665105137346\n",
      "Training loss per 100 training steps: 0.0458174444813016\n",
      "Training loss per 100 training steps: 0.045959143883182625\n",
      "Training loss per 100 training steps: 0.04606843536287498\n",
      "Training loss per 100 training steps: 0.04594959174982806\n",
      "Training loss per 100 training steps: 0.046031374524491124\n",
      "Training loss per 100 training steps: 0.04616589075135294\n",
      "Training loss per 100 training steps: 0.04611071169211739\n",
      "Training loss per 100 training steps: 0.04589748321132153\n",
      "Training loss per 100 training steps: 0.045746258019381814\n",
      "Training loss per 100 training steps: 0.045771283795034375\n",
      "Training loss per 100 training steps: 0.04572872423576599\n",
      "Training loss per 100 training steps: 0.0457602673372407\n",
      "Training loss per 100 training steps: 0.0457911365332834\n",
      "Training loss per 100 training steps: 0.04595400795052535\n",
      "Training loss per 100 training steps: 0.04594860221024689\n",
      "Training loss per 100 training steps: 0.04627523442581215\n",
      "Training loss per 100 training steps: 0.04665965427041321\n",
      "Training loss per 100 training steps: 0.047237244445697275\n",
      "Training loss per 100 training steps: 0.047204772279827494\n",
      "Training loss per 100 training steps: 0.047312538264057506\n",
      "Training loss per 100 training steps: 0.04732381503434289\n",
      "Training loss per 100 training steps: 0.04721986503668574\n",
      "Training loss per 100 training steps: 0.047314961626857534\n",
      "Training loss per 100 training steps: 0.04737643568487404\n",
      "Training loss per 100 training steps: 0.04744012748206279\n",
      "Training loss per 100 training steps: 0.0474646914823688\n",
      "Training loss per 100 training steps: 0.04755855902313957\n",
      "Training loss per 100 training steps: 0.047692062839921556\n",
      "Training loss per 100 training steps: 0.04790717947994798\n",
      "Training loss per 100 training steps: 0.04809293555085426\n",
      "Training loss per 100 training steps: 0.048111991415383464\n",
      "Training loss per 100 training steps: 0.04836435390871991\n",
      "Training loss per 100 training steps: 0.04842456149735811\n",
      "Training loss per 100 training steps: 0.048448218781974146\n",
      "Training loss per 100 training steps: 0.04834496117585657\n",
      "Training loss per 100 training steps: 0.0483258809535312\n",
      "Training loss per 100 training steps: 0.048240508903299846\n",
      "Training loss per 100 training steps: 0.048216992331276576\n",
      "Training loss per 100 training steps: 0.0482051505810572\n",
      "Training loss epoch: 0.048259571958626624\n",
      "Training accuracy epoch: 0.9838447520860586\n",
      "Training epoch: 32\n",
      "Training loss per 100 training steps: 0.024627624079585075\n",
      "Training loss per 100 training steps: 0.038379855224595284\n",
      "Training loss per 100 training steps: 0.04191826623157872\n",
      "Training loss per 100 training steps: 0.042006167629192216\n",
      "Training loss per 100 training steps: 0.040774703185780074\n",
      "Training loss per 100 training steps: 0.045538891125200855\n",
      "Training loss per 100 training steps: 0.04504795057893562\n",
      "Training loss per 100 training steps: 0.044049119833888706\n",
      "Training loss per 100 training steps: 0.04365172032202907\n",
      "Training loss per 100 training steps: 0.04437502344088788\n",
      "Training loss per 100 training steps: 0.044568757673229584\n",
      "Training loss per 100 training steps: 0.044214912939441985\n",
      "Training loss per 100 training steps: 0.04449409674525456\n",
      "Training loss per 100 training steps: 0.04549662189564949\n",
      "Training loss per 100 training steps: 0.04548434096082399\n",
      "Training loss per 100 training steps: 0.04537843565075292\n",
      "Training loss per 100 training steps: 0.04497646388823491\n",
      "Training loss per 100 training steps: 0.04466729601891684\n",
      "Training loss per 100 training steps: 0.04420174946473277\n",
      "Training loss per 100 training steps: 0.04406378568953069\n",
      "Training loss per 100 training steps: 0.043771266152892355\n",
      "Training loss per 100 training steps: 0.04339554281171672\n",
      "Training loss per 100 training steps: 0.04325519458400082\n",
      "Training loss per 100 training steps: 0.04328495683846691\n",
      "Training loss per 100 training steps: 0.0432578078411971\n",
      "Training loss per 100 training steps: 0.04344530486105541\n",
      "Training loss per 100 training steps: 0.04362585948567489\n",
      "Training loss per 100 training steps: 0.04357967180569209\n",
      "Training loss per 100 training steps: 0.04375057788472656\n",
      "Training loss per 100 training steps: 0.04387990804605818\n",
      "Training loss per 100 training steps: 0.04394515681403892\n",
      "Training loss per 100 training steps: 0.04384340170335556\n",
      "Training loss per 100 training steps: 0.04389301237016096\n",
      "Training loss per 100 training steps: 0.044113563215795945\n",
      "Training loss per 100 training steps: 0.04440588216761895\n",
      "Training loss per 100 training steps: 0.04470416014346751\n",
      "Training loss per 100 training steps: 0.045001084312039585\n",
      "Training loss per 100 training steps: 0.04510601758762517\n",
      "Training loss per 100 training steps: 0.04493317745245089\n",
      "Training loss per 100 training steps: 0.045268275026358294\n",
      "Training loss per 100 training steps: 0.0452653821475417\n",
      "Training loss per 100 training steps: 0.04530509012569945\n",
      "Training loss per 100 training steps: 0.0452157825257767\n",
      "Training loss per 100 training steps: 0.04518214895835931\n",
      "Training loss per 100 training steps: 0.045251855157469996\n",
      "Training loss per 100 training steps: 0.04514949913929661\n",
      "Training loss per 100 training steps: 0.045061875932585385\n",
      "Training loss per 100 training steps: 0.04499049833381397\n",
      "Training loss per 100 training steps: 0.04508532238274524\n",
      "Training loss per 100 training steps: 0.045045049201845905\n",
      "Training loss per 100 training steps: 0.04501628238674539\n",
      "Training loss per 100 training steps: 0.045017326026354325\n",
      "Training loss per 100 training steps: 0.0450409345588767\n",
      "Training loss per 100 training steps: 0.0451093080124467\n",
      "Training loss per 100 training steps: 0.04532395509121913\n",
      "Training loss per 100 training steps: 0.045388846336435605\n",
      "Training loss per 100 training steps: 0.045419435599718894\n",
      "Training loss per 100 training steps: 0.0454443129026982\n",
      "Training loss per 100 training steps: 0.04546007081938751\n",
      "Training loss per 100 training steps: 0.04560609233366059\n",
      "Training loss per 100 training steps: 0.04576352124100164\n",
      "Training loss per 100 training steps: 0.0456704654091593\n",
      "Training loss per 100 training steps: 0.04567923272831551\n",
      "Training loss epoch: 0.04578638106151062\n",
      "Training accuracy epoch: 0.984981880853532\n",
      "Training epoch: 33\n",
      "Training loss per 100 training steps: 0.01145802903920412\n",
      "Training loss per 100 training steps: 0.03190018682473219\n",
      "Training loss per 100 training steps: 0.03326712586379504\n",
      "Training loss per 100 training steps: 0.033303438053478754\n",
      "Training loss per 100 training steps: 0.034784695596034705\n",
      "Training loss per 100 training steps: 0.035316401671064264\n",
      "Training loss per 100 training steps: 0.03894700771453364\n",
      "Training loss per 100 training steps: 0.03983871774255071\n",
      "Training loss per 100 training steps: 0.04061988763425141\n",
      "Training loss per 100 training steps: 0.041873807827637145\n",
      "Training loss per 100 training steps: 0.04174253845203121\n",
      "Training loss per 100 training steps: 0.04148279565372094\n",
      "Training loss per 100 training steps: 0.04236904871444545\n",
      "Training loss per 100 training steps: 0.04253281929621832\n",
      "Training loss per 100 training steps: 0.04315301930338415\n",
      "Training loss per 100 training steps: 0.0428948828995198\n",
      "Training loss per 100 training steps: 0.04241426564202755\n",
      "Training loss per 100 training steps: 0.04278142751632001\n",
      "Training loss per 100 training steps: 0.04246638395651856\n",
      "Training loss per 100 training steps: 0.04338074896471386\n",
      "Training loss per 100 training steps: 0.043482467851092825\n",
      "Training loss per 100 training steps: 0.04320959136424598\n",
      "Training loss per 100 training steps: 0.04319373176223482\n",
      "Training loss per 100 training steps: 0.04282303473550741\n",
      "Training loss per 100 training steps: 0.04300642524429305\n",
      "Training loss per 100 training steps: 0.04302841533258072\n",
      "Training loss per 100 training steps: 0.043228830934241874\n",
      "Training loss per 100 training steps: 0.04297621051533653\n",
      "Training loss per 100 training steps: 0.0428850771127796\n",
      "Training loss per 100 training steps: 0.04273538877014154\n",
      "Training loss per 100 training steps: 0.04293867583996103\n",
      "Training loss per 100 training steps: 0.042780039138909966\n",
      "Training loss per 100 training steps: 0.0426764507053239\n",
      "Training loss per 100 training steps: 0.042546564899399085\n",
      "Training loss per 100 training steps: 0.042456459594133036\n",
      "Training loss per 100 training steps: 0.04252541870327272\n",
      "Training loss per 100 training steps: 0.04284216048466484\n",
      "Training loss per 100 training steps: 0.043107922074692745\n",
      "Training loss per 100 training steps: 0.04329150553832654\n",
      "Training loss per 100 training steps: 0.04335481512661206\n",
      "Training loss per 100 training steps: 0.04326676617753481\n",
      "Training loss per 100 training steps: 0.04344124085035618\n",
      "Training loss per 100 training steps: 0.0435309458517726\n",
      "Training loss per 100 training steps: 0.043552718313859755\n",
      "Training loss per 100 training steps: 0.04341701925159245\n",
      "Training loss per 100 training steps: 0.043614672795880584\n",
      "Training loss per 100 training steps: 0.043879586375503474\n",
      "Training loss per 100 training steps: 0.043934239618627004\n",
      "Training loss per 100 training steps: 0.04398178158526675\n",
      "Training loss per 100 training steps: 0.04416568037472613\n",
      "Training loss per 100 training steps: 0.04434446005973893\n",
      "Training loss per 100 training steps: 0.044360302640828984\n",
      "Training loss per 100 training steps: 0.044553765255653965\n",
      "Training loss per 100 training steps: 0.04460026075001216\n",
      "Training loss per 100 training steps: 0.044471786913889276\n",
      "Training loss per 100 training steps: 0.04457910061984292\n",
      "Training loss per 100 training steps: 0.044804631582317636\n",
      "Training loss per 100 training steps: 0.04486368703745339\n",
      "Training loss per 100 training steps: 0.044838380808192274\n",
      "Training loss per 100 training steps: 0.044853886105379805\n",
      "Training loss per 100 training steps: 0.04507357607053761\n",
      "Training loss per 100 training steps: 0.04506884190564787\n",
      "Training loss per 100 training steps: 0.04507670958229884\n",
      "Training loss epoch: 0.045055777613918165\n",
      "Training accuracy epoch: 0.9851456141694785\n",
      "Training epoch: 34\n",
      "Training loss per 100 training steps: 0.04041152819991112\n",
      "Training loss per 100 training steps: 0.040660233241459814\n",
      "Training loss per 100 training steps: 0.0375727816601863\n",
      "Training loss per 100 training steps: 0.041820889558918474\n",
      "Training loss per 100 training steps: 0.03984639527040425\n",
      "Training loss per 100 training steps: 0.03855401592885893\n",
      "Training loss per 100 training steps: 0.039344657164651235\n",
      "Training loss per 100 training steps: 0.040152506663672594\n",
      "Training loss per 100 training steps: 0.03947796188754154\n",
      "Training loss per 100 training steps: 0.03893040918427769\n",
      "Training loss per 100 training steps: 0.03952386256234793\n",
      "Training loss per 100 training steps: 0.03958710240133337\n",
      "Training loss per 100 training steps: 0.03954487232930282\n",
      "Training loss per 100 training steps: 0.03981273363972173\n",
      "Training loss per 100 training steps: 0.040485903616565184\n",
      "Training loss per 100 training steps: 0.03995500958206965\n",
      "Training loss per 100 training steps: 0.03971099006788669\n",
      "Training loss per 100 training steps: 0.03986855625856048\n",
      "Training loss per 100 training steps: 0.040091642544582605\n",
      "Training loss per 100 training steps: 0.04010487827054739\n",
      "Training loss per 100 training steps: 0.040078634657475375\n",
      "Training loss per 100 training steps: 0.03981101850469041\n",
      "Training loss per 100 training steps: 0.03972941025009828\n",
      "Training loss per 100 training steps: 0.03984054232351943\n",
      "Training loss per 100 training steps: 0.040242818263859746\n",
      "Training loss per 100 training steps: 0.04066899686190458\n",
      "Training loss per 100 training steps: 0.040766891457644595\n",
      "Training loss per 100 training steps: 0.04044881909084015\n",
      "Training loss per 100 training steps: 0.040330977998256634\n",
      "Training loss per 100 training steps: 0.04008753263639233\n",
      "Training loss per 100 training steps: 0.0403127897231241\n",
      "Training loss per 100 training steps: 0.04029653384736915\n",
      "Training loss per 100 training steps: 0.040494518042894084\n",
      "Training loss per 100 training steps: 0.04070654273620262\n",
      "Training loss per 100 training steps: 0.04117093490258952\n",
      "Training loss per 100 training steps: 0.04119875849568054\n",
      "Training loss per 100 training steps: 0.04138628480323869\n",
      "Training loss per 100 training steps: 0.04150251798530282\n",
      "Training loss per 100 training steps: 0.04142702700452861\n",
      "Training loss per 100 training steps: 0.04136431556812401\n",
      "Training loss per 100 training steps: 0.041489149029761224\n",
      "Training loss per 100 training steps: 0.04157134279854218\n",
      "Training loss per 100 training steps: 0.04170820002421476\n",
      "Training loss per 100 training steps: 0.04163844948191688\n",
      "Training loss per 100 training steps: 0.041435695862819935\n",
      "Training loss per 100 training steps: 0.04139479529510888\n",
      "Training loss per 100 training steps: 0.04151363619012158\n",
      "Training loss per 100 training steps: 0.04158365330817342\n",
      "Training loss per 100 training steps: 0.04163284295321386\n",
      "Training loss per 100 training steps: 0.04177343122533036\n",
      "Training loss per 100 training steps: 0.04189291784965441\n",
      "Training loss per 100 training steps: 0.04204594298500759\n",
      "Training loss per 100 training steps: 0.041975109179456224\n",
      "Training loss per 100 training steps: 0.0422192047538051\n",
      "Training loss per 100 training steps: 0.042225887516357065\n",
      "Training loss per 100 training steps: 0.04233826458436102\n",
      "Training loss per 100 training steps: 0.04252926883191037\n",
      "Training loss per 100 training steps: 0.04268600215825895\n",
      "Training loss per 100 training steps: 0.04270606137890564\n",
      "Training loss per 100 training steps: 0.042832256619767375\n",
      "Training loss per 100 training steps: 0.04282423009315092\n",
      "Training loss per 100 training steps: 0.04278403029859465\n",
      "Training loss per 100 training steps: 0.04275150002581346\n",
      "Training loss epoch: 0.042796824176648124\n",
      "Training accuracy epoch: 0.9858500800732706\n",
      "Training epoch: 35\n",
      "Training loss per 100 training steps: 0.0062443045899271965\n",
      "Training loss per 100 training steps: 0.031948862772545605\n",
      "Training loss per 100 training steps: 0.03419031189356129\n",
      "Training loss per 100 training steps: 0.03628938246372951\n",
      "Training loss per 100 training steps: 0.0363480104023168\n",
      "Training loss per 100 training steps: 0.03616344233037915\n",
      "Training loss per 100 training steps: 0.03573959758125568\n",
      "Training loss per 100 training steps: 0.03791038439720045\n",
      "Training loss per 100 training steps: 0.03948975725319776\n",
      "Training loss per 100 training steps: 0.03924652157050581\n",
      "Training loss per 100 training steps: 0.03875461782558591\n",
      "Training loss per 100 training steps: 0.0388318708347923\n",
      "Training loss per 100 training steps: 0.03839563258414116\n",
      "Training loss per 100 training steps: 0.03803581120201145\n",
      "Training loss per 100 training steps: 0.03764068259088805\n",
      "Training loss per 100 training steps: 0.03772811834809918\n",
      "Training loss per 100 training steps: 0.03767693959113134\n",
      "Training loss per 100 training steps: 0.03773354852874859\n",
      "Training loss per 100 training steps: 0.03828663307307769\n",
      "Training loss per 100 training steps: 0.03860105137000971\n",
      "Training loss per 100 training steps: 0.038802455647905326\n",
      "Training loss per 100 training steps: 0.039237096290607655\n",
      "Training loss per 100 training steps: 0.03890550918209627\n",
      "Training loss per 100 training steps: 0.03898937191027154\n",
      "Training loss per 100 training steps: 0.03899297908118971\n",
      "Training loss per 100 training steps: 0.03940652338613826\n",
      "Training loss per 100 training steps: 0.03920525144872473\n",
      "Training loss per 100 training steps: 0.039130404028965324\n",
      "Training loss per 100 training steps: 0.039211452359781504\n",
      "Training loss per 100 training steps: 0.039929532614550706\n",
      "Training loss per 100 training steps: 0.0402042426081481\n",
      "Training loss per 100 training steps: 0.04023974791839052\n",
      "Training loss per 100 training steps: 0.0404115534095745\n",
      "Training loss per 100 training steps: 0.04049948445203966\n",
      "Training loss per 100 training steps: 0.040790890910691054\n",
      "Training loss per 100 training steps: 0.041017437532044156\n",
      "Training loss per 100 training steps: 0.04089008038567733\n",
      "Training loss per 100 training steps: 0.040810497553567526\n",
      "Training loss per 100 training steps: 0.0410805527365604\n",
      "Training loss per 100 training steps: 0.04148138389205028\n",
      "Training loss per 100 training steps: 0.041695791120626734\n",
      "Training loss per 100 training steps: 0.04173582908112653\n",
      "Training loss per 100 training steps: 0.041898252802199984\n",
      "Training loss per 100 training steps: 0.04183734908564221\n",
      "Training loss per 100 training steps: 0.041787960836602474\n",
      "Training loss per 100 training steps: 0.04167024396340315\n",
      "Training loss per 100 training steps: 0.04164335945407532\n",
      "Training loss per 100 training steps: 0.04166018039455805\n",
      "Training loss per 100 training steps: 0.04161480651798415\n",
      "Training loss per 100 training steps: 0.041479133138008106\n",
      "Training loss per 100 training steps: 0.04159187400765288\n",
      "Training loss per 100 training steps: 0.04167652705880711\n",
      "Training loss per 100 training steps: 0.04157444778821531\n",
      "Training loss per 100 training steps: 0.04141510109267762\n",
      "Training loss per 100 training steps: 0.04137430754666727\n",
      "Training loss per 100 training steps: 0.04139818759960614\n",
      "Training loss per 100 training steps: 0.04130892215700141\n",
      "Training loss per 100 training steps: 0.041166659651702416\n",
      "Training loss per 100 training steps: 0.04118307158873318\n",
      "Training loss per 100 training steps: 0.04120970018958443\n",
      "Training loss per 100 training steps: 0.041289120749743086\n",
      "Training loss per 100 training steps: 0.04132490594635459\n",
      "Training loss per 100 training steps: 0.04127892493642619\n",
      "Training loss epoch: 0.04127591235525498\n",
      "Training accuracy epoch: 0.9864736322523517\n",
      "Training epoch: 36\n",
      "Training loss per 100 training steps: 0.044583648443222046\n",
      "Training loss per 100 training steps: 0.03583293341764129\n",
      "Training loss per 100 training steps: 0.0377792319102185\n",
      "Training loss per 100 training steps: 0.04058814612008396\n",
      "Training loss per 100 training steps: 0.039898472740849074\n",
      "Training loss per 100 training steps: 0.03899943109851762\n",
      "Training loss per 100 training steps: 0.03913611970735736\n",
      "Training loss per 100 training steps: 0.03953021429509015\n",
      "Training loss per 100 training steps: 0.03959580010966457\n",
      "Training loss per 100 training steps: 0.040273187185490875\n",
      "Training loss per 100 training steps: 0.04091504932910141\n",
      "Training loss per 100 training steps: 0.04055545888268063\n",
      "Training loss per 100 training steps: 0.04027852923622887\n",
      "Training loss per 100 training steps: 0.03993890140458675\n",
      "Training loss per 100 training steps: 0.03987206833985176\n",
      "Training loss per 100 training steps: 0.039914855943154484\n",
      "Training loss per 100 training steps: 0.039928384285712784\n",
      "Training loss per 100 training steps: 0.04023852135263644\n",
      "Training loss per 100 training steps: 0.04032710478021202\n",
      "Training loss per 100 training steps: 0.04041691489389944\n",
      "Training loss per 100 training steps: 0.04044263049342164\n",
      "Training loss per 100 training steps: 0.04037102562602435\n",
      "Training loss per 100 training steps: 0.04053839244223392\n",
      "Training loss per 100 training steps: 0.04066780771125089\n",
      "Training loss per 100 training steps: 0.04028303791180324\n",
      "Training loss per 100 training steps: 0.04101617668864925\n",
      "Training loss per 100 training steps: 0.04078191147948626\n",
      "Training loss per 100 training steps: 0.040860049550879955\n",
      "Training loss per 100 training steps: 0.040578642516939406\n",
      "Training loss per 100 training steps: 0.04051478281721902\n",
      "Training loss per 100 training steps: 0.04035653998984597\n",
      "Training loss per 100 training steps: 0.04067572341491632\n",
      "Training loss per 100 training steps: 0.04086257549336556\n",
      "Training loss per 100 training steps: 0.0410814810809211\n",
      "Training loss per 100 training steps: 0.04107416156642492\n",
      "Training loss per 100 training steps: 0.04087390707071482\n",
      "Training loss per 100 training steps: 0.04090276020025251\n",
      "Training loss per 100 training steps: 0.04094538459925422\n",
      "Training loss per 100 training steps: 0.040834721921193\n",
      "Training loss per 100 training steps: 0.04065554283313839\n",
      "Training loss per 100 training steps: 0.04048776520819477\n",
      "Training loss per 100 training steps: 0.0405026361907826\n",
      "Training loss per 100 training steps: 0.04056877028064867\n",
      "Training loss per 100 training steps: 0.040660551419046724\n",
      "Training loss per 100 training steps: 0.040842045760465184\n",
      "Training loss per 100 training steps: 0.04084663368812803\n",
      "Training loss per 100 training steps: 0.040916919870856404\n",
      "Training loss per 100 training steps: 0.04086924008025376\n",
      "Training loss per 100 training steps: 0.040843976372852434\n",
      "Training loss per 100 training steps: 0.040962858284228146\n",
      "Training loss per 100 training steps: 0.04097450708459839\n",
      "Training loss per 100 training steps: 0.041361434691555204\n",
      "Training loss per 100 training steps: 0.041474739818418246\n",
      "Training loss per 100 training steps: 0.04128533690624999\n",
      "Training loss per 100 training steps: 0.04129188044318585\n",
      "Training loss per 100 training steps: 0.04117401188676547\n",
      "Training loss per 100 training steps: 0.0412625672592267\n",
      "Training loss per 100 training steps: 0.041103988648444126\n",
      "Training loss per 100 training steps: 0.04107401183303412\n",
      "Training loss per 100 training steps: 0.04154314923072278\n",
      "Training loss per 100 training steps: 0.041614308968271195\n",
      "Training loss per 100 training steps: 0.041582883665805044\n",
      "Training loss per 100 training steps: 0.04155959061248311\n",
      "Training loss epoch: 0.041484461824155335\n",
      "Training accuracy epoch: 0.9862501465196879\n",
      "Training epoch: 37\n",
      "Training loss per 100 training steps: 0.026132887229323387\n",
      "Training loss per 100 training steps: 0.04274754101724023\n",
      "Training loss per 100 training steps: 0.03829601339286944\n",
      "Training loss per 100 training steps: 0.03909858625824641\n",
      "Training loss per 100 training steps: 0.040368773686488224\n",
      "Training loss per 100 training steps: 0.03899054360406812\n",
      "Training loss per 100 training steps: 0.038061447998279464\n",
      "Training loss per 100 training steps: 0.03767748677853319\n",
      "Training loss per 100 training steps: 0.03770443912247863\n",
      "Training loss per 100 training steps: 0.03640903854307478\n",
      "Training loss per 100 training steps: 0.03708476223447895\n",
      "Training loss per 100 training steps: 0.03664790179692953\n",
      "Training loss per 100 training steps: 0.03716559887520168\n",
      "Training loss per 100 training steps: 0.03710210885348616\n",
      "Training loss per 100 training steps: 0.036750489511998434\n",
      "Training loss per 100 training steps: 0.036968668960165145\n",
      "Training loss per 100 training steps: 0.036672246789293296\n",
      "Training loss per 100 training steps: 0.03713727701164624\n",
      "Training loss per 100 training steps: 0.03746039049892786\n",
      "Training loss per 100 training steps: 0.03780508570345868\n",
      "Training loss per 100 training steps: 0.03856769674123665\n",
      "Training loss per 100 training steps: 0.03874516094429042\n",
      "Training loss per 100 training steps: 0.03879468174319527\n",
      "Training loss per 100 training steps: 0.03861897482259684\n",
      "Training loss per 100 training steps: 0.038341579966214887\n",
      "Training loss per 100 training steps: 0.03846530813444704\n",
      "Training loss per 100 training steps: 0.038436955995100414\n",
      "Training loss per 100 training steps: 0.03850294355950768\n",
      "Training loss per 100 training steps: 0.03865521453032207\n",
      "Training loss per 100 training steps: 0.03859640394179526\n",
      "Training loss per 100 training steps: 0.03862797146328216\n",
      "Training loss per 100 training steps: 0.03876205744844691\n",
      "Training loss per 100 training steps: 0.03908735670886005\n",
      "Training loss per 100 training steps: 0.03923741050686104\n",
      "Training loss per 100 training steps: 0.039165567153965096\n",
      "Training loss per 100 training steps: 0.03917070786719804\n",
      "Training loss per 100 training steps: 0.03914930915870693\n",
      "Training loss per 100 training steps: 0.03927713253008646\n",
      "Training loss per 100 training steps: 0.03916068777449452\n",
      "Training loss per 100 training steps: 0.03898430382501634\n",
      "Training loss per 100 training steps: 0.039173552698036296\n",
      "Training loss per 100 training steps: 0.03912546804889113\n",
      "Training loss per 100 training steps: 0.039140625324509716\n",
      "Training loss per 100 training steps: 0.039343804349652706\n",
      "Training loss per 100 training steps: 0.039268180308826485\n",
      "Training loss per 100 training steps: 0.039344727707676254\n",
      "Training loss per 100 training steps: 0.039371705378821314\n",
      "Training loss per 100 training steps: 0.039477261028329214\n",
      "Training loss per 100 training steps: 0.03951087197007786\n",
      "Training loss per 100 training steps: 0.039541173835633325\n",
      "Training loss per 100 training steps: 0.039664979675156004\n",
      "Training loss per 100 training steps: 0.03966420480433831\n",
      "Training loss per 100 training steps: 0.03969154938701519\n",
      "Training loss per 100 training steps: 0.03969397224867381\n",
      "Training loss per 100 training steps: 0.03971168148487657\n",
      "Training loss per 100 training steps: 0.039656464472942805\n",
      "Training loss per 100 training steps: 0.0396575217371864\n",
      "Training loss per 100 training steps: 0.03968236597391361\n",
      "Training loss per 100 training steps: 0.039715281766323975\n",
      "Training loss per 100 training steps: 0.03967929535401867\n",
      "Training loss per 100 training steps: 0.039607242444239764\n",
      "Training loss per 100 training steps: 0.03969406845199423\n",
      "Training loss per 100 training steps: 0.039598563887961236\n",
      "Training loss epoch: 0.039534104452725996\n",
      "Training accuracy epoch: 0.9868839915702758\n",
      "Training epoch: 38\n",
      "Training loss per 100 training steps: 0.030195530503988266\n",
      "Training loss per 100 training steps: 0.03157060847175077\n",
      "Training loss per 100 training steps: 0.036735818171132334\n",
      "Training loss per 100 training steps: 0.03784994208068274\n",
      "Training loss per 100 training steps: 0.03500004646982626\n",
      "Training loss per 100 training steps: 0.035500193469016304\n",
      "Training loss per 100 training steps: 0.03424680415915167\n",
      "Training loss per 100 training steps: 0.033810524944516265\n",
      "Training loss per 100 training steps: 0.0335579393446078\n",
      "Training loss per 100 training steps: 0.03407410541073844\n",
      "Training loss per 100 training steps: 0.03365541048921072\n",
      "Training loss per 100 training steps: 0.03376209944846966\n",
      "Training loss per 100 training steps: 0.03305984603046834\n",
      "Training loss per 100 training steps: 0.03348600738885871\n",
      "Training loss per 100 training steps: 0.03346404387365673\n",
      "Training loss per 100 training steps: 0.03413600697693074\n",
      "Training loss per 100 training steps: 0.03419018727935197\n",
      "Training loss per 100 training steps: 0.03442756640351159\n",
      "Training loss per 100 training steps: 0.03441688571732102\n",
      "Training loss per 100 training steps: 0.03432488889914292\n",
      "Training loss per 100 training steps: 0.03424069976077128\n",
      "Training loss per 100 training steps: 0.03404809543911145\n",
      "Training loss per 100 training steps: 0.03432570784100425\n",
      "Training loss per 100 training steps: 0.034930464370929636\n",
      "Training loss per 100 training steps: 0.03485802234802996\n",
      "Training loss per 100 training steps: 0.03463483418702282\n",
      "Training loss per 100 training steps: 0.03428017110299295\n",
      "Training loss per 100 training steps: 0.03439436522911133\n",
      "Training loss per 100 training steps: 0.03479188401303984\n",
      "Training loss per 100 training steps: 0.03515983226713321\n",
      "Training loss per 100 training steps: 0.036076703937932934\n",
      "Training loss per 100 training steps: 0.036219998979953774\n",
      "Training loss per 100 training steps: 0.036405722952315596\n",
      "Training loss per 100 training steps: 0.036953241619155495\n",
      "Training loss per 100 training steps: 0.037344742265347966\n",
      "Training loss per 100 training steps: 0.0375601015831554\n",
      "Training loss per 100 training steps: 0.037427627447203356\n",
      "Training loss per 100 training steps: 0.037369053146990745\n",
      "Training loss per 100 training steps: 0.037086303476745894\n",
      "Training loss per 100 training steps: 0.037098062290115676\n",
      "Training loss per 100 training steps: 0.03692235218530537\n",
      "Training loss per 100 training steps: 0.036770779670892775\n",
      "Training loss per 100 training steps: 0.03660665521485699\n",
      "Training loss per 100 training steps: 0.036390875404387416\n",
      "Training loss per 100 training steps: 0.03641978501002011\n",
      "Training loss per 100 training steps: 0.0363908576232632\n",
      "Training loss per 100 training steps: 0.036583776560113974\n",
      "Training loss per 100 training steps: 0.03729557062343856\n",
      "Training loss per 100 training steps: 0.037504684459687215\n",
      "Training loss per 100 training steps: 0.0376029724652337\n",
      "Training loss per 100 training steps: 0.03758563269625977\n",
      "Training loss per 100 training steps: 0.0375225034048307\n",
      "Training loss per 100 training steps: 0.037334788191878326\n",
      "Training loss per 100 training steps: 0.037331929796203076\n",
      "Training loss per 100 training steps: 0.037263977579282154\n",
      "Training loss per 100 training steps: 0.03724598460375541\n",
      "Training loss per 100 training steps: 0.037143025289129945\n",
      "Training loss per 100 training steps: 0.03723421508398022\n",
      "Training loss per 100 training steps: 0.037264212360770155\n",
      "Training loss per 100 training steps: 0.037253231285118614\n",
      "Training loss per 100 training steps: 0.037486608861999886\n",
      "Training loss per 100 training steps: 0.037479583386270277\n",
      "Training loss per 100 training steps: 0.03746597349217996\n",
      "Training loss epoch: 0.037461707176514175\n",
      "Training accuracy epoch: 0.9875872319427433\n",
      "Training epoch: 39\n",
      "Training loss per 100 training steps: 0.012995311059057713\n",
      "Training loss per 100 training steps: 0.02920441668812591\n",
      "Training loss per 100 training steps: 0.027577982885607828\n",
      "Training loss per 100 training steps: 0.030990657273430897\n",
      "Training loss per 100 training steps: 0.032425977034637114\n",
      "Training loss per 100 training steps: 0.03295497515295984\n",
      "Training loss per 100 training steps: 0.03336915185397255\n",
      "Training loss per 100 training steps: 0.03304995095764836\n",
      "Training loss per 100 training steps: 0.03269904451181684\n",
      "Training loss per 100 training steps: 0.03217525080281292\n",
      "Training loss per 100 training steps: 0.033708413341745534\n",
      "Training loss per 100 training steps: 0.03403372464203094\n",
      "Training loss per 100 training steps: 0.03424329154473923\n",
      "Training loss per 100 training steps: 0.034130980681143355\n",
      "Training loss per 100 training steps: 0.033814299866114395\n",
      "Training loss per 100 training steps: 0.033706901779585706\n",
      "Training loss per 100 training steps: 0.03326914918365807\n",
      "Training loss per 100 training steps: 0.034184180536137115\n",
      "Training loss per 100 training steps: 0.03481151091001063\n",
      "Training loss per 100 training steps: 0.03488606452828725\n",
      "Training loss per 100 training steps: 0.03472514756389013\n",
      "Training loss per 100 training steps: 0.035739373289097875\n",
      "Training loss per 100 training steps: 0.03583348613332446\n",
      "Training loss per 100 training steps: 0.035684477902243786\n",
      "Training loss per 100 training steps: 0.03557597945684002\n",
      "Training loss per 100 training steps: 0.03556153263606433\n",
      "Training loss per 100 training steps: 0.035588839901702515\n",
      "Training loss per 100 training steps: 0.03530808321465985\n",
      "Training loss per 100 training steps: 0.035397032365445294\n",
      "Training loss per 100 training steps: 0.035315181303845436\n",
      "Training loss per 100 training steps: 0.03542191550530206\n",
      "Training loss per 100 training steps: 0.03555716138297193\n",
      "Training loss per 100 training steps: 0.035489179599917875\n",
      "Training loss per 100 training steps: 0.03534325161886449\n",
      "Training loss per 100 training steps: 0.03533984454954139\n",
      "Training loss per 100 training steps: 0.03534026228052917\n",
      "Training loss per 100 training steps: 0.03535851574814792\n",
      "Training loss per 100 training steps: 0.03528968721419139\n",
      "Training loss per 100 training steps: 0.03545220945286742\n",
      "Training loss per 100 training steps: 0.03540858408724815\n",
      "Training loss per 100 training steps: 0.035453818988108995\n",
      "Training loss per 100 training steps: 0.03559624165996239\n",
      "Training loss per 100 training steps: 0.035583477472619085\n",
      "Training loss per 100 training steps: 0.03574421225595339\n",
      "Training loss per 100 training steps: 0.03588022981376784\n",
      "Training loss per 100 training steps: 0.035948895770052156\n",
      "Training loss per 100 training steps: 0.03601623534074784\n",
      "Training loss per 100 training steps: 0.03612434253537329\n",
      "Training loss per 100 training steps: 0.03610234894892616\n",
      "Training loss per 100 training steps: 0.03618215213919938\n",
      "Training loss per 100 training steps: 0.036136051588271365\n",
      "Training loss per 100 training steps: 0.03606486477021837\n",
      "Training loss per 100 training steps: 0.036151834962058864\n",
      "Training loss per 100 training steps: 0.0362016916854861\n",
      "Training loss per 100 training steps: 0.036233555762240965\n",
      "Training loss per 100 training steps: 0.03629593797985417\n",
      "Training loss per 100 training steps: 0.036506875038836276\n",
      "Training loss per 100 training steps: 0.036464220439920696\n",
      "Training loss per 100 training steps: 0.036473048477937496\n",
      "Training loss per 100 training steps: 0.03663489329025559\n",
      "Training loss per 100 training steps: 0.03674161227216784\n",
      "Training loss per 100 training steps: 0.03674069510875215\n",
      "Training loss per 100 training steps: 0.036827921182498946\n",
      "Training loss epoch: 0.03682779095361032\n",
      "Training accuracy epoch: 0.9877945082061138\n",
      "Training epoch: 40\n",
      "Training loss per 100 training steps: 0.015696417540311813\n",
      "Training loss per 100 training steps: 0.03176975470395888\n",
      "Training loss per 100 training steps: 0.030275966578269777\n",
      "Training loss per 100 training steps: 0.03212395408523402\n",
      "Training loss per 100 training steps: 0.031217157506869358\n",
      "Training loss per 100 training steps: 0.03275063400112182\n",
      "Training loss per 100 training steps: 0.03339195217944592\n",
      "Training loss per 100 training steps: 0.03222402004309506\n",
      "Training loss per 100 training steps: 0.03198091059505472\n",
      "Training loss per 100 training steps: 0.031943461753272416\n",
      "Training loss per 100 training steps: 0.03184538319553169\n",
      "Training loss per 100 training steps: 0.031989295095870174\n",
      "Training loss per 100 training steps: 0.03178866245393423\n",
      "Training loss per 100 training steps: 0.03244856613359595\n",
      "Training loss per 100 training steps: 0.032368665229865506\n",
      "Training loss per 100 training steps: 0.03244048315849587\n",
      "Training loss per 100 training steps: 0.032596394378804766\n",
      "Training loss per 100 training steps: 0.03284064019978666\n",
      "Training loss per 100 training steps: 0.033042440810010044\n",
      "Training loss per 100 training steps: 0.03309462019248164\n",
      "Training loss per 100 training steps: 0.03342475522511014\n",
      "Training loss per 100 training steps: 0.03324082517784046\n",
      "Training loss per 100 training steps: 0.03354327422228996\n",
      "Training loss per 100 training steps: 0.03362935214427236\n",
      "Training loss per 100 training steps: 0.03356403586619688\n",
      "Training loss per 100 training steps: 0.03359611291811978\n",
      "Training loss per 100 training steps: 0.033575753545868295\n",
      "Training loss per 100 training steps: 0.03384338147384277\n",
      "Training loss per 100 training steps: 0.033681279135219504\n",
      "Training loss per 100 training steps: 0.033841478161162515\n",
      "Training loss per 100 training steps: 0.033710407452140406\n",
      "Training loss per 100 training steps: 0.033522485157418035\n",
      "Training loss per 100 training steps: 0.03338009759466285\n",
      "Training loss per 100 training steps: 0.03346277449906324\n",
      "Training loss per 100 training steps: 0.033381171205580595\n",
      "Training loss per 100 training steps: 0.033488048946890135\n",
      "Training loss per 100 training steps: 0.03368442215714703\n",
      "Training loss per 100 training steps: 0.03387512324589556\n",
      "Training loss per 100 training steps: 0.033816410045219916\n",
      "Training loss per 100 training steps: 0.03376084648780612\n",
      "Training loss per 100 training steps: 0.03373883357157057\n",
      "Training loss per 100 training steps: 0.03365312737010478\n",
      "Training loss per 100 training steps: 0.033734546471652346\n",
      "Training loss per 100 training steps: 0.03380090545048764\n",
      "Training loss per 100 training steps: 0.03385112796151278\n",
      "Training loss per 100 training steps: 0.03432680324285605\n",
      "Training loss per 100 training steps: 0.034526826747556943\n",
      "Training loss per 100 training steps: 0.0348082131688311\n",
      "Training loss per 100 training steps: 0.034952585712045216\n",
      "Training loss per 100 training steps: 0.03488345382734017\n",
      "Training loss per 100 training steps: 0.03495331780190017\n",
      "Training loss per 100 training steps: 0.03499047927485232\n",
      "Training loss per 100 training steps: 0.034939656127052854\n",
      "Training loss per 100 training steps: 0.03482849093242378\n",
      "Training loss per 100 training steps: 0.03478976269715479\n",
      "Training loss per 100 training steps: 0.03500638523138658\n",
      "Training loss per 100 training steps: 0.03492818333821732\n",
      "Training loss per 100 training steps: 0.035250484178843855\n",
      "Training loss per 100 training steps: 0.03536418713442311\n",
      "Training loss per 100 training steps: 0.03554325026898527\n",
      "Training loss per 100 training steps: 0.035620268153408206\n",
      "Training loss per 100 training steps: 0.03575700650131326\n",
      "Training loss per 100 training steps: 0.0357492551191031\n",
      "Training loss epoch: 0.0357244151774942\n",
      "Training accuracy epoch: 0.9880955018852894\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11f4306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T04:26:13.243850Z",
     "iopub.status.busy": "2022-01-31T04:26:13.242998Z",
     "iopub.status.idle": "2022-01-31T04:26:13.253590Z",
     "shell.execute_reply": "2022-01-31T04:26:13.254558Z",
     "shell.execute_reply.started": "2022-01-25T17:59:46.144602Z"
    },
    "papermill": {
     "duration": 0.792815,
     "end_time": "2022-01-31T04:26:13.254748",
     "exception": false,
     "start_time": "2022-01-31T04:26:12.461933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = output[0]\n",
    "            eval_logits = output[1]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "#             eval_labels.append(labels)\n",
    "#             eval_preds.append(predictions)\n",
    "            \n",
    "            eval_labels.append([idx_to_label[id.item()] for id in labels])\n",
    "            eval_preds.append([idx_to_label[id.item()] for id in predictions])\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#     labels = [idx_to_label[id.item()] for id in eval_labels]\n",
    "#     predictions = [idx_to_label[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "#     return labels, predictions\n",
    "    return eval_labels, eval_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce07e83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T04:26:14.931754Z",
     "iopub.status.busy": "2022-01-31T04:26:14.930749Z",
     "iopub.status.idle": "2022-01-31T04:28:01.392603Z",
     "shell.execute_reply": "2022-01-31T04:28:01.391861Z"
    },
    "papermill": {
     "duration": 107.125298,
     "end_time": "2022-01-31T04:28:01.392751",
     "exception": false,
     "start_time": "2022-01-31T04:26:14.267453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 2.691830635070801\n",
      "Validation loss per 100 evaluation steps: 1.6960436483420949\n",
      "Validation loss per 100 evaluation steps: 1.7317183865836603\n",
      "Validation loss per 100 evaluation steps: 1.733861021722274\n",
      "Validation loss per 100 evaluation steps: 1.7127212099823867\n",
      "Validation loss per 100 evaluation steps: 1.7073517568751486\n",
      "Validation loss per 100 evaluation steps: 1.721878132350532\n",
      "Validation loss per 100 evaluation steps: 1.7190289160664514\n",
      "Validation loss per 100 evaluation steps: 1.709740483768275\n",
      "Validation loss per 100 evaluation steps: 1.7086721779768264\n",
      "Validation loss per 100 evaluation steps: 1.706198286841978\n",
      "Validation loss per 100 evaluation steps: 1.6989965771513567\n",
      "Validation loss per 100 evaluation steps: 1.6963462932368774\n",
      "Validation loss per 100 evaluation steps: 1.6976503893277743\n",
      "Validation loss per 100 evaluation steps: 1.6957979453614596\n",
      "Validation loss per 100 evaluation steps: 1.6899496261018025\n",
      "Validation Loss: 1.6772418710856865\n",
      "Validation Accuracy: 0.7353174850508825\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8f965c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T04:28:02.745022Z",
     "iopub.status.busy": "2022-01-31T04:28:02.744299Z",
     "iopub.status.idle": "2022-01-31T04:28:27.174467Z",
     "shell.execute_reply": "2022-01-31T04:28:27.174902Z"
    },
    "papermill": {
     "duration": 25.109608,
     "end_time": "2022-01-31T04:28:27.175077",
     "exception": false,
     "start_time": "2022-01-31T04:28:02.065469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Claim       0.28      0.43      0.34      9571\n",
      "Concluding Statement       0.30      0.49      0.37      1905\n",
      "        Counterclaim       0.17      0.25      0.21       984\n",
      "            Evidence       0.16      0.25      0.20      8532\n",
      "                Lead       0.41      0.56      0.47      1836\n",
      "            Position       0.35      0.50      0.41      3028\n",
      "            Rebuttal       0.13      0.19      0.15       677\n",
      "\n",
      "           micro avg       0.25      0.38      0.30     26533\n",
      "           macro avg       0.26      0.38      0.31     26533\n",
      "        weighted avg       0.25      0.38      0.30     26533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95ba2cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T04:28:28.517135Z",
     "iopub.status.busy": "2022-01-31T04:28:28.516357Z",
     "iopub.status.idle": "2022-01-31T04:28:29.245646Z",
     "shell.execute_reply": "2022-01-31T04:28:29.246030Z"
    },
    "papermill": {
     "duration": 1.406979,
     "end_time": "2022-01-31T04:28:29.246218",
     "exception": false,
     "start_time": "2022-01-31T04:28:27.839239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dirname = \"/kaggle/working/feedback-bert-uncased-model1/\"\n",
    "if not os.path.isdir(dirname):\n",
    "    os.makedirs(dirname)\n",
    "# os.rename(dirname, \"/kaggle/working/bert-base-uncased-model1/\")\n",
    "\n",
    "model.save_pretrained(dirname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38654.571325,
   "end_time": "2022-01-31T04:28:33.700222",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-30T17:44:19.128897",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "006b4c76f50042cfa110b9b4c4d376c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "056b6ffdc96c4b60a76e7f7dafe0433f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1204205c64b84807a5fca4016ca2dd49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13ab9848e0704fd58f0225dac2309aff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cd84114c598843b586f338215a7e80d6",
       "placeholder": "​",
       "style": "IPY_MODEL_e944be7c98bc402ea9e9fcf027a077e7",
       "value": " 226k/226k [00:00&lt;00:00, 668kB/s]"
      }
     },
     "14768feddc10486d94bab49200d9b66b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16aff4a52fbb4b5385c77897bc995d12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_82d3bae1a5614702b5d0be9f6bdc0dc2",
        "IPY_MODEL_68b7055c0ae946f8a59dcfb87e16d481",
        "IPY_MODEL_4dc53e9abfc04fe68e363de1c9c4e610"
       ],
       "layout": "IPY_MODEL_6324d9d3407e40b4a7624927b6212b28"
      }
     },
     "22b26ec6df244262a2898a0334e1a82f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3b406eae0f96479f933c16314439028c",
        "IPY_MODEL_ab6999b799704a739782d2979a01f177",
        "IPY_MODEL_13ab9848e0704fd58f0225dac2309aff"
       ],
       "layout": "IPY_MODEL_418e697fecc6403bb9a345cfa0b9ba13"
      }
     },
     "2309039c8a3c4486a131f72a59579b5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1204205c64b84807a5fca4016ca2dd49",
       "placeholder": "​",
       "style": "IPY_MODEL_a5138202c3ed40c095dcc2298902129f",
       "value": " 28.0/28.0 [00:00&lt;00:00, 1.08kB/s]"
      }
     },
     "2c444ef95dcd45e4accba73e1cfb6493": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_914cb29533374765957640a1fd084f01",
       "placeholder": "​",
       "style": "IPY_MODEL_862ee8e52c4a45568cc5150ecdb449f1",
       "value": "Downloading: 100%"
      }
     },
     "2f0cb1385e334d82b5037174aa587777": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3649d7adedf44a69afc3c290ba04a231": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39082aeb96ef48138cc28b16ded45a55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e9ef267876ad48ce88bb1995bafc7f9b",
       "placeholder": "​",
       "style": "IPY_MODEL_e0eb3aedce5e4d9cb6b24687c1980c6a",
       "value": " 420M/420M [00:09&lt;00:00, 50.4MB/s]"
      }
     },
     "3b406eae0f96479f933c16314439028c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_056b6ffdc96c4b60a76e7f7dafe0433f",
       "placeholder": "​",
       "style": "IPY_MODEL_701463373e3e4f2abb038718d9b8a8c6",
       "value": "Downloading: 100%"
      }
     },
     "416f643aedf54b91a25b0fced1b30eaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0152531c1634f94a142fbe74b3b7024",
       "max": 440473133.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7efda83958564e048c37bd18dd7968eb",
       "value": 440473133.0
      }
     },
     "418e697fecc6403bb9a345cfa0b9ba13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43bbf782e3004af3879f95eae6a8dbe6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_65f8813b5a9d4156aa971e27827a2d85",
       "placeholder": "​",
       "style": "IPY_MODEL_ad257b427f3d4ddbb52cc9cb31009b5d",
       "value": " 455k/455k [00:00&lt;00:00, 842kB/s]"
      }
     },
     "4dc53e9abfc04fe68e363de1c9c4e610": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e15e34b81a604c12907f5d5ebe75933a",
       "placeholder": "​",
       "style": "IPY_MODEL_555e834808af44e693085669d473ad3b",
       "value": " 570/570 [00:00&lt;00:00, 21.8kB/s]"
      }
     },
     "524af31a833b498091bd11f072f83dee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2c444ef95dcd45e4accba73e1cfb6493",
        "IPY_MODEL_8e757781e1a946f7a04d2f6854632f63",
        "IPY_MODEL_2309039c8a3c4486a131f72a59579b5d"
       ],
       "layout": "IPY_MODEL_b033681c85f04759a509257264910b53"
      }
     },
     "555e834808af44e693085669d473ad3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6324d9d3407e40b4a7624927b6212b28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63b4f7e0838e46618bb5e9b967306f37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_76f5e9d2c7e0456c88a29cf81453be25",
        "IPY_MODEL_416f643aedf54b91a25b0fced1b30eaf",
        "IPY_MODEL_39082aeb96ef48138cc28b16ded45a55"
       ],
       "layout": "IPY_MODEL_2f0cb1385e334d82b5037174aa587777"
      }
     },
     "63b8cbc7432645efafcb2888d5bcbb02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "65e7b4ea22e74822a5a7e0b613d0facc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "65f8813b5a9d4156aa971e27827a2d85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68b7055c0ae946f8a59dcfb87e16d481": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_006b4c76f50042cfa110b9b4c4d376c2",
       "max": 570.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7044fbcb3a1c4f779c7d31f3a70b23c3",
       "value": 570.0
      }
     },
     "701463373e3e4f2abb038718d9b8a8c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7044fbcb3a1c4f779c7d31f3a70b23c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "76f5e9d2c7e0456c88a29cf81453be25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_77f0a839267f414ab0d392c7d0c2e078",
       "placeholder": "​",
       "style": "IPY_MODEL_7ccb541c28f24ab78e4ee30e2cb94a48",
       "value": "Downloading: 100%"
      }
     },
     "77f0a839267f414ab0d392c7d0c2e078": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b98d20ab81b4b7aa726accfb5e5a1f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ccb541c28f24ab78e4ee30e2cb94a48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7d254a7400c143c3abf4a19b80c7eb4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b391bcd1b2854317ba9eaaf78146b34f",
        "IPY_MODEL_c2b821c700ab49858e7657766e361f65",
        "IPY_MODEL_43bbf782e3004af3879f95eae6a8dbe6"
       ],
       "layout": "IPY_MODEL_eb13ff2ab4e4489d9e3c2a9d9b0b6dca"
      }
     },
     "7efda83958564e048c37bd18dd7968eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "82d3bae1a5614702b5d0be9f6bdc0dc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7b98d20ab81b4b7aa726accfb5e5a1f7",
       "placeholder": "​",
       "style": "IPY_MODEL_98626bc2141d4c859c889b60c1039e7e",
       "value": "Downloading: 100%"
      }
     },
     "862ee8e52c4a45568cc5150ecdb449f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8e757781e1a946f7a04d2f6854632f63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_14768feddc10486d94bab49200d9b66b",
       "max": 28.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_990c2da30d5042678fcc76051ba4ab6c",
       "value": 28.0
      }
     },
     "90eb6cd0c92f4317bce1302a1a0625b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "914cb29533374765957640a1fd084f01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98626bc2141d4c859c889b60c1039e7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "98edc0ccfe144277a31c3774266dc2b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "990c2da30d5042678fcc76051ba4ab6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a5138202c3ed40c095dcc2298902129f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ab6999b799704a739782d2979a01f177": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_98edc0ccfe144277a31c3774266dc2b9",
       "max": 231508.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_65e7b4ea22e74822a5a7e0b613d0facc",
       "value": 231508.0
      }
     },
     "ad257b427f3d4ddbb52cc9cb31009b5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b0152531c1634f94a142fbe74b3b7024": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b033681c85f04759a509257264910b53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b391bcd1b2854317ba9eaaf78146b34f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_90eb6cd0c92f4317bce1302a1a0625b5",
       "placeholder": "​",
       "style": "IPY_MODEL_63b8cbc7432645efafcb2888d5bcbb02",
       "value": "Downloading: 100%"
      }
     },
     "c2b821c700ab49858e7657766e361f65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3649d7adedf44a69afc3c290ba04a231",
       "max": 466062.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ded9a712d8f340bd99dc4c37e9b91b05",
       "value": 466062.0
      }
     },
     "cd84114c598843b586f338215a7e80d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ded9a712d8f340bd99dc4c37e9b91b05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e0eb3aedce5e4d9cb6b24687c1980c6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e15e34b81a604c12907f5d5ebe75933a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e944be7c98bc402ea9e9fcf027a077e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e9ef267876ad48ce88bb1995bafc7f9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb13ff2ab4e4489d9e3c2a9d9b0b6dca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
