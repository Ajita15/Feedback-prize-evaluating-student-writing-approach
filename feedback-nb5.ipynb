{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f233520",
   "metadata": {
    "papermill": {
     "duration": 0.027724,
     "end_time": "2022-03-20T16:52:42.702186",
     "exception": false,
     "start_time": "2022-03-20T16:52:42.674462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Approach3\n",
    " - For Feedback prize-2021 competition, approach 2 will be divide each content into batch of 512, in that way we won't need to trim the content till 512 .\n",
    " - https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f - Followed this blog fir this approach.\n",
    " - https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e#6196 - Use Layer wise learning rate . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7faf97a4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-20T16:52:42.766507Z",
     "iopub.status.busy": "2022-03-20T16:52:42.765963Z",
     "iopub.status.idle": "2022-03-20T16:52:42.768394Z",
     "shell.execute_reply": "2022-03-20T16:52:42.768762Z",
     "shell.execute_reply.started": "2022-03-19T11:00:18.905144Z"
    },
    "papermill": {
     "duration": 0.039618,
     "end_time": "2022-03-20T16:52:42.769000",
     "exception": false,
     "start_time": "2022-03-20T16:52:42.729382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4eba5",
   "metadata": {
    "papermill": {
     "duration": 0.025346,
     "end_time": "2022-03-20T16:52:42.819982",
     "exception": false,
     "start_time": "2022-03-20T16:52:42.794636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## **Create Dataset for Transformer model**\n",
    "- Inorder to create Training data, we need to create divide each content, into chunks of 510 tokens. Bert Transformer takes max_size 512\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f95e27a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:52:42.875108Z",
     "iopub.status.busy": "2022-03-20T16:52:42.874571Z",
     "iopub.status.idle": "2022-03-20T16:52:45.124512Z",
     "shell.execute_reply": "2022-03-20T16:52:45.124069Z",
     "shell.execute_reply.started": "2022-03-19T11:00:22.675623Z"
    },
    "papermill": {
     "duration": 2.279065,
     "end_time": "2022-03-20T16:52:45.124633",
     "exception": false,
     "start_time": "2022-03-20T16:52:42.845568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15594 entries, 0 to 15593\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         15594 non-null  object\n",
      " 1   text       15594 non-null  object\n",
      " 2   label_str  15594 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 365.6+ KB\n"
     ]
    }
   ],
   "source": [
    "label_df_new=pd.read_csv(\"../input/feedback-nb1/train_file_labels2.csv\")\n",
    "label_df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80e9ade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:52:45.231980Z",
     "iopub.status.busy": "2022-03-20T16:52:45.206442Z",
     "iopub.status.idle": "2022-03-20T16:52:46.512825Z",
     "shell.execute_reply": "2022-03-20T16:52:46.513285Z",
     "shell.execute_reply.started": "2022-03-19T11:00:24.383351Z"
    },
    "papermill": {
     "duration": 1.36242,
     "end_time": "2022-03-20T16:52:46.513434",
     "exception": false,
     "start_time": "2022-03-20T16:52:45.151014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'I-Evidence': 3489544,\n",
       " 'I-Claim': 824107,\n",
       " 'I-Concluding Statement': 814366,\n",
       " 'I-Lead': 474070,\n",
       " 'O': 305458,\n",
       " 'I-Position': 265856,\n",
       " 'I-Counterclaim': 133965,\n",
       " 'I-Rebuttal': 117445,\n",
       " 'B-Claim': 50202,\n",
       " 'B-Evidence': 45702,\n",
       " 'B-Position': 15419,\n",
       " 'B-Concluding Statement': 13505,\n",
       " 'B-Lead': 9305,\n",
       " 'B-Counterclaim': 5817,\n",
       " 'B-Rebuttal': 4337}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find out the number of unique tags and their count\n",
    "from collections import Counter\n",
    "\n",
    "labels_lst = []\n",
    "for label_str in label_df_new[\"label_str\"]:\n",
    "    labels_lst.extend(label_str.strip().split(\",\"))\n",
    "labels_count = Counter(labels_lst)\n",
    "labels_count = {k: v for k, v in sorted(labels_count.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(len(labels_count))\n",
    "labels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3bca91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:52:46.577485Z",
     "iopub.status.busy": "2022-03-20T16:52:46.576534Z",
     "iopub.status.idle": "2022-03-20T16:53:00.384584Z",
     "shell.execute_reply": "2022-03-20T16:53:00.385048Z",
     "shell.execute_reply.started": "2022-03-19T11:00:26.443037Z"
    },
    "papermill": {
     "duration": 13.840802,
     "end_time": "2022-03-20T16:53:00.385202",
     "exception": false,
     "start_time": "2022-03-20T16:52:46.544400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.16.2)',\n",
       " 'Collecting seqeval[gpu]',\n",
       " '  Downloading seqeval-1.2.2.tar.gz (43 kB)',\n",
       " '  Preparing metadata (setup.py): started',\n",
       " \"  Preparing metadata (setup.py): finished with status 'done'\",\n",
       " 'Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.0)',\n",
       " 'Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)',\n",
       " 'Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)',\n",
       " 'Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)',\n",
       " 'Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.6)',\n",
       " 'Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)',\n",
       " 'Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)',\n",
       " 'Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)',\n",
       " 'Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)',\n",
       " 'Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)',\n",
       " 'Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)',\n",
       " 'Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval[gpu]) (1.0.1)',\n",
       " 'Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)',\n",
       " 'Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)',\n",
       " 'Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.7.3)',\n",
       " 'Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.1.0)',\n",
       " 'Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.0.0)',\n",
       " 'Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)',\n",
       " 'Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)',\n",
       " 'Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)',\n",
       " 'Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)',\n",
       " 'Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)',\n",
       " 'Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)',\n",
       " 'Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)',\n",
       " 'Building wheels for collected packages: seqeval',\n",
       " '  Building wheel for seqeval (setup.py): started',\n",
       " \"  Building wheel for seqeval (setup.py): finished with status 'done'\",\n",
       " '  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=d0e4145cafe57fa002533a4f29ea3a793411ed7672488e0fa8d8d144c1273ffc',\n",
       " '  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7',\n",
       " 'Successfully built seqeval',\n",
       " 'Installing collected packages: seqeval',\n",
       " 'Successfully installed seqeval-1.2.2',\n",
       " \"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip install transformers seqeval[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78f75b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:00.446260Z",
     "iopub.status.busy": "2022-03-20T16:53:00.445516Z",
     "iopub.status.idle": "2022-03-20T16:53:03.548364Z",
     "shell.execute_reply": "2022-03-20T16:53:03.547686Z",
     "shell.execute_reply.started": "2022-03-19T11:00:49.111141Z"
    },
    "papermill": {
     "duration": 3.134347,
     "end_time": "2022-03-20T16:53:03.548505",
     "exception": false,
     "start_time": "2022-03-20T16:53:00.414158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad3bb61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:03.647444Z",
     "iopub.status.busy": "2022-03-20T16:53:03.646731Z",
     "iopub.status.idle": "2022-03-20T16:53:03.650246Z",
     "shell.execute_reply": "2022-03-20T16:53:03.650821Z",
     "shell.execute_reply.started": "2022-03-19T11:00:50.896896Z"
    },
    "papermill": {
     "duration": 0.074982,
     "end_time": "2022-03-20T16:53:03.651010",
     "exception": false,
     "start_time": "2022-03-20T16:53:03.576028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ee921a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:03.714772Z",
     "iopub.status.busy": "2022-03-20T16:53:03.710670Z",
     "iopub.status.idle": "2022-03-20T16:53:03.717406Z",
     "shell.execute_reply": "2022-03-20T16:53:03.717790Z",
     "shell.execute_reply.started": "2022-03-19T11:00:50.921211Z"
    },
    "papermill": {
     "duration": 0.03876,
     "end_time": "2022-03-20T16:53:03.717953",
     "exception": false,
     "start_time": "2022-03-20T16:53:03.679193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-Claim',\n",
       " 1: 'I-Claim',\n",
       " 2: 'B-Evidence',\n",
       " 3: 'I-Evidence',\n",
       " 4: 'B-Position',\n",
       " 5: 'I-Position',\n",
       " 6: 'B-Concluding Statement',\n",
       " 7: 'I-Concluding Statement',\n",
       " 8: 'B-Lead',\n",
       " 9: 'I-Lead',\n",
       " 10: 'B-Counterclaim',\n",
       " 11: 'I-Counterclaim',\n",
       " 12: 'B-Rebuttal',\n",
       " 13: 'I-Rebuttal',\n",
       " 14: 'O'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint as pprint\n",
    "label_to_idx = {'B-Claim': 0, 'I-Claim': 1,\n",
    "                'B-Evidence': 2, 'I-Evidence': 3,\n",
    "                'B-Position': 4, 'I-Position': 5,\n",
    "                'B-Concluding Statement': 6, 'I-Concluding Statement': 7,\n",
    "                'B-Lead': 8, 'I-Lead': 9,\n",
    "                'B-Counterclaim': 10, 'I-Counterclaim': 11,\n",
    "                'B-Rebuttal': 12, 'I-Rebuttal': 13,\n",
    "                'O': 14}\n",
    "idx_to_label = {v:k for k,v in label_to_idx.items()}\n",
    "idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff363738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:03.778380Z",
     "iopub.status.busy": "2022-03-20T16:53:03.777611Z",
     "iopub.status.idle": "2022-03-20T16:53:15.949821Z",
     "shell.execute_reply": "2022-03-20T16:53:15.950839Z",
     "shell.execute_reply.started": "2022-03-19T11:00:51.19842Z"
    },
    "papermill": {
     "duration": 12.205678,
     "end_time": "2022-03-20T16:53:15.951044",
     "exception": false,
     "start_time": "2022-03-20T16:53:03.745366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afef30be51cc453985281e761db25259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184adddd518d48f4ba37935736b3fe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6daa7bd2ebf6478694d4ecfa24a39e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e699e9df334394bc88d036debdb3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e918a796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:16.071349Z",
     "iopub.status.busy": "2022-03-20T16:53:16.070580Z",
     "iopub.status.idle": "2022-03-20T16:53:16.121652Z",
     "shell.execute_reply": "2022-03-20T16:53:16.122229Z",
     "shell.execute_reply.started": "2022-03-19T11:00:54.490282Z"
    },
    "papermill": {
     "duration": 0.122832,
     "end_time": "2022-03-20T16:53:16.122440",
     "exception": false,
     "start_time": "2022-03-20T16:53:15.999608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert-base-uncased-tokenizer/tokenizer_config.json',\n",
       " './bert-base-uncased-tokenizer/special_tokens_map.json',\n",
       " './bert-base-uncased-tokenizer/vocab.txt',\n",
       " './bert-base-uncased-tokenizer/added_tokens.json',\n",
       " './bert-base-uncased-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./bert-base-uncased-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb44b648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:16.233496Z",
     "iopub.status.busy": "2022-03-20T16:53:16.232610Z",
     "iopub.status.idle": "2022-03-20T16:53:16.248018Z",
     "shell.execute_reply": "2022-03-20T16:53:16.247521Z",
     "shell.execute_reply.started": "2022-03-19T11:00:54.570941Z"
    },
    "papermill": {
     "duration": 0.077453,
     "end_time": "2022-03-20T16:53:16.248131",
     "exception": false,
     "start_time": "2022-03-20T16:53:16.170678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# include all these functions in Dataset class\n",
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data.text[index].strip().split()  \n",
    "        word_labels = self.data.label_str[index].split(\",\") \n",
    "\n",
    "        encoding = tokenizer.encode_plus(sentence,\n",
    "                                         add_special_tokens=False,\n",
    "                                         return_tensors=\"pt\",\n",
    "                                         is_split_into_words=True,\n",
    "                                         return_offsets_mapping=True)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [label_to_idx[label] for label in word_labels] \n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"][0]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"][0]):\n",
    "          if mapping[0] == 0 and mapping[1] != 0:\n",
    "            # overwrite label\n",
    "            encoded_labels[idx] = labels[i]\n",
    "            i += 1\n",
    "        encoded_labels = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        # divide the content and encodings into chunks of MAX_LEN\n",
    "        chunksize = self.max_len\n",
    "\n",
    "        # Step4: split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
    "        input_id_chunks = list(encoding['input_ids'][0].split(chunksize - 2))\n",
    "        mask_chunks = list(encoding['attention_mask'][0].split(chunksize - 2))\n",
    "        offset_mapping_chunks = list(encoding['offset_mapping'][0].split(chunksize - 2))\n",
    "        encoded_labels_chunks = list(encoded_labels.split(chunksize - 2))\n",
    "        \n",
    "        \n",
    "        # Step 5: loop through each chunk\n",
    "        for i in range(len(input_id_chunks)):\n",
    "            # add CLS and SEP tokens to input IDs\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "            ])\n",
    "            # add attention tokens to attention mask\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
    "            ])\n",
    "\n",
    "            # add offset_mapping tokens to offset_maps\n",
    "            offset_mapping_chunks[i] = torch.cat([\n",
    "                torch.tensor([[0,0]]), offset_mapping_chunks[i], torch.tensor([[0,0]])\n",
    "            ])\n",
    "\n",
    "            # to the encoded_labels also, add -100\n",
    "            encoded_labels_chunks[i] = torch.cat([\n",
    "                torch.tensor([-100]), encoded_labels_chunks[i], torch.tensor([-100])\n",
    "            ])\n",
    "\n",
    "\n",
    "            # get required padding length\n",
    "            pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "            # check if tensor length satisfies required chunk size\n",
    "            if pad_len > 0:\n",
    "                # if padding length is more than 0, we must add padding\n",
    "                input_id_chunks[i] = torch.cat([\n",
    "                    input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "                ])\n",
    "                mask_chunks[i] = torch.cat([\n",
    "                    mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "                ])\n",
    "                offset_mapping_chunks[i] = torch.cat([\n",
    "                    offset_mapping_chunks[i], torch.Tensor([[0,0]] * pad_len)\n",
    "                ])\n",
    "                encoded_labels_chunks[i] = torch.cat([\n",
    "                    encoded_labels_chunks[i], torch.Tensor([-100] * pad_len)\n",
    "                ])\n",
    "\n",
    "        # Step 6: Stack the chunks\n",
    "        input_ids = torch.stack(input_id_chunks)\n",
    "        attention_mask = torch.stack(mask_chunks)\n",
    "        offset_mapping_merged = torch.stack(offset_mapping_chunks)\n",
    "        encoded_labels_merged = torch.stack(encoded_labels_chunks)\n",
    "\n",
    "        # step 7: turn everything into PyTorch tensors\n",
    "        input_dict = {\n",
    "            'input_ids': input_ids.long(),\n",
    "            'attention_mask': attention_mask.int(),\n",
    "            'offset_mapping': offset_mapping_merged,\n",
    "            'labels': encoded_labels_merged.long()\n",
    "        }\n",
    "        \n",
    "#         item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "#         item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return input_dict\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca5aee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:16.310271Z",
     "iopub.status.busy": "2022-03-20T16:53:16.309493Z",
     "iopub.status.idle": "2022-03-20T16:53:16.345960Z",
     "shell.execute_reply": "2022-03-20T16:53:16.346493Z",
     "shell.execute_reply.started": "2022-03-19T11:00:54.608835Z"
    },
    "papermill": {
     "duration": 0.068989,
     "end_time": "2022-03-20T16:53:16.346653",
     "exception": false,
     "start_time": "2022-03-20T16:53:16.277664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = label_df_new.sample(frac=train_size,random_state=200)\n",
    "test_dataset = label_df_new.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88979ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:16.412787Z",
     "iopub.status.busy": "2022-03-20T16:53:16.412044Z",
     "iopub.status.idle": "2022-03-20T16:53:16.413997Z",
     "shell.execute_reply": "2022-03-20T16:53:16.414394Z",
     "shell.execute_reply.started": "2022-03-19T11:00:54.659664Z"
    },
    "papermill": {
     "duration": 0.035548,
     "end_time": "2022-03-20T16:53:16.414520",
     "exception": false,
     "start_time": "2022-03-20T16:53:16.378972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56f915bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:16.479164Z",
     "iopub.status.busy": "2022-03-20T16:53:16.478491Z",
     "iopub.status.idle": "2022-03-20T16:53:16.480964Z",
     "shell.execute_reply": "2022-03-20T16:53:16.480527Z",
     "shell.execute_reply.started": "2022-03-19T11:00:54.671497Z"
    },
    "papermill": {
     "duration": 0.036648,
     "end_time": "2022-03-20T16:53:16.481074",
     "exception": false,
     "start_time": "2022-03-20T16:53:16.444426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(data_list):\n",
    "    # convert each key into a list of keys\n",
    "    data_key_list = {key: [data[key] for data in data_list] for key in data_list[0]}\n",
    "#     input_ids_list = [data['input_ids'] for data in data_list]\n",
    "#     attention_mask_list = [data['attention_mask'] for data in data_list]\n",
    "#     offset_mapping_list = [data['offset_mapping'] for data in data_list]\n",
    "#     labels_list = [data['labels'] for data in data_list]\n",
    "    \n",
    "    # concat all the key on dim=0\n",
    "    # i.e if one example has (1,512) and 2nd example has (2,512)\n",
    "    # this may be able to handle\n",
    "    data_key_cat = {key: torch.cat(data_key_list[key], dim=0) for key in data_key_list}\n",
    "#     input_ids_cat = torch.cat(input_ids_list, dim=0)\n",
    "#     attention_mask_cat = torch.cat(attention_mask_list, dim=0)\n",
    "#     offset_mapping_cat = torch.cat(offset_mapping_list, dim=0)\n",
    "#     labels_cat = torch.cat(labels_list, dim=0)\n",
    "\n",
    "    return data_key_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c83c672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:16.545363Z",
     "iopub.status.busy": "2022-03-20T16:53:16.544606Z",
     "iopub.status.idle": "2022-03-20T16:53:16.546483Z",
     "shell.execute_reply": "2022-03-20T16:53:16.546877Z",
     "shell.execute_reply.started": "2022-03-19T11:00:54.685569Z"
    },
    "papermill": {
     "duration": 0.036632,
     "end_time": "2022-03-20T16:53:16.547003",
     "exception": false,
     "start_time": "2022-03-20T16:53:16.510371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params, collate_fn=custom_collate_fn)\n",
    "testing_loader = DataLoader(testing_set, **test_params, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723ffd6",
   "metadata": {
    "papermill": {
     "duration": 0.02922,
     "end_time": "2022-03-20T16:53:16.605504",
     "exception": false,
     "start_time": "2022-03-20T16:53:16.576284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Load the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9db95136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:16.668944Z",
     "iopub.status.busy": "2022-03-20T16:53:16.668152Z",
     "iopub.status.idle": "2022-03-20T16:53:49.762412Z",
     "shell.execute_reply": "2022-03-20T16:53:49.762843Z",
     "shell.execute_reply.started": "2022-03-19T11:00:55.997759Z"
    },
    "papermill": {
     "duration": 33.128148,
     "end_time": "2022-03-20T16:53:49.762990",
     "exception": false,
     "start_time": "2022-03-20T16:53:16.634842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f509c7f515488d82f21732551dfbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_to_idx))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539ad566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:49.835326Z",
     "iopub.status.busy": "2022-03-20T16:53:49.834734Z",
     "iopub.status.idle": "2022-03-20T16:53:50.782485Z",
     "shell.execute_reply": "2022-03-20T16:53:50.783039Z",
     "shell.execute_reply.started": "2022-03-19T11:01:02.391514Z"
    },
    "papermill": {
     "duration": 0.988369,
     "end_time": "2022-03-20T16:53:50.783194",
     "exception": false,
     "start_time": "2022-03-20T16:53:49.794825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (790 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.8092, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sanity check\n",
    "inputs = training_set[1]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids[0], attention_mask=attention_mask[0], labels=labels[0])\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc0f9b",
   "metadata": {
    "papermill": {
     "duration": 0.032006,
     "end_time": "2022-03-20T16:53:50.847469",
     "exception": false,
     "start_time": "2022-03-20T16:53:50.815463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Learning Rate for different layers**\n",
    "  - Tried to implement this but facing issues\n",
    "      - ValueError: some parameters appear in more than one parameter group\n",
    "  - Approach 2 - Use Learning rate scheduler with warm-up steps instead. Same Learning rate for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "264376fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:50.918368Z",
     "iopub.status.busy": "2022-03-20T16:53:50.917607Z",
     "iopub.status.idle": "2022-03-20T16:53:50.920068Z",
     "shell.execute_reply": "2022-03-20T16:53:50.919586Z",
     "shell.execute_reply.started": "2022-03-19T11:01:02.953858Z"
    },
    "papermill": {
     "duration": 0.040614,
     "end_time": "2022-03-20T16:53:50.920180",
     "exception": false,
     "start_time": "2022-03-20T16:53:50.879566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt_parameters = []    # To be passed to the optimizer (only parameters of the layers you want to update).\n",
    "named_parameters = list(model.named_parameters()) \n",
    "\n",
    "# According to AAAMLP book by A. Thakur, we generally do not use any decay \n",
    "# for bias and LayerNorm.weight layers.\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "init_lr = 3.5e-6 \n",
    "head_lr = 3.6e-6\n",
    "lr = init_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d90a2353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:51.000900Z",
     "iopub.status.busy": "2022-03-20T16:53:51.000090Z",
     "iopub.status.idle": "2022-03-20T16:53:51.002488Z",
     "shell.execute_reply": "2022-03-20T16:53:51.002076Z",
     "shell.execute_reply.started": "2022-03-19T11:01:02.964972Z"
    },
    "papermill": {
     "duration": 0.05031,
     "end_time": "2022-03-20T16:53:51.002597",
     "exception": false,
     "start_time": "2022-03-20T16:53:50.952287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for 12 hidden layers\n",
    "for layer in range(11, -1, -1):\n",
    "    params_0 = [model.get_parameter(n) for n,p in named_parameters if f\"bert.encoder.layer.{layer}\" in n\n",
    "               and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [model.get_parameter(n) for n,p in named_parameters if f\"bert.encoder.layer.{layer}\" in n\n",
    "               and not any(nd in n for nd in no_decay)]\n",
    "    layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n",
    "    opt_parameters.append(layer_params)\n",
    "    \n",
    "    layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01}\n",
    "    opt_parameters.append(layer_params)\n",
    "    \n",
    "    lr *= 0.9\n",
    "\n",
    "# === Embeddings layer ==========================================================\n",
    "params_0 = [model.get_parameter(n) for n,p in named_parameters if \"embeddings\" in n\n",
    "            and any(nd in n for nd in no_decay)]\n",
    "params_1 = [model.get_parameter(n) for n,p in named_parameters if \"embeddings\" in n\n",
    "            and not any(nd in n for nd in no_decay)]\n",
    "\n",
    "embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0} \n",
    "opt_parameters.append(embed_params)\n",
    "        \n",
    "embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01} \n",
    "opt_parameters.append(embed_params)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ca80e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:51.072095Z",
     "iopub.status.busy": "2022-03-20T16:53:51.070514Z",
     "iopub.status.idle": "2022-03-20T16:53:51.072719Z",
     "shell.execute_reply": "2022-03-20T16:53:51.073154Z",
     "shell.execute_reply.started": "2022-03-19T11:01:03.168384Z"
    },
    "papermill": {
     "duration": 0.037856,
     "end_time": "2022-03-20T16:53:51.073276",
     "exception": false,
     "start_time": "2022-03-20T16:53:51.035420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer = transformers.AdamW(opt_parameters, lr=init_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fec7e",
   "metadata": {
    "papermill": {
     "duration": 0.034275,
     "end_time": "2022-03-20T16:53:51.141770",
     "exception": false,
     "start_time": "2022-03-20T16:53:51.107495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Learning Rate Scheduling with Warm_up_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da60277b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:51.213646Z",
     "iopub.status.busy": "2022-03-20T16:53:51.213106Z",
     "iopub.status.idle": "2022-03-20T16:53:55.109286Z",
     "shell.execute_reply": "2022-03-20T16:53:55.109693Z",
     "shell.execute_reply.started": "2022-03-19T11:01:04.226842Z"
    },
    "papermill": {
     "duration": 3.933976,
     "end_time": "2022-03-20T16:53:55.109866",
     "exception": false,
     "start_time": "2022-03-20T16:53:51.175890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(opt_parameters, lr=init_lr)\n",
    "# optimizer = transformers.AdamW(opt_parameters, lr=init_lr)\n",
    "num_training_steps = EPOCHS * len(training_loader)\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=5e-5)\n",
    "lr_scheduler = transformers.get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=50,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc1a63c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:55.193515Z",
     "iopub.status.busy": "2022-03-20T16:53:55.187541Z",
     "iopub.status.idle": "2022-03-20T16:53:55.195444Z",
     "shell.execute_reply": "2022-03-20T16:53:55.195911Z",
     "shell.execute_reply.started": "2022-03-19T11:01:06.816551Z"
    },
    "papermill": {
     "duration": 0.052687,
     "end_time": "2022-03-20T16:53:55.196048",
     "exception": false,
     "start_time": "2022-03-20T16:53:55.143361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#efining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        # after using custom collate_fn in DataLoader, need to update the same\n",
    "        ids = batch['input_ids'].unsqueeze(0)\n",
    "        mask = batch['attention_mask'].unsqueeze(0)\n",
    "        labels = batch['labels'].unsqueeze(0)\n",
    "        \n",
    "        ids = ids.to(device, dtype = torch.long)\n",
    "        mask = mask.to(device, dtype = torch.long)\n",
    "        labels = labels.to(device, dtype = torch.long)\n",
    "\n",
    "        output = model(input_ids=ids[0], attention_mask=mask[0], labels=labels[0])\n",
    "        loss = output[0]\n",
    "        tr_logits = output[1]\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels[0].size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels[0].view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        # if flattened_target is [1024,] i.e [2*512,] where 2 is batch_size and 512 is seq_length\n",
    "        # and out of 1024 i.e 50 tokens have -100 i.e they don't have a label assigned then\n",
    "        # labels is [974,] and predictions is [974,]\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877ed2e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:55.269419Z",
     "iopub.status.busy": "2022-03-20T16:53:55.268761Z",
     "iopub.status.idle": "2022-03-21T02:09:09.841417Z",
     "shell.execute_reply": "2022-03-21T02:09:09.841853Z"
    },
    "papermill": {
     "duration": 33314.611557,
     "end_time": "2022-03-21T02:09:09.842008",
     "exception": false,
     "start_time": "2022-03-20T16:53:55.230451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 2.744267463684082\n",
      "Training loss per 100 training steps: 1.6253047402542415\n",
      "Training loss per 100 training steps: 1.4794797138195133\n",
      "Training loss per 100 training steps: 1.3954441541453138\n",
      "Training loss per 100 training steps: 1.325261650834595\n",
      "Training loss per 100 training steps: 1.2831850871592463\n",
      "Training loss per 100 training steps: 1.2425245122485074\n",
      "Training loss per 100 training steps: 1.208709621097834\n",
      "Training loss per 100 training steps: 1.1802214182048254\n",
      "Training loss per 100 training steps: 1.1592794021676305\n",
      "Training loss per 100 training steps: 1.1407017129939514\n",
      "Training loss per 100 training steps: 1.1282861508043325\n",
      "Training loss per 100 training steps: 1.1110868513286363\n",
      "Training loss per 100 training steps: 1.097030442188008\n",
      "Training loss per 100 training steps: 1.0852865195972081\n",
      "Training loss per 100 training steps: 1.073166509952488\n",
      "Training loss per 100 training steps: 1.0613324443561296\n",
      "Training loss per 100 training steps: 1.0506891832850667\n",
      "Training loss per 100 training steps: 1.0421375384402236\n",
      "Training loss per 100 training steps: 1.0331528181781398\n",
      "Training loss per 100 training steps: 1.0259089562995383\n",
      "Training loss per 100 training steps: 1.0175683447246266\n",
      "Training loss per 100 training steps: 1.010097066626826\n",
      "Training loss per 100 training steps: 1.005366561731169\n",
      "Training loss per 100 training steps: 1.00026701056848\n",
      "Training loss per 100 training steps: 0.9943797876302455\n",
      "Training loss per 100 training steps: 0.9898428908967826\n",
      "Training loss per 100 training steps: 0.98557660994332\n",
      "Training loss per 100 training steps: 0.9803442929587761\n",
      "Training loss per 100 training steps: 0.9756409385129526\n",
      "Training loss per 100 training steps: 0.9705138279473293\n",
      "Training loss per 100 training steps: 0.9645607069216402\n",
      "Training loss per 100 training steps: 0.9597601247453869\n",
      "Training loss per 100 training steps: 0.9564088261450684\n",
      "Training loss per 100 training steps: 0.9509569638285768\n",
      "Training loss per 100 training steps: 0.9478737013199029\n",
      "Training loss per 100 training steps: 0.9437924012876954\n",
      "Training loss per 100 training steps: 0.9409947705748789\n",
      "Training loss per 100 training steps: 0.9377411337423814\n",
      "Training loss per 100 training steps: 0.9324639141483204\n",
      "Training loss per 100 training steps: 0.9301782212415894\n",
      "Training loss per 100 training steps: 0.9278165429387258\n",
      "Training loss per 100 training steps: 0.9256901937498986\n",
      "Training loss per 100 training steps: 0.9240585376227687\n",
      "Training loss per 100 training steps: 0.9209119490576559\n",
      "Training loss per 100 training steps: 0.9186867790019823\n",
      "Training loss per 100 training steps: 0.9156998547295647\n",
      "Training loss per 100 training steps: 0.9129449899394165\n",
      "Training loss per 100 training steps: 0.9087730943194481\n",
      "Training loss per 100 training steps: 0.9071200625205911\n",
      "Training loss per 100 training steps: 0.9044139031272391\n",
      "Training loss per 100 training steps: 0.9015551265624896\n",
      "Training loss per 100 training steps: 0.8997450669845282\n",
      "Training loss per 100 training steps: 0.8979383526045609\n",
      "Training loss per 100 training steps: 0.8952613883196392\n",
      "Training loss per 100 training steps: 0.8929092168558773\n",
      "Training loss per 100 training steps: 0.891310603767394\n",
      "Training loss per 100 training steps: 0.8892846240367498\n",
      "Training loss per 100 training steps: 0.8876565984766394\n",
      "Training loss per 100 training steps: 0.8855551970629263\n",
      "Training loss per 100 training steps: 0.8844151018987276\n",
      "Training loss per 100 training steps: 0.8831859397421195\n",
      "Training loss per 100 training steps: 0.8811481275533096\n",
      "Training loss epoch: 0.8804624160729604\n",
      "Training accuracy epoch: 0.7107787346460963\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.6085411310195923\n",
      "Training loss per 100 training steps: 0.7580227993502475\n",
      "Training loss per 100 training steps: 0.7392963919770065\n",
      "Training loss per 100 training steps: 0.7338104890133456\n",
      "Training loss per 100 training steps: 0.7202564016021695\n",
      "Training loss per 100 training steps: 0.714711255299117\n",
      "Training loss per 100 training steps: 0.7114768230081993\n",
      "Training loss per 100 training steps: 0.7059458592606339\n",
      "Training loss per 100 training steps: 0.6997553968288182\n",
      "Training loss per 100 training steps: 0.6967709125510596\n",
      "Training loss per 100 training steps: 0.701295793160692\n",
      "Training loss per 100 training steps: 0.6996865457805258\n",
      "Training loss per 100 training steps: 0.7020643768858453\n",
      "Training loss per 100 training steps: 0.7018487200793809\n",
      "Training loss per 100 training steps: 0.7002001846691781\n",
      "Training loss per 100 training steps: 0.6963552628097178\n",
      "Training loss per 100 training steps: 0.6985683293453535\n",
      "Training loss per 100 training steps: 0.6984655171860533\n",
      "Training loss per 100 training steps: 0.7001885052590421\n",
      "Training loss per 100 training steps: 0.7025602524575906\n",
      "Training loss per 100 training steps: 0.7031908348269728\n",
      "Training loss per 100 training steps: 0.702581147894242\n",
      "Training loss per 100 training steps: 0.6994904832967787\n",
      "Training loss per 100 training steps: 0.7001181774129561\n",
      "Training loss per 100 training steps: 0.7008630894450335\n",
      "Training loss per 100 training steps: 0.7004050800546271\n",
      "Training loss per 100 training steps: 0.700685765469638\n",
      "Training loss per 100 training steps: 0.7021872245109068\n",
      "Training loss per 100 training steps: 0.7015838507923302\n",
      "Training loss per 100 training steps: 0.7016893027723349\n",
      "Training loss per 100 training steps: 0.7016307307368157\n",
      "Training loss per 100 training steps: 0.7014933395764398\n",
      "Training loss per 100 training steps: 0.7010554012815419\n",
      "Training loss per 100 training steps: 0.6998467302501148\n",
      "Training loss per 100 training steps: 0.7007068752523942\n",
      "Training loss per 100 training steps: 0.7010059885444453\n",
      "Training loss per 100 training steps: 0.7020114728044516\n",
      "Training loss per 100 training steps: 0.7007204520037225\n",
      "Training loss per 100 training steps: 0.6998161679387124\n",
      "Training loss per 100 training steps: 0.7001538328736662\n",
      "Training loss per 100 training steps: 0.6991777484251898\n",
      "Training loss per 100 training steps: 0.6991102364594632\n",
      "Training loss per 100 training steps: 0.6989563978078733\n",
      "Training loss per 100 training steps: 0.698006868310458\n",
      "Training loss per 100 training steps: 0.6977867380959477\n",
      "Training loss per 100 training steps: 0.6984708768093594\n",
      "Training loss per 100 training steps: 0.6986413187201814\n",
      "Training loss per 100 training steps: 0.6993274546333943\n",
      "Training loss per 100 training steps: 0.6997798035738741\n",
      "Training loss per 100 training steps: 0.6989668574723096\n",
      "Training loss per 100 training steps: 0.6988674133890892\n",
      "Training loss per 100 training steps: 0.6984327193733011\n",
      "Training loss per 100 training steps: 0.6981199030617388\n",
      "Training loss per 100 training steps: 0.698170949430696\n",
      "Training loss per 100 training steps: 0.6977184008716588\n",
      "Training loss per 100 training steps: 0.6965997890707927\n",
      "Training loss per 100 training steps: 0.6962488840529162\n",
      "Training loss per 100 training steps: 0.6965936764387306\n",
      "Training loss per 100 training steps: 0.6967825987404368\n",
      "Training loss per 100 training steps: 0.697897334545068\n",
      "Training loss per 100 training steps: 0.6985079102408109\n",
      "Training loss per 100 training steps: 0.6979298504932535\n",
      "Training loss per 100 training steps: 0.6977501524886552\n",
      "Training loss epoch: 0.6981603268769239\n",
      "Training accuracy epoch: 0.7641954946909025\n",
      "Training epoch: 3\n",
      "Training loss per 100 training steps: 0.9874451160430908\n",
      "Training loss per 100 training steps: 0.5829615401159419\n",
      "Training loss per 100 training steps: 0.5667836719484471\n",
      "Training loss per 100 training steps: 0.5738473173688813\n",
      "Training loss per 100 training steps: 0.5639130245346083\n",
      "Training loss per 100 training steps: 0.571013971865772\n",
      "Training loss per 100 training steps: 0.574532991067542\n",
      "Training loss per 100 training steps: 0.5821481147268189\n",
      "Training loss per 100 training steps: 0.5808427475588152\n",
      "Training loss per 100 training steps: 0.5865935537381125\n",
      "Training loss per 100 training steps: 0.5881013273597359\n",
      "Training loss per 100 training steps: 0.5875140898450736\n",
      "Training loss per 100 training steps: 0.5893419385328579\n",
      "Training loss per 100 training steps: 0.596554698346671\n",
      "Training loss per 100 training steps: 0.5965755334034892\n",
      "Training loss per 100 training steps: 0.5989857509106815\n",
      "Training loss per 100 training steps: 0.5980207064258241\n",
      "Training loss per 100 training steps: 0.5992152627375741\n",
      "Training loss per 100 training steps: 0.5978657077710408\n",
      "Training loss per 100 training steps: 0.5976197070773811\n",
      "Training loss per 100 training steps: 0.5981907685999868\n",
      "Training loss per 100 training steps: 0.5982379038197151\n",
      "Training loss per 100 training steps: 0.5997082890163276\n",
      "Training loss per 100 training steps: 0.6016079342427434\n",
      "Training loss per 100 training steps: 0.6021858613002504\n",
      "Training loss per 100 training steps: 0.6013731264421006\n",
      "Training loss per 100 training steps: 0.6018601922273223\n",
      "Training loss per 100 training steps: 0.6008431624727353\n",
      "Training loss per 100 training steps: 0.6006398384766211\n",
      "Training loss per 100 training steps: 0.6000882380355312\n",
      "Training loss per 100 training steps: 0.600009308757384\n",
      "Training loss per 100 training steps: 0.5992980353685587\n",
      "Training loss per 100 training steps: 0.6001753044669348\n",
      "Training loss per 100 training steps: 0.6002286240927959\n",
      "Training loss per 100 training steps: 0.5997865673519345\n",
      "Training loss per 100 training steps: 0.5994777059412895\n",
      "Training loss per 100 training steps: 0.6004470471924823\n",
      "Training loss per 100 training steps: 0.6001959295489923\n",
      "Training loss per 100 training steps: 0.6016209301933493\n",
      "Training loss per 100 training steps: 0.6017586967404822\n",
      "Training loss per 100 training steps: 0.6023143423524269\n",
      "Training loss per 100 training steps: 0.6022351453636251\n",
      "Training loss per 100 training steps: 0.6023758567321644\n",
      "Training loss per 100 training steps: 0.6020749338149637\n",
      "Training loss per 100 training steps: 0.601989933954697\n",
      "Training loss per 100 training steps: 0.6026913893504875\n",
      "Training loss per 100 training steps: 0.6033560149444495\n",
      "Training loss per 100 training steps: 0.6038490830552348\n",
      "Training loss per 100 training steps: 0.6037460995065598\n",
      "Training loss per 100 training steps: 0.6031859439475223\n",
      "Training loss per 100 training steps: 0.603198979156443\n",
      "Training loss per 100 training steps: 0.6026282437807368\n",
      "Training loss per 100 training steps: 0.6024631808721957\n",
      "Training loss per 100 training steps: 0.602330749746573\n",
      "Training loss per 100 training steps: 0.6027953874259784\n",
      "Training loss per 100 training steps: 0.6025913485143536\n",
      "Training loss per 100 training steps: 0.6025867057091912\n",
      "Training loss per 100 training steps: 0.6028192028262138\n",
      "Training loss per 100 training steps: 0.603370534978875\n",
      "Training loss per 100 training steps: 0.6036420591621253\n",
      "Training loss per 100 training steps: 0.6033555567425959\n",
      "Training loss per 100 training steps: 0.6028725503763885\n",
      "Training loss per 100 training steps: 0.6029573495247021\n",
      "Training loss epoch: 0.6030295265276545\n",
      "Training accuracy epoch: 0.7934555321767668\n",
      "Training epoch: 4\n",
      "Training loss per 100 training steps: 0.40705353021621704\n",
      "Training loss per 100 training steps: 0.5247284190194441\n",
      "Training loss per 100 training steps: 0.524796045352867\n",
      "Training loss per 100 training steps: 0.5177709620408838\n",
      "Training loss per 100 training steps: 0.5101410992759422\n",
      "Training loss per 100 training steps: 0.4993727067987362\n",
      "Training loss per 100 training steps: 0.4974604100484816\n",
      "Training loss per 100 training steps: 0.49381075242531625\n",
      "Training loss per 100 training steps: 0.4956863390074836\n",
      "Training loss per 100 training steps: 0.49541338406643776\n",
      "Training loss per 100 training steps: 0.49462672057268503\n",
      "Training loss per 100 training steps: 0.4933729078908274\n",
      "Training loss per 100 training steps: 0.4991869919504552\n",
      "Training loss per 100 training steps: 0.49878029474884067\n",
      "Training loss per 100 training steps: 0.49936846231006027\n",
      "Training loss per 100 training steps: 0.5000817716310296\n",
      "Training loss per 100 training steps: 0.5000243872906699\n",
      "Training loss per 100 training steps: 0.4980698831462636\n",
      "Training loss per 100 training steps: 0.4972204272032843\n",
      "Training loss per 100 training steps: 0.4978568497329935\n",
      "Training loss per 100 training steps: 0.4979894667387247\n",
      "Training loss per 100 training steps: 0.4979543516759019\n",
      "Training loss per 100 training steps: 0.49803024362510356\n",
      "Training loss per 100 training steps: 0.49907571662125094\n",
      "Training loss per 100 training steps: 0.4989525788664967\n",
      "Training loss per 100 training steps: 0.4993959003772701\n",
      "Training loss per 100 training steps: 0.5002210791874785\n",
      "Training loss per 100 training steps: 0.5004438544612865\n",
      "Training loss per 100 training steps: 0.5002929956912568\n",
      "Training loss per 100 training steps: 0.5010611839779942\n",
      "Training loss per 100 training steps: 0.5016411406865361\n",
      "Training loss per 100 training steps: 0.5034698526975956\n",
      "Training loss per 100 training steps: 0.504875051953371\n",
      "Training loss per 100 training steps: 0.5044620697451736\n",
      "Training loss per 100 training steps: 0.5042760253688652\n",
      "Training loss per 100 training steps: 0.5047198569211099\n",
      "Training loss per 100 training steps: 0.5048840768848383\n",
      "Training loss per 100 training steps: 0.5054807841890536\n",
      "Training loss per 100 training steps: 0.5062838313423373\n",
      "Training loss per 100 training steps: 0.5068779116111118\n",
      "Training loss per 100 training steps: 0.507623984521477\n",
      "Training loss per 100 training steps: 0.507685306232438\n",
      "Training loss per 100 training steps: 0.5080985603856277\n",
      "Training loss per 100 training steps: 0.5082172696573725\n",
      "Training loss per 100 training steps: 0.5083285946271158\n",
      "Training loss per 100 training steps: 0.5080389389716514\n",
      "Training loss per 100 training steps: 0.5084353149984938\n",
      "Training loss per 100 training steps: 0.5082839341060363\n",
      "Training loss per 100 training steps: 0.5082285941148382\n",
      "Training loss per 100 training steps: 0.5082502463341088\n",
      "Training loss per 100 training steps: 0.509360859388442\n",
      "Training loss per 100 training steps: 0.5103879721922527\n",
      "Training loss per 100 training steps: 0.5107057223809531\n",
      "Training loss per 100 training steps: 0.5107644013020424\n",
      "Training loss per 100 training steps: 0.511231871077974\n",
      "Training loss per 100 training steps: 0.5114416554360265\n",
      "Training loss per 100 training steps: 0.5117240295145336\n",
      "Training loss per 100 training steps: 0.5123398668340028\n",
      "Training loss per 100 training steps: 0.5130135367504535\n",
      "Training loss per 100 training steps: 0.513375091617253\n",
      "Training loss per 100 training steps: 0.5139477280032345\n",
      "Training loss per 100 training steps: 0.5143817470394779\n",
      "Training loss per 100 training steps: 0.5150922822287354\n",
      "Training loss epoch: 0.515113026386831\n",
      "Training accuracy epoch: 0.8220477515295012\n",
      "Training epoch: 5\n",
      "Training loss per 100 training steps: 0.40447354316711426\n",
      "Training loss per 100 training steps: 0.41917812927524645\n",
      "Training loss per 100 training steps: 0.4316890556272583\n",
      "Training loss per 100 training steps: 0.4283715841976115\n",
      "Training loss per 100 training steps: 0.42196667026217444\n",
      "Training loss per 100 training steps: 0.42370832719637486\n",
      "Training loss per 100 training steps: 0.4273064144997053\n",
      "Training loss per 100 training steps: 0.4269665444851687\n",
      "Training loss per 100 training steps: 0.42800379728203114\n",
      "Training loss per 100 training steps: 0.4276522248197806\n",
      "Training loss per 100 training steps: 0.4266532562583774\n",
      "Training loss per 100 training steps: 0.4254152302568496\n",
      "Training loss per 100 training steps: 0.42591105191522494\n",
      "Training loss per 100 training steps: 0.4263255967859531\n",
      "Training loss per 100 training steps: 0.4242443774004522\n",
      "Training loss per 100 training steps: 0.42686552038387726\n",
      "Training loss per 100 training steps: 0.4281532443352486\n",
      "Training loss per 100 training steps: 0.42597028929116026\n",
      "Training loss per 100 training steps: 0.42597542501063496\n",
      "Training loss per 100 training steps: 0.42872122625993275\n",
      "Training loss per 100 training steps: 0.4287036328908356\n",
      "Training loss per 100 training steps: 0.4294151780432687\n",
      "Training loss per 100 training steps: 0.43097013309225607\n",
      "Training loss per 100 training steps: 0.43239628991475576\n",
      "Training loss per 100 training steps: 0.43317738451569443\n",
      "Training loss per 100 training steps: 0.4338363657970993\n",
      "Training loss per 100 training steps: 0.4338049015222903\n",
      "Training loss per 100 training steps: 0.43437193864001383\n",
      "Training loss per 100 training steps: 0.43599658330682345\n",
      "Training loss per 100 training steps: 0.43701821576158\n",
      "Training loss per 100 training steps: 0.43625034175388655\n",
      "Training loss per 100 training steps: 0.43526976992145194\n",
      "Training loss per 100 training steps: 0.43378207005586\n",
      "Training loss per 100 training steps: 0.4345265864712005\n",
      "Training loss per 100 training steps: 0.4342109402425322\n",
      "Training loss per 100 training steps: 0.4340197864054816\n",
      "Training loss per 100 training steps: 0.4349378960181759\n",
      "Training loss per 100 training steps: 0.43543433379487584\n",
      "Training loss per 100 training steps: 0.43526069266573125\n",
      "Training loss per 100 training steps: 0.4350147270252539\n",
      "Training loss per 100 training steps: 0.4343342789092889\n",
      "Training loss per 100 training steps: 0.43369733476436767\n",
      "Training loss per 100 training steps: 0.4334643131347674\n",
      "Training loss per 100 training steps: 0.4333761380948568\n",
      "Training loss per 100 training steps: 0.4329929346456985\n",
      "Training loss per 100 training steps: 0.4332770203496583\n",
      "Training loss per 100 training steps: 0.4329423164024583\n",
      "Training loss per 100 training steps: 0.4331469121188008\n",
      "Training loss per 100 training steps: 0.43260830830587893\n",
      "Training loss per 100 training steps: 0.4327587317709557\n",
      "Training loss per 100 training steps: 0.43342270722683135\n",
      "Training loss per 100 training steps: 0.43353553855829297\n",
      "Training loss per 100 training steps: 0.43333478989801233\n",
      "Training loss per 100 training steps: 0.43378316473874295\n",
      "Training loss per 100 training steps: 0.434070158677396\n",
      "Training loss per 100 training steps: 0.43412532061040454\n",
      "Training loss per 100 training steps: 0.4340550060680117\n",
      "Training loss per 100 training steps: 0.4349297065692311\n",
      "Training loss per 100 training steps: 0.4353440647645873\n",
      "Training loss per 100 training steps: 0.4351558765654361\n",
      "Training loss per 100 training steps: 0.43615813448545915\n",
      "Training loss per 100 training steps: 0.4360796738610739\n",
      "Training loss per 100 training steps: 0.4364199517662728\n",
      "Training loss epoch: 0.4366915392647305\n",
      "Training accuracy epoch: 0.8482148099583918\n",
      "Training epoch: 6\n",
      "Training loss per 100 training steps: 0.22754669189453125\n",
      "Training loss per 100 training steps: 0.3423907685250339\n",
      "Training loss per 100 training steps: 0.33265561732783244\n",
      "Training loss per 100 training steps: 0.3500872979627495\n",
      "Training loss per 100 training steps: 0.35127385769810465\n",
      "Training loss per 100 training steps: 0.3520168124209205\n",
      "Training loss per 100 training steps: 0.34584501995309913\n",
      "Training loss per 100 training steps: 0.3484333378904514\n",
      "Training loss per 100 training steps: 0.35140877038231133\n",
      "Training loss per 100 training steps: 0.35275756120971186\n",
      "Training loss per 100 training steps: 0.3572757246696598\n",
      "Training loss per 100 training steps: 0.35972796818858976\n",
      "Training loss per 100 training steps: 0.3604115976081179\n",
      "Training loss per 100 training steps: 0.3608404701299318\n",
      "Training loss per 100 training steps: 0.3616242866569613\n",
      "Training loss per 100 training steps: 0.3619157012432893\n",
      "Training loss per 100 training steps: 0.36305272250990617\n",
      "Training loss per 100 training steps: 0.364090826271484\n",
      "Training loss per 100 training steps: 0.36322867956171756\n",
      "Training loss per 100 training steps: 0.3636903793211519\n",
      "Training loss per 100 training steps: 0.3645695498206656\n",
      "Training loss per 100 training steps: 0.36536011875193497\n",
      "Training loss per 100 training steps: 0.36498222375345307\n",
      "Training loss per 100 training steps: 0.36559557969293766\n",
      "Training loss per 100 training steps: 0.36625598037288815\n",
      "Training loss per 100 training steps: 0.36744312892129116\n",
      "Training loss per 100 training steps: 0.3664834889261391\n",
      "Training loss per 100 training steps: 0.3658836243942419\n",
      "Training loss per 100 training steps: 0.3662133750501423\n",
      "Training loss per 100 training steps: 0.3654992264361268\n",
      "Training loss per 100 training steps: 0.36570904450723823\n",
      "Training loss per 100 training steps: 0.3652759690386697\n",
      "Training loss per 100 training steps: 0.3665072536459661\n",
      "Training loss per 100 training steps: 0.36631039080882355\n",
      "Training loss per 100 training steps: 0.36695773403361\n",
      "Training loss per 100 training steps: 0.36755057244692757\n",
      "Training loss per 100 training steps: 0.3673594993997186\n",
      "Training loss per 100 training steps: 0.36788014521919465\n",
      "Training loss per 100 training steps: 0.36761998656085526\n",
      "Training loss per 100 training steps: 0.3679142167910616\n",
      "Training loss per 100 training steps: 0.36799559506553464\n",
      "Training loss per 100 training steps: 0.36823798977774225\n",
      "Training loss per 100 training steps: 0.3697233357890408\n",
      "Training loss per 100 training steps: 0.37062699139253313\n",
      "Training loss per 100 training steps: 0.37150332778506023\n",
      "Training loss per 100 training steps: 0.3721618176713318\n",
      "Training loss per 100 training steps: 0.37248017769197594\n",
      "Training loss per 100 training steps: 0.3730103822717817\n",
      "Training loss per 100 training steps: 0.37356562394911835\n",
      "Training loss per 100 training steps: 0.37340924577812984\n",
      "Training loss per 100 training steps: 0.37293035600768687\n",
      "Training loss per 100 training steps: 0.37261218628306597\n",
      "Training loss per 100 training steps: 0.3723167050282466\n",
      "Training loss per 100 training steps: 0.37287029748864386\n",
      "Training loss per 100 training steps: 0.3730858450466131\n",
      "Training loss per 100 training steps: 0.3723333153248624\n",
      "Training loss per 100 training steps: 0.3721950107929675\n",
      "Training loss per 100 training steps: 0.3732261932155235\n",
      "Training loss per 100 training steps: 0.3733308527498559\n",
      "Training loss per 100 training steps: 0.3732582962235451\n",
      "Training loss per 100 training steps: 0.3731609938960784\n",
      "Training loss per 100 training steps: 0.3729146560612828\n",
      "Training loss per 100 training steps: 0.37314625699995235\n",
      "Training loss epoch: 0.3735226918449049\n",
      "Training accuracy epoch: 0.870468503048935\n",
      "Training epoch: 7\n",
      "Training loss per 100 training steps: 0.19811373949050903\n",
      "Training loss per 100 training steps: 0.28525073789428956\n",
      "Training loss per 100 training steps: 0.29370331882837397\n",
      "Training loss per 100 training steps: 0.30490866971768416\n",
      "Training loss per 100 training steps: 0.30328786350842424\n",
      "Training loss per 100 training steps: 0.29997312972794277\n",
      "Training loss per 100 training steps: 0.29813483663014884\n",
      "Training loss per 100 training steps: 0.297569130227161\n",
      "Training loss per 100 training steps: 0.3007813356119149\n",
      "Training loss per 100 training steps: 0.301754797524571\n",
      "Training loss per 100 training steps: 0.30143645105438754\n",
      "Training loss per 100 training steps: 0.30121633540043824\n",
      "Training loss per 100 training steps: 0.2999614495768535\n",
      "Training loss per 100 training steps: 0.300153411712947\n",
      "Training loss per 100 training steps: 0.3005685120911406\n",
      "Training loss per 100 training steps: 0.30217394776776346\n",
      "Training loss per 100 training steps: 0.30450842539540757\n",
      "Training loss per 100 training steps: 0.3040027995383053\n",
      "Training loss per 100 training steps: 0.3035979240942438\n",
      "Training loss per 100 training steps: 0.30498642806181275\n",
      "Training loss per 100 training steps: 0.3053270985783338\n",
      "Training loss per 100 training steps: 0.3064157677134379\n",
      "Training loss per 100 training steps: 0.3070391293078566\n",
      "Training loss per 100 training steps: 0.3072250821441088\n",
      "Training loss per 100 training steps: 0.30588025886445974\n",
      "Training loss per 100 training steps: 0.3051235474964849\n",
      "Training loss per 100 training steps: 0.3050961808667807\n",
      "Training loss per 100 training steps: 0.30608145814142107\n",
      "Training loss per 100 training steps: 0.3064830309921867\n",
      "Training loss per 100 training steps: 0.3071201341197804\n",
      "Training loss per 100 training steps: 0.308669545528532\n",
      "Training loss per 100 training steps: 0.3105422981210733\n",
      "Training loss per 100 training steps: 0.31112323420675647\n",
      "Training loss per 100 training steps: 0.31014950893496573\n",
      "Training loss per 100 training steps: 0.3104784583842113\n",
      "Training loss per 100 training steps: 0.31033080620268216\n",
      "Training loss per 100 training steps: 0.3105624151234824\n",
      "Training loss per 100 training steps: 0.3102120246996882\n",
      "Training loss per 100 training steps: 0.30973317613396417\n",
      "Training loss per 100 training steps: 0.3100799326626147\n",
      "Training loss per 100 training steps: 0.3098203845050254\n",
      "Training loss per 100 training steps: 0.3098589978972257\n",
      "Training loss per 100 training steps: 0.3100769523211077\n",
      "Training loss per 100 training steps: 0.3102678705168807\n",
      "Training loss per 100 training steps: 0.3101347447630087\n",
      "Training loss per 100 training steps: 0.3096554357781129\n",
      "Training loss per 100 training steps: 0.3096242127634408\n",
      "Training loss per 100 training steps: 0.3092741705596105\n",
      "Training loss per 100 training steps: 0.30989398131753426\n",
      "Training loss per 100 training steps: 0.3098299830933051\n",
      "Training loss per 100 training steps: 0.3102128655627188\n",
      "Training loss per 100 training steps: 0.3103198755530312\n",
      "Training loss per 100 training steps: 0.31003300208434353\n",
      "Training loss per 100 training steps: 0.31091054223816567\n",
      "Training loss per 100 training steps: 0.31106193121291603\n",
      "Training loss per 100 training steps: 0.3110759345026661\n",
      "Training loss per 100 training steps: 0.3108671047644804\n",
      "Training loss per 100 training steps: 0.31105353045405537\n",
      "Training loss per 100 training steps: 0.31136695407247506\n",
      "Training loss per 100 training steps: 0.31134666736624217\n",
      "Training loss per 100 training steps: 0.31155985916414236\n",
      "Training loss per 100 training steps: 0.3122113634897996\n",
      "Training loss per 100 training steps: 0.31222512151212717\n",
      "Training loss epoch: 0.3123666547595948\n",
      "Training accuracy epoch: 0.8913966698128833\n",
      "Training epoch: 8\n",
      "Training loss per 100 training steps: 0.2342202067375183\n",
      "Training loss per 100 training steps: 0.210035536622647\n",
      "Training loss per 100 training steps: 0.23953512051508793\n",
      "Training loss per 100 training steps: 0.2392271436330092\n",
      "Training loss per 100 training steps: 0.2424400234293016\n",
      "Training loss per 100 training steps: 0.24241113080503698\n",
      "Training loss per 100 training steps: 0.24184453437498424\n",
      "Training loss per 100 training steps: 0.2469231758489886\n",
      "Training loss per 100 training steps: 0.24324796581117625\n",
      "Training loss per 100 training steps: 0.24092045127642606\n",
      "Training loss per 100 training steps: 0.23772944763980128\n",
      "Training loss per 100 training steps: 0.23883254094037493\n",
      "Training loss per 100 training steps: 0.23943282393653328\n",
      "Training loss per 100 training steps: 0.24129051050825545\n",
      "Training loss per 100 training steps: 0.242003341518518\n",
      "Training loss per 100 training steps: 0.2437318642062199\n",
      "Training loss per 100 training steps: 0.24561388314766186\n",
      "Training loss per 100 training steps: 0.24509726403207727\n",
      "Training loss per 100 training steps: 0.24426830212675088\n",
      "Training loss per 100 training steps: 0.24608034615859053\n",
      "Training loss per 100 training steps: 0.2475600088111077\n",
      "Training loss per 100 training steps: 0.24808725108991403\n",
      "Training loss per 100 training steps: 0.24850547712961418\n",
      "Training loss per 100 training steps: 0.25095182555893203\n",
      "Training loss per 100 training steps: 0.2525347723473936\n",
      "Training loss per 100 training steps: 0.2530551975379225\n",
      "Training loss per 100 training steps: 0.2535702861696539\n",
      "Training loss per 100 training steps: 0.2556091487057361\n",
      "Training loss per 100 training steps: 0.2560664967390064\n",
      "Training loss per 100 training steps: 0.2555304396331249\n",
      "Training loss per 100 training steps: 0.2566813219578236\n",
      "Training loss per 100 training steps: 0.2566734794353101\n",
      "Training loss per 100 training steps: 0.25678105948516744\n",
      "Training loss per 100 training steps: 0.25741430938636595\n",
      "Training loss per 100 training steps: 0.2570686355612184\n",
      "Training loss per 100 training steps: 0.2573984571666343\n",
      "Training loss per 100 training steps: 0.2574325688281587\n",
      "Training loss per 100 training steps: 0.2581269566263622\n",
      "Training loss per 100 training steps: 0.2582883640287762\n",
      "Training loss per 100 training steps: 0.2586495268147394\n",
      "Training loss per 100 training steps: 0.25911249461430397\n",
      "Training loss per 100 training steps: 0.25877035000983517\n",
      "Training loss per 100 training steps: 0.2589836184292769\n",
      "Training loss per 100 training steps: 0.25914312755505836\n",
      "Training loss per 100 training steps: 0.2594988361539366\n",
      "Training loss per 100 training steps: 0.2601970510167881\n",
      "Training loss per 100 training steps: 0.2602599250608956\n",
      "Training loss per 100 training steps: 0.2603757945732331\n",
      "Training loss per 100 training steps: 0.2604699474137678\n",
      "Training loss per 100 training steps: 0.26027153742113324\n",
      "Training loss per 100 training steps: 0.26064490646621485\n",
      "Training loss per 100 training steps: 0.2609364186720132\n",
      "Training loss per 100 training steps: 0.2609497831152726\n",
      "Training loss per 100 training steps: 0.2607458046549807\n",
      "Training loss per 100 training steps: 0.26141363501664644\n",
      "Training loss per 100 training steps: 0.2617676292448407\n",
      "Training loss per 100 training steps: 0.26213568933022124\n",
      "Training loss per 100 training steps: 0.2622305524153702\n",
      "Training loss per 100 training steps: 0.2621913039380929\n",
      "Training loss per 100 training steps: 0.26279847352050395\n",
      "Training loss per 100 training steps: 0.2631307411650244\n",
      "Training loss per 100 training steps: 0.26325072778104625\n",
      "Training loss per 100 training steps: 0.26317701438149604\n",
      "Training loss epoch: 0.2634023384445799\n",
      "Training accuracy epoch: 0.9096338616208979\n",
      "Training epoch: 9\n",
      "Training loss per 100 training steps: 0.0913706123828888\n",
      "Training loss per 100 training steps: 0.21301871622995575\n",
      "Training loss per 100 training steps: 0.2067621218513197\n",
      "Training loss per 100 training steps: 0.20799806643512558\n",
      "Training loss per 100 training steps: 0.203638256985016\n",
      "Training loss per 100 training steps: 0.2065261012712817\n",
      "Training loss per 100 training steps: 0.20769153707993904\n",
      "Training loss per 100 training steps: 0.21039173212407233\n",
      "Training loss per 100 training steps: 0.21391037103025878\n",
      "Training loss per 100 training steps: 0.21344565561911572\n",
      "Training loss per 100 training steps: 0.21424060770227657\n",
      "Training loss per 100 training steps: 0.21443087859358384\n",
      "Training loss per 100 training steps: 0.2130819764965331\n",
      "Training loss per 100 training steps: 0.21326120913194793\n",
      "Training loss per 100 training steps: 0.2131803678342013\n",
      "Training loss per 100 training steps: 0.21443802154784913\n",
      "Training loss per 100 training steps: 0.2150643533561088\n",
      "Training loss per 100 training steps: 0.2152569486104228\n",
      "Training loss per 100 training steps: 0.21440293059230828\n",
      "Training loss per 100 training steps: 0.2152852149012017\n",
      "Training loss per 100 training steps: 0.21621276203799863\n",
      "Training loss per 100 training steps: 0.2165575107818435\n",
      "Training loss per 100 training steps: 0.21614871997139107\n",
      "Training loss per 100 training steps: 0.2162563438170704\n",
      "Training loss per 100 training steps: 0.21593005619268213\n",
      "Training loss per 100 training steps: 0.21518795480128708\n",
      "Training loss per 100 training steps: 0.21447954063727356\n",
      "Training loss per 100 training steps: 0.2137662652071037\n",
      "Training loss per 100 training steps: 0.21374080987046687\n",
      "Training loss per 100 training steps: 0.21357776753710997\n",
      "Training loss per 100 training steps: 0.21412317480045343\n",
      "Training loss per 100 training steps: 0.2146433018095688\n",
      "Training loss per 100 training steps: 0.21429953477917119\n",
      "Training loss per 100 training steps: 0.21394222915275274\n",
      "Training loss per 100 training steps: 0.2143143371300171\n",
      "Training loss per 100 training steps: 0.2146086414613019\n",
      "Training loss per 100 training steps: 0.2149316787068499\n",
      "Training loss per 100 training steps: 0.21497724145893446\n",
      "Training loss per 100 training steps: 0.21496936417327128\n",
      "Training loss per 100 training steps: 0.2158951014593068\n",
      "Training loss per 100 training steps: 0.21579063295832956\n",
      "Training loss per 100 training steps: 0.21605918355766657\n",
      "Training loss per 100 training steps: 0.2156526458182968\n",
      "Training loss per 100 training steps: 0.215399158792646\n",
      "Training loss per 100 training steps: 0.21526637758553482\n",
      "Training loss per 100 training steps: 0.2154689593918471\n",
      "Training loss per 100 training steps: 0.2161671379896865\n",
      "Training loss per 100 training steps: 0.216368380419108\n",
      "Training loss per 100 training steps: 0.21700711024626765\n",
      "Training loss per 100 training steps: 0.21699349744611401\n",
      "Training loss per 100 training steps: 0.21724285291803197\n",
      "Training loss per 100 training steps: 0.21693985205555583\n",
      "Training loss per 100 training steps: 0.216959736541478\n",
      "Training loss per 100 training steps: 0.21736774136591824\n",
      "Training loss per 100 training steps: 0.21773886651358015\n",
      "Training loss per 100 training steps: 0.21848566998578467\n",
      "Training loss per 100 training steps: 0.21846511640243926\n",
      "Training loss per 100 training steps: 0.218868082398057\n",
      "Training loss per 100 training steps: 0.21922949614917445\n",
      "Training loss per 100 training steps: 0.2193031944302942\n",
      "Training loss per 100 training steps: 0.21979382634349137\n",
      "Training loss per 100 training steps: 0.21988724998933157\n",
      "Training loss per 100 training steps: 0.22004606595349693\n",
      "Training loss epoch: 0.22003183797994375\n",
      "Training accuracy epoch: 0.9248006374144051\n",
      "Training epoch: 10\n",
      "Training loss per 100 training steps: 0.12066007405519485\n",
      "Training loss per 100 training steps: 0.17556978393309186\n",
      "Training loss per 100 training steps: 0.16642996816159183\n",
      "Training loss per 100 training steps: 0.17153719238367587\n",
      "Training loss per 100 training steps: 0.1710095702208337\n",
      "Training loss per 100 training steps: 0.171923473492235\n",
      "Training loss per 100 training steps: 0.17246883856860948\n",
      "Training loss per 100 training steps: 0.17311727898341528\n",
      "Training loss per 100 training steps: 0.17435507643293502\n",
      "Training loss per 100 training steps: 0.17383720047490778\n",
      "Training loss per 100 training steps: 0.17358947233236455\n",
      "Training loss per 100 training steps: 0.1741980891962359\n",
      "Training loss per 100 training steps: 0.17374148677342838\n",
      "Training loss per 100 training steps: 0.17435867586484513\n",
      "Training loss per 100 training steps: 0.17407087742721264\n",
      "Training loss per 100 training steps: 0.17433263125365908\n",
      "Training loss per 100 training steps: 0.17458437506585317\n",
      "Training loss per 100 training steps: 0.174800399144173\n",
      "Training loss per 100 training steps: 0.17619388366918567\n",
      "Training loss per 100 training steps: 0.17695378651352564\n",
      "Training loss per 100 training steps: 0.17728839855783549\n",
      "Training loss per 100 training steps: 0.1780127094376359\n",
      "Training loss per 100 training steps: 0.17866347530418358\n",
      "Training loss per 100 training steps: 0.17920372304022927\n",
      "Training loss per 100 training steps: 0.17900468940916053\n",
      "Training loss per 100 training steps: 0.17998390236343653\n",
      "Training loss per 100 training steps: 0.17992790969144531\n",
      "Training loss per 100 training steps: 0.1803601264071943\n",
      "Training loss per 100 training steps: 0.1801436672312025\n",
      "Training loss per 100 training steps: 0.17982309342367445\n",
      "Training loss per 100 training steps: 0.1798248379559544\n",
      "Training loss per 100 training steps: 0.18047091471702845\n",
      "Training loss per 100 training steps: 0.17981419336037366\n",
      "Training loss per 100 training steps: 0.17947103372419249\n",
      "Training loss per 100 training steps: 0.179509636739138\n",
      "Training loss per 100 training steps: 0.18013810114142312\n",
      "Training loss per 100 training steps: 0.18037897085779242\n",
      "Training loss per 100 training steps: 0.1800631839726387\n",
      "Training loss per 100 training steps: 0.18068565258298583\n",
      "Training loss per 100 training steps: 0.18102909041085427\n",
      "Training loss per 100 training steps: 0.18113289244407338\n",
      "Training loss per 100 training steps: 0.18156084901910077\n",
      "Training loss per 100 training steps: 0.18183276501101356\n",
      "Training loss per 100 training steps: 0.18204447572348845\n",
      "Training loss per 100 training steps: 0.18235857702746827\n",
      "Training loss per 100 training steps: 0.18279341371472155\n",
      "Training loss per 100 training steps: 0.18319500296222202\n",
      "Training loss per 100 training steps: 0.18398587134565902\n",
      "Training loss per 100 training steps: 0.18406424334284768\n",
      "Training loss per 100 training steps: 0.18422503730955264\n",
      "Training loss per 100 training steps: 0.184504099824339\n",
      "Training loss per 100 training steps: 0.18478489259465686\n",
      "Training loss per 100 training steps: 0.18523415640140498\n",
      "Training loss per 100 training steps: 0.18491114623520655\n",
      "Training loss per 100 training steps: 0.18449200589314635\n",
      "Training loss per 100 training steps: 0.18426086913274187\n",
      "Training loss per 100 training steps: 0.18436315021906802\n",
      "Training loss per 100 training steps: 0.1848382098540478\n",
      "Training loss per 100 training steps: 0.18529460114410695\n",
      "Training loss per 100 training steps: 0.18517101501171326\n",
      "Training loss per 100 training steps: 0.18553078873742573\n",
      "Training loss per 100 training steps: 0.18546201789547404\n",
      "Training loss per 100 training steps: 0.18566819537629853\n",
      "Training loss epoch: 0.1856463179278924\n",
      "Training accuracy epoch: 0.9372931399349025\n",
      "Training epoch: 11\n",
      "Training loss per 100 training steps: 0.10156827419996262\n",
      "Training loss per 100 training steps: 0.15225918935888474\n",
      "Training loss per 100 training steps: 0.15219205247229012\n",
      "Training loss per 100 training steps: 0.14935341208207845\n",
      "Training loss per 100 training steps: 0.15163954896875598\n",
      "Training loss per 100 training steps: 0.14885482627121424\n",
      "Training loss per 100 training steps: 0.15015898957405358\n",
      "Training loss per 100 training steps: 0.1478333565441044\n",
      "Training loss per 100 training steps: 0.1472552729108965\n",
      "Training loss per 100 training steps: 0.14891248701872112\n",
      "Training loss per 100 training steps: 0.14890998897545568\n",
      "Training loss per 100 training steps: 0.14800338555494497\n",
      "Training loss per 100 training steps: 0.14717125308371887\n",
      "Training loss per 100 training steps: 0.14856953912741755\n",
      "Training loss per 100 training steps: 0.14914669367922823\n",
      "Training loss per 100 training steps: 0.1501560493907224\n",
      "Training loss per 100 training steps: 0.1500176620450939\n",
      "Training loss per 100 training steps: 0.15046836210355635\n",
      "Training loss per 100 training steps: 0.14979211428538297\n",
      "Training loss per 100 training steps: 0.14954075338402228\n",
      "Training loss per 100 training steps: 0.14934915644831132\n",
      "Training loss per 100 training steps: 0.15050058557905935\n",
      "Training loss per 100 training steps: 0.1508993775833877\n",
      "Training loss per 100 training steps: 0.15100257192252253\n",
      "Training loss per 100 training steps: 0.15100903873188715\n",
      "Training loss per 100 training steps: 0.15109658870557066\n",
      "Training loss per 100 training steps: 0.15107893167120526\n",
      "Training loss per 100 training steps: 0.15098596815861517\n",
      "Training loss per 100 training steps: 0.1512277886935306\n",
      "Training loss per 100 training steps: 0.15139651107675658\n",
      "Training loss per 100 training steps: 0.1520087399580651\n",
      "Training loss per 100 training steps: 0.1519073084296652\n",
      "Training loss per 100 training steps: 0.15200033782348815\n",
      "Training loss per 100 training steps: 0.15238721273176178\n",
      "Training loss per 100 training steps: 0.1529258185868491\n",
      "Training loss per 100 training steps: 0.15282127206475532\n",
      "Training loss per 100 training steps: 0.15290149851710946\n",
      "Training loss per 100 training steps: 0.15292916766111683\n",
      "Training loss per 100 training steps: 0.15329292914999212\n",
      "Training loss per 100 training steps: 0.1533750244248582\n",
      "Training loss per 100 training steps: 0.15370334859992596\n",
      "Training loss per 100 training steps: 0.15366581088072565\n",
      "Training loss per 100 training steps: 0.15372852410375584\n",
      "Training loss per 100 training steps: 0.1545096516477213\n",
      "Training loss per 100 training steps: 0.15449926142019602\n",
      "Training loss per 100 training steps: 0.15453014894538836\n",
      "Training loss per 100 training steps: 0.15507807538494867\n",
      "Training loss per 100 training steps: 0.1551028943385959\n",
      "Training loss per 100 training steps: 0.15496276726128974\n",
      "Training loss per 100 training steps: 0.15527804027407657\n",
      "Training loss per 100 training steps: 0.15496067008445139\n",
      "Training loss per 100 training steps: 0.15473226782131136\n",
      "Training loss per 100 training steps: 0.15512099499995785\n",
      "Training loss per 100 training steps: 0.15522362276954077\n",
      "Training loss per 100 training steps: 0.15533043101825333\n",
      "Training loss per 100 training steps: 0.1556413542962884\n",
      "Training loss per 100 training steps: 0.1558111319757803\n",
      "Training loss per 100 training steps: 0.15615412899374032\n",
      "Training loss per 100 training steps: 0.15611642475441898\n",
      "Training loss per 100 training steps: 0.15627104176032086\n",
      "Training loss per 100 training steps: 0.15598022787705396\n",
      "Training loss per 100 training steps: 0.15613849180113384\n",
      "Training loss per 100 training steps: 0.15650389329536307\n",
      "Training loss epoch: 0.15652123571213922\n",
      "Training accuracy epoch: 0.9474321902030404\n",
      "Training epoch: 12\n",
      "Training loss per 100 training steps: 0.07717772573232651\n",
      "Training loss per 100 training steps: 0.13605660489658908\n",
      "Training loss per 100 training steps: 0.13337539116022598\n",
      "Training loss per 100 training steps: 0.12741732814247128\n",
      "Training loss per 100 training steps: 0.1243656063011088\n",
      "Training loss per 100 training steps: 0.12609644720847615\n",
      "Training loss per 100 training steps: 0.12387745166677802\n",
      "Training loss per 100 training steps: 0.12347350368002555\n",
      "Training loss per 100 training steps: 0.1216185193220514\n",
      "Training loss per 100 training steps: 0.12338118706121792\n",
      "Training loss per 100 training steps: 0.12447419705950505\n",
      "Training loss per 100 training steps: 0.12550614127545845\n",
      "Training loss per 100 training steps: 0.12638901263943794\n",
      "Training loss per 100 training steps: 0.1267857616094387\n",
      "Training loss per 100 training steps: 0.12647792940718694\n",
      "Training loss per 100 training steps: 0.12548232235220433\n",
      "Training loss per 100 training steps: 0.12671464079087\n",
      "Training loss per 100 training steps: 0.12752279113629927\n",
      "Training loss per 100 training steps: 0.12838059645156474\n",
      "Training loss per 100 training steps: 0.1287572128396444\n",
      "Training loss per 100 training steps: 0.12962557497001004\n",
      "Training loss per 100 training steps: 0.1291189588320888\n",
      "Training loss per 100 training steps: 0.12984338533560635\n",
      "Training loss per 100 training steps: 0.1304877422673499\n",
      "Training loss per 100 training steps: 0.13079351087704746\n",
      "Training loss per 100 training steps: 0.13164884341219332\n",
      "Training loss per 100 training steps: 0.13272021501960632\n",
      "Training loss per 100 training steps: 0.13357591844170386\n",
      "Training loss per 100 training steps: 0.13355844297424033\n",
      "Training loss per 100 training steps: 0.13360870528246427\n",
      "Training loss per 100 training steps: 0.13424864699176497\n",
      "Training loss per 100 training steps: 0.13447133517307247\n",
      "Training loss per 100 training steps: 0.1344369940510062\n",
      "Training loss per 100 training steps: 0.1344826680617122\n",
      "Training loss per 100 training steps: 0.1351600918324346\n",
      "Training loss per 100 training steps: 0.13586694194015644\n",
      "Training loss per 100 training steps: 0.1358633383978453\n",
      "Training loss per 100 training steps: 0.13614480553237157\n",
      "Training loss per 100 training steps: 0.13572635545665573\n",
      "Training loss per 100 training steps: 0.13585644398112365\n",
      "Training loss per 100 training steps: 0.13572360956468763\n",
      "Training loss per 100 training steps: 0.13568587805287788\n",
      "Training loss per 100 training steps: 0.1357252763016063\n",
      "Training loss per 100 training steps: 0.1358400694223654\n",
      "Training loss per 100 training steps: 0.13601100190907253\n",
      "Training loss per 100 training steps: 0.13620424482293164\n",
      "Training loss per 100 training steps: 0.1358479976904443\n",
      "Training loss per 100 training steps: 0.13535331768801737\n",
      "Training loss per 100 training steps: 0.13523000164498308\n",
      "Training loss per 100 training steps: 0.1353897447562899\n",
      "Training loss per 100 training steps: 0.13544982468200575\n",
      "Training loss per 100 training steps: 0.13553247216315178\n",
      "Training loss per 100 training steps: 0.13554133166537732\n",
      "Training loss per 100 training steps: 0.13557033363082832\n",
      "Training loss per 100 training steps: 0.13562298177235715\n",
      "Training loss per 100 training steps: 0.1353497612725187\n",
      "Training loss per 100 training steps: 0.1351658209600153\n",
      "Training loss per 100 training steps: 0.13468616927327506\n",
      "Training loss per 100 training steps: 0.1347466401362956\n",
      "Training loss per 100 training steps: 0.13484228137933277\n",
      "Training loss per 100 training steps: 0.1348961132963124\n",
      "Training loss per 100 training steps: 0.13483246380553002\n",
      "Training loss per 100 training steps: 0.13469357500342394\n",
      "Training loss epoch: 0.13474142406117584\n",
      "Training accuracy epoch: 0.9545222630097946\n",
      "Training epoch: 13\n",
      "Training loss per 100 training steps: 0.11775501817464828\n",
      "Training loss per 100 training steps: 0.10365871111355206\n",
      "Training loss per 100 training steps: 0.10861205661539637\n",
      "Training loss per 100 training steps: 0.10718578395226419\n",
      "Training loss per 100 training steps: 0.10953635446369611\n",
      "Training loss per 100 training steps: 0.1077363120899452\n",
      "Training loss per 100 training steps: 0.11037701863203564\n",
      "Training loss per 100 training steps: 0.11123139933004082\n",
      "Training loss per 100 training steps: 0.10919641288806604\n",
      "Training loss per 100 training steps: 0.10824695850394823\n",
      "Training loss per 100 training steps: 0.10696441365301713\n",
      "Training loss per 100 training steps: 0.10631118976569427\n",
      "Training loss per 100 training steps: 0.10601547453649417\n",
      "Training loss per 100 training steps: 0.10656472659980495\n",
      "Training loss per 100 training steps: 0.10716449831835766\n",
      "Training loss per 100 training steps: 0.10736206167583527\n",
      "Training loss per 100 training steps: 0.10740811903472364\n",
      "Training loss per 100 training steps: 0.10646161354181939\n",
      "Training loss per 100 training steps: 0.1072160638065989\n",
      "Training loss per 100 training steps: 0.10751625485137704\n",
      "Training loss per 100 training steps: 0.10760334632657319\n",
      "Training loss per 100 training steps: 0.10823851549718219\n",
      "Training loss per 100 training steps: 0.10894106528154747\n",
      "Training loss per 100 training steps: 0.10929737035992569\n",
      "Training loss per 100 training steps: 0.10979617739754532\n",
      "Training loss per 100 training steps: 0.10962496911304392\n",
      "Training loss per 100 training steps: 0.10994948463355825\n",
      "Training loss per 100 training steps: 0.11035391515486703\n",
      "Training loss per 100 training steps: 0.11051517392851821\n",
      "Training loss per 100 training steps: 0.11085853796387755\n",
      "Training loss per 100 training steps: 0.11037732071400878\n",
      "Training loss per 100 training steps: 0.11067878662023521\n",
      "Training loss per 100 training steps: 0.1107773019069406\n",
      "Training loss per 100 training steps: 0.11124328763991023\n",
      "Training loss per 100 training steps: 0.11128653758476573\n",
      "Training loss per 100 training steps: 0.11182823844574034\n",
      "Training loss per 100 training steps: 0.11219401165481706\n",
      "Training loss per 100 training steps: 0.1121127555456709\n",
      "Training loss per 100 training steps: 0.11221121168760076\n",
      "Training loss per 100 training steps: 0.11245450450870706\n",
      "Training loss per 100 training steps: 0.11255039546774434\n",
      "Training loss per 100 training steps: 0.11277117528505279\n",
      "Training loss per 100 training steps: 0.11303884833243069\n",
      "Training loss per 100 training steps: 0.11329658370688289\n",
      "Training loss per 100 training steps: 0.11317847667024314\n",
      "Training loss per 100 training steps: 0.11331318419709532\n",
      "Training loss per 100 training steps: 0.11358556303805337\n",
      "Training loss per 100 training steps: 0.11371772028591981\n",
      "Training loss per 100 training steps: 0.11371596388349461\n",
      "Training loss per 100 training steps: 0.11348941650881435\n",
      "Training loss per 100 training steps: 0.11363694696911379\n",
      "Training loss per 100 training steps: 0.11366091864743141\n",
      "Training loss per 100 training steps: 0.11370661012752964\n",
      "Training loss per 100 training steps: 0.11368616113768266\n",
      "Training loss per 100 training steps: 0.11371687135674333\n",
      "Training loss per 100 training steps: 0.11379351696258364\n",
      "Training loss per 100 training steps: 0.11392880052139717\n",
      "Training loss per 100 training steps: 0.11409517560346802\n",
      "Training loss per 100 training steps: 0.11400723924791806\n",
      "Training loss per 100 training steps: 0.11413080988621815\n",
      "Training loss per 100 training steps: 0.11407297384610762\n",
      "Training loss per 100 training steps: 0.1141708878373385\n",
      "Training loss per 100 training steps: 0.11451542548007886\n",
      "Training loss epoch: 0.11461875656867951\n",
      "Training accuracy epoch: 0.9619183040530626\n",
      "Training epoch: 14\n",
      "Training loss per 100 training steps: 0.09573132544755936\n",
      "Training loss per 100 training steps: 0.09419861913538806\n",
      "Training loss per 100 training steps: 0.08887554972724461\n",
      "Training loss per 100 training steps: 0.0896564541721834\n",
      "Training loss per 100 training steps: 0.09228783245883566\n",
      "Training loss per 100 training steps: 0.09316223045669779\n",
      "Training loss per 100 training steps: 0.09201072578042585\n",
      "Training loss per 100 training steps: 0.09191261025340948\n",
      "Training loss per 100 training steps: 0.09033515524513294\n",
      "Training loss per 100 training steps: 0.08956029701623425\n",
      "Training loss per 100 training steps: 0.09015325613605504\n",
      "Training loss per 100 training steps: 0.0897120848899922\n",
      "Training loss per 100 training steps: 0.08984260494180248\n",
      "Training loss per 100 training steps: 0.09022907476079758\n",
      "Training loss per 100 training steps: 0.09138844870871796\n",
      "Training loss per 100 training steps: 0.09162594238839061\n",
      "Training loss per 100 training steps: 0.09135330633317318\n",
      "Training loss per 100 training steps: 0.09200728714708643\n",
      "Training loss per 100 training steps: 0.0927065244351236\n",
      "Training loss per 100 training steps: 0.09354109006325398\n",
      "Training loss per 100 training steps: 0.09333492953342845\n",
      "Training loss per 100 training steps: 0.09328793104007035\n",
      "Training loss per 100 training steps: 0.09304701979589419\n",
      "Training loss per 100 training steps: 0.09359015953388698\n",
      "Training loss per 100 training steps: 0.09379629850624574\n",
      "Training loss per 100 training steps: 0.09372461265333218\n",
      "Training loss per 100 training steps: 0.09412902304646657\n",
      "Training loss per 100 training steps: 0.0937696513627621\n",
      "Training loss per 100 training steps: 0.0940305395865622\n",
      "Training loss per 100 training steps: 0.09424688855514524\n",
      "Training loss per 100 training steps: 0.09468450646632423\n",
      "Training loss per 100 training steps: 0.0949480031054318\n",
      "Training loss per 100 training steps: 0.09494666997202192\n",
      "Training loss per 100 training steps: 0.09481025537196208\n",
      "Training loss per 100 training steps: 0.09509352952071708\n",
      "Training loss per 100 training steps: 0.09545145978621358\n",
      "Training loss per 100 training steps: 0.09526602807671367\n",
      "Training loss per 100 training steps: 0.09509983383976824\n",
      "Training loss per 100 training steps: 0.09480836854256387\n",
      "Training loss per 100 training steps: 0.09513212687233132\n",
      "Training loss per 100 training steps: 0.0955065498883007\n",
      "Training loss per 100 training steps: 0.09546909489692411\n",
      "Training loss per 100 training steps: 0.09583467884371964\n",
      "Training loss per 100 training steps: 0.09633593251825291\n",
      "Training loss per 100 training steps: 0.0963646713688441\n",
      "Training loss per 100 training steps: 0.09630098806596797\n",
      "Training loss per 100 training steps: 0.0966496182051962\n",
      "Training loss per 100 training steps: 0.09678879650309187\n",
      "Training loss per 100 training steps: 0.09695549907423792\n",
      "Training loss per 100 training steps: 0.0971926024025291\n",
      "Training loss per 100 training steps: 0.09701344665525684\n",
      "Training loss per 100 training steps: 0.09677193478381813\n",
      "Training loss per 100 training steps: 0.09667668187840996\n",
      "Training loss per 100 training steps: 0.09682824814070116\n",
      "Training loss per 100 training steps: 0.09652577183779516\n",
      "Training loss per 100 training steps: 0.09619446176181194\n",
      "Training loss per 100 training steps: 0.09623617905257674\n",
      "Training loss per 100 training steps: 0.09621498610329281\n",
      "Training loss per 100 training steps: 0.09627050043760345\n",
      "Training loss per 100 training steps: 0.09622752639799069\n",
      "Training loss per 100 training steps: 0.09625579343963671\n",
      "Training loss per 100 training steps: 0.09608206246130262\n",
      "Training loss per 100 training steps: 0.09589844470332054\n",
      "Training loss epoch: 0.09595024085781842\n",
      "Training accuracy epoch: 0.9681805396147816\n",
      "Training epoch: 15\n",
      "Training loss per 100 training steps: 0.04230598360300064\n",
      "Training loss per 100 training steps: 0.08144971805781422\n",
      "Training loss per 100 training steps: 0.07802669969456855\n",
      "Training loss per 100 training steps: 0.07857761057898204\n",
      "Training loss per 100 training steps: 0.07903174031235538\n",
      "Training loss per 100 training steps: 0.07852732006715102\n",
      "Training loss per 100 training steps: 0.07874290291227698\n",
      "Training loss per 100 training steps: 0.07804805324705166\n",
      "Training loss per 100 training steps: 0.0786267883076301\n",
      "Training loss per 100 training steps: 0.07953109969405642\n",
      "Training loss per 100 training steps: 0.07903232362300909\n",
      "Training loss per 100 training steps: 0.07920172627694688\n",
      "Training loss per 100 training steps: 0.08015696202066698\n",
      "Training loss per 100 training steps: 0.07986655540325514\n",
      "Training loss per 100 training steps: 0.08005319847053928\n",
      "Training loss per 100 training steps: 0.08066198804855054\n",
      "Training loss per 100 training steps: 0.08066222982566634\n",
      "Training loss per 100 training steps: 0.08054062763657717\n",
      "Training loss per 100 training steps: 0.08061130402952553\n",
      "Training loss per 100 training steps: 0.08079856178681143\n",
      "Training loss per 100 training steps: 0.081157530528907\n",
      "Training loss per 100 training steps: 0.08122254006960462\n",
      "Training loss per 100 training steps: 0.08073076913258088\n",
      "Training loss per 100 training steps: 0.08081670255911841\n",
      "Training loss per 100 training steps: 0.0804911551283155\n",
      "Training loss per 100 training steps: 0.08024314958860994\n",
      "Training loss per 100 training steps: 0.08036327610090976\n",
      "Training loss per 100 training steps: 0.08049793587980315\n",
      "Training loss per 100 training steps: 0.08024597472535504\n",
      "Training loss per 100 training steps: 0.08031686733087402\n",
      "Training loss per 100 training steps: 0.08097949326960063\n",
      "Training loss per 100 training steps: 0.08109512879542936\n",
      "Training loss per 100 training steps: 0.08111978067906782\n",
      "Training loss per 100 training steps: 0.08160581763002031\n",
      "Training loss per 100 training steps: 0.08179132216816762\n",
      "Training loss per 100 training steps: 0.08209622402932687\n",
      "Training loss per 100 training steps: 0.08224897060110871\n",
      "Training loss per 100 training steps: 0.08230176460354216\n",
      "Training loss per 100 training steps: 0.08250244180920567\n",
      "Training loss per 100 training steps: 0.0827000258160744\n",
      "Training loss per 100 training steps: 0.08287244790632411\n",
      "Training loss per 100 training steps: 0.08323208659592543\n",
      "Training loss per 100 training steps: 0.08328369761187777\n",
      "Training loss per 100 training steps: 0.08359188753422943\n",
      "Training loss per 100 training steps: 0.083709613511699\n",
      "Training loss per 100 training steps: 0.08356423097904925\n",
      "Training loss per 100 training steps: 0.08377918729581257\n",
      "Training loss per 100 training steps: 0.08350706732152571\n",
      "Training loss per 100 training steps: 0.08355794313901652\n",
      "Training loss per 100 training steps: 0.08365264757129884\n",
      "Training loss per 100 training steps: 0.0834556813707504\n",
      "Training loss per 100 training steps: 0.0832887876679702\n",
      "Training loss per 100 training steps: 0.0835690845821681\n",
      "Training loss per 100 training steps: 0.08352157297814565\n",
      "Training loss per 100 training steps: 0.08335665290933147\n",
      "Training loss per 100 training steps: 0.0835062178900444\n",
      "Training loss per 100 training steps: 0.08360943844376627\n",
      "Training loss per 100 training steps: 0.0836139034027019\n",
      "Training loss per 100 training steps: 0.08355745626942615\n",
      "Training loss per 100 training steps: 0.0837063584815243\n",
      "Training loss per 100 training steps: 0.08360878757494919\n",
      "Training loss per 100 training steps: 0.08367226724950219\n",
      "Training loss per 100 training steps: 0.08386438476538126\n",
      "Training loss epoch: 0.08387941332035598\n",
      "Training accuracy epoch: 0.9726179622246239\n",
      "Training epoch: 16\n",
      "Training loss per 100 training steps: 0.012365960516035557\n",
      "Training loss per 100 training steps: 0.05928106339519272\n",
      "Training loss per 100 training steps: 0.059984027058238264\n",
      "Training loss per 100 training steps: 0.06416291537295495\n",
      "Training loss per 100 training steps: 0.0687370871012552\n",
      "Training loss per 100 training steps: 0.06651263131421736\n",
      "Training loss per 100 training steps: 0.06479885497133553\n",
      "Training loss per 100 training steps: 0.06473877522715775\n",
      "Training loss per 100 training steps: 0.0659413729441444\n",
      "Training loss per 100 training steps: 0.06684128523000064\n",
      "Training loss per 100 training steps: 0.0663252651810527\n",
      "Training loss per 100 training steps: 0.06620641747281483\n",
      "Training loss per 100 training steps: 0.06598918414756122\n",
      "Training loss per 100 training steps: 0.06644417702912608\n",
      "Training loss per 100 training steps: 0.06614368282282442\n",
      "Training loss per 100 training steps: 0.06644429617823858\n",
      "Training loss per 100 training steps: 0.06629995698706106\n",
      "Training loss per 100 training steps: 0.06583850716171169\n",
      "Training loss per 100 training steps: 0.06571973055375659\n",
      "Training loss per 100 training steps: 0.06671038922645066\n",
      "Training loss per 100 training steps: 0.06646443949965783\n",
      "Training loss per 100 training steps: 0.06677470898906435\n",
      "Training loss per 100 training steps: 0.067056500403907\n",
      "Training loss per 100 training steps: 0.06698623900708525\n",
      "Training loss per 100 training steps: 0.06699026397216844\n",
      "Training loss per 100 training steps: 0.06687188398821733\n",
      "Training loss per 100 training steps: 0.0669609746146125\n",
      "Training loss per 100 training steps: 0.06728255188757032\n",
      "Training loss per 100 training steps: 0.0675512456539779\n",
      "Training loss per 100 training steps: 0.06808660609346821\n",
      "Training loss per 100 training steps: 0.06827922986197493\n",
      "Training loss per 100 training steps: 0.06882218161520663\n",
      "Training loss per 100 training steps: 0.06878892918523508\n",
      "Training loss per 100 training steps: 0.06862684325749543\n",
      "Training loss per 100 training steps: 0.06877439174675419\n",
      "Training loss per 100 training steps: 0.0689626720080738\n",
      "Training loss per 100 training steps: 0.06934685533376443\n",
      "Training loss per 100 training steps: 0.06928877082793759\n",
      "Training loss per 100 training steps: 0.06912194767870573\n",
      "Training loss per 100 training steps: 0.06916062046343523\n",
      "Training loss per 100 training steps: 0.06908121743061159\n",
      "Training loss per 100 training steps: 0.06915563993571315\n",
      "Training loss per 100 training steps: 0.06915311264604644\n",
      "Training loss per 100 training steps: 0.06943429225712099\n",
      "Training loss per 100 training steps: 0.06942216778326195\n",
      "Training loss per 100 training steps: 0.06916100028804525\n",
      "Training loss per 100 training steps: 0.06928939005917971\n",
      "Training loss per 100 training steps: 0.06951092852308172\n",
      "Training loss per 100 training steps: 0.06948685106742115\n",
      "Training loss per 100 training steps: 0.06951193161392093\n",
      "Training loss per 100 training steps: 0.0694911242904286\n",
      "Training loss per 100 training steps: 0.06943085313404654\n",
      "Training loss per 100 training steps: 0.06974220439305784\n",
      "Training loss per 100 training steps: 0.06966162118918887\n",
      "Training loss per 100 training steps: 0.06963642111277528\n",
      "Training loss per 100 training steps: 0.06962230701587056\n",
      "Training loss per 100 training steps: 0.06948440732498658\n",
      "Training loss per 100 training steps: 0.06944165468365664\n",
      "Training loss per 100 training steps: 0.06957134101500115\n",
      "Training loss per 100 training steps: 0.06946136222913096\n",
      "Training loss per 100 training steps: 0.06948658951694045\n",
      "Training loss per 100 training steps: 0.06963057121148764\n",
      "Training loss per 100 training steps: 0.06967914144451394\n",
      "Training loss epoch: 0.06968042742199769\n",
      "Training accuracy epoch: 0.9772923818640745\n",
      "Training epoch: 17\n",
      "Training loss per 100 training steps: 0.018434705212712288\n",
      "Training loss per 100 training steps: 0.07221550043294261\n",
      "Training loss per 100 training steps: 0.0667671488055876\n",
      "Training loss per 100 training steps: 0.06217486397641714\n",
      "Training loss per 100 training steps: 0.06241507512898163\n",
      "Training loss per 100 training steps: 0.06248370814508946\n",
      "Training loss per 100 training steps: 0.06329187710448703\n",
      "Training loss per 100 training steps: 0.0633386234406079\n",
      "Training loss per 100 training steps: 0.06319694727259256\n",
      "Training loss per 100 training steps: 0.06285198818691712\n",
      "Training loss per 100 training steps: 0.06284565614982063\n",
      "Training loss per 100 training steps: 0.06252125120632093\n",
      "Training loss per 100 training steps: 0.06287334300015435\n",
      "Training loss per 100 training steps: 0.06201810910198966\n",
      "Training loss per 100 training steps: 0.061614395913466555\n",
      "Training loss per 100 training steps: 0.06053879911804089\n",
      "Training loss per 100 training steps: 0.06119991440793091\n",
      "Training loss per 100 training steps: 0.06062150716949703\n",
      "Training loss per 100 training steps: 0.06047105582274273\n",
      "Training loss per 100 training steps: 0.06064387606733676\n",
      "Training loss per 100 training steps: 0.06024408729616385\n",
      "Training loss per 100 training steps: 0.05956908692243416\n",
      "Training loss per 100 training steps: 0.059688224699820006\n",
      "Training loss per 100 training steps: 0.059300567892194685\n",
      "Training loss per 100 training steps: 0.05918832524992434\n",
      "Training loss per 100 training steps: 0.059545347203559404\n",
      "Training loss per 100 training steps: 0.05941190052291816\n",
      "Training loss per 100 training steps: 0.05941237508022267\n",
      "Training loss per 100 training steps: 0.05942494768205623\n",
      "Training loss per 100 training steps: 0.05925976314596538\n",
      "Training loss per 100 training steps: 0.059334211509250556\n",
      "Training loss per 100 training steps: 0.05959860922625905\n",
      "Training loss per 100 training steps: 0.06002083539929164\n",
      "Training loss per 100 training steps: 0.060334018507392013\n",
      "Training loss per 100 training steps: 0.0603044642808389\n",
      "Training loss per 100 training steps: 0.06016247260013302\n",
      "Training loss per 100 training steps: 0.06038618744527484\n",
      "Training loss per 100 training steps: 0.06032660831248735\n",
      "Training loss per 100 training steps: 0.06009866936849364\n",
      "Training loss per 100 training steps: 0.06025619656891438\n",
      "Training loss per 100 training steps: 0.06026787742448287\n",
      "Training loss per 100 training steps: 0.06030015381370685\n",
      "Training loss per 100 training steps: 0.06011861425765604\n",
      "Training loss per 100 training steps: 0.0601394924988201\n",
      "Training loss per 100 training steps: 0.06016803539724281\n",
      "Training loss per 100 training steps: 0.059910298683988894\n",
      "Training loss per 100 training steps: 0.060145680490395204\n",
      "Training loss per 100 training steps: 0.06026353098000298\n",
      "Training loss per 100 training steps: 0.06034241587004976\n",
      "Training loss per 100 training steps: 0.06023901173332526\n",
      "Training loss per 100 training steps: 0.06026563212054174\n",
      "Training loss per 100 training steps: 0.06029673568154285\n",
      "Training loss per 100 training steps: 0.06028602771904467\n",
      "Training loss per 100 training steps: 0.060200096379128036\n",
      "Training loss per 100 training steps: 0.06015308221265928\n",
      "Training loss per 100 training steps: 0.060359649900947165\n",
      "Training loss per 100 training steps: 0.06031299236849023\n",
      "Training loss per 100 training steps: 0.06057797451630588\n",
      "Training loss per 100 training steps: 0.06046239535831277\n",
      "Training loss per 100 training steps: 0.06036395899727505\n",
      "Training loss per 100 training steps: 0.060396369503197364\n",
      "Training loss per 100 training steps: 0.060383184577575466\n",
      "Training loss per 100 training steps: 0.06037247616757435\n",
      "Training loss epoch: 0.06034206778927652\n",
      "Training accuracy epoch: 0.980462228425416\n",
      "Training epoch: 18\n",
      "Training loss per 100 training steps: 0.025964580476284027\n",
      "Training loss per 100 training steps: 0.055370521727064165\n",
      "Training loss per 100 training steps: 0.058020141589645274\n",
      "Training loss per 100 training steps: 0.05432225415789432\n",
      "Training loss per 100 training steps: 0.055310112683153584\n",
      "Training loss per 100 training steps: 0.05372954504108967\n",
      "Training loss per 100 training steps: 0.053700752519520493\n",
      "Training loss per 100 training steps: 0.052141570364896965\n",
      "Training loss per 100 training steps: 0.05100184549085131\n",
      "Training loss per 100 training steps: 0.05052996810035187\n",
      "Training loss per 100 training steps: 0.04891893782452415\n",
      "Training loss per 100 training steps: 0.04891330015147316\n",
      "Training loss per 100 training steps: 0.04941797561853966\n",
      "Training loss per 100 training steps: 0.04951302408193131\n",
      "Training loss per 100 training steps: 0.049997561629266635\n",
      "Training loss per 100 training steps: 0.0497882843712037\n",
      "Training loss per 100 training steps: 0.050153928773165296\n",
      "Training loss per 100 training steps: 0.04962897971788501\n",
      "Training loss per 100 training steps: 0.0491180999855046\n",
      "Training loss per 100 training steps: 0.04939478630864349\n",
      "Training loss per 100 training steps: 0.04955896932486517\n",
      "Training loss per 100 training steps: 0.04936708103415903\n",
      "Training loss per 100 training steps: 0.04988906776104317\n",
      "Training loss per 100 training steps: 0.04979920814701011\n",
      "Training loss per 100 training steps: 0.04971885465000946\n",
      "Training loss per 100 training steps: 0.049495489581064304\n",
      "Training loss per 100 training steps: 0.04954539199773457\n",
      "Training loss per 100 training steps: 0.050045595520851124\n",
      "Training loss per 100 training steps: 0.05048669378836937\n",
      "Training loss per 100 training steps: 0.05049286070818313\n",
      "Training loss per 100 training steps: 0.050434056526989984\n",
      "Training loss per 100 training steps: 0.05064480475791103\n",
      "Training loss per 100 training steps: 0.050459371685598574\n",
      "Training loss per 100 training steps: 0.05028445589214634\n",
      "Training loss per 100 training steps: 0.05040603849349663\n",
      "Training loss per 100 training steps: 0.05053299436115193\n",
      "Training loss per 100 training steps: 0.050571741857185756\n",
      "Training loss per 100 training steps: 0.05066711335019323\n",
      "Training loss per 100 training steps: 0.05103809006177222\n",
      "Training loss per 100 training steps: 0.05107531027520547\n",
      "Training loss per 100 training steps: 0.05110794773499438\n",
      "Training loss per 100 training steps: 0.05099799330520015\n",
      "Training loss per 100 training steps: 0.05092065078290603\n",
      "Training loss per 100 training steps: 0.051033466137417084\n",
      "Training loss per 100 training steps: 0.05112548702014168\n",
      "Training loss per 100 training steps: 0.05120272943791924\n",
      "Training loss per 100 training steps: 0.051211178765263404\n",
      "Training loss per 100 training steps: 0.05128820066062812\n",
      "Training loss per 100 training steps: 0.05119481268805878\n",
      "Training loss per 100 training steps: 0.051259190704455185\n",
      "Training loss per 100 training steps: 0.05124105613693499\n",
      "Training loss per 100 training steps: 0.05113986199807048\n",
      "Training loss per 100 training steps: 0.051012272794755675\n",
      "Training loss per 100 training steps: 0.050990520519233605\n",
      "Training loss per 100 training steps: 0.05087103046318804\n",
      "Training loss per 100 training steps: 0.05080093635399629\n",
      "Training loss per 100 training steps: 0.05072962467407286\n",
      "Training loss per 100 training steps: 0.05101341290923903\n",
      "Training loss per 100 training steps: 0.05112624009546577\n",
      "Training loss per 100 training steps: 0.0510550042342054\n",
      "Training loss per 100 training steps: 0.05112969846060179\n",
      "Training loss per 100 training steps: 0.051222638425205365\n",
      "Training loss per 100 training steps: 0.0511961968254559\n",
      "Training loss epoch: 0.05119411437232874\n",
      "Training accuracy epoch: 0.9836523882630946\n",
      "Training epoch: 19\n",
      "Training loss per 100 training steps: 0.018691129982471466\n",
      "Training loss per 100 training steps: 0.04680356984072835\n",
      "Training loss per 100 training steps: 0.040621576707958434\n",
      "Training loss per 100 training steps: 0.039765159604124155\n",
      "Training loss per 100 training steps: 0.038758304088546004\n",
      "Training loss per 100 training steps: 0.03890841402937872\n",
      "Training loss per 100 training steps: 0.0400660710090449\n",
      "Training loss per 100 training steps: 0.03970797832529811\n",
      "Training loss per 100 training steps: 0.03928453033716161\n",
      "Training loss per 100 training steps: 0.03921163078324123\n",
      "Training loss per 100 training steps: 0.03980671992519876\n",
      "Training loss per 100 training steps: 0.04107130047434655\n",
      "Training loss per 100 training steps: 0.0413526847127276\n",
      "Training loss per 100 training steps: 0.04147810629156664\n",
      "Training loss per 100 training steps: 0.04192373403088774\n",
      "Training loss per 100 training steps: 0.042047928377327015\n",
      "Training loss per 100 training steps: 0.04250511292296623\n",
      "Training loss per 100 training steps: 0.0423866724566683\n",
      "Training loss per 100 training steps: 0.04227854657628889\n",
      "Training loss per 100 training steps: 0.04260541707920797\n",
      "Training loss per 100 training steps: 0.04246371384724734\n",
      "Training loss per 100 training steps: 0.04281969937840091\n",
      "Training loss per 100 training steps: 0.043357094300279085\n",
      "Training loss per 100 training steps: 0.04340424316556601\n",
      "Training loss per 100 training steps: 0.043536379482872124\n",
      "Training loss per 100 training steps: 0.04327866394968904\n",
      "Training loss per 100 training steps: 0.04326892343792142\n",
      "Training loss per 100 training steps: 0.04340708847822272\n",
      "Training loss per 100 training steps: 0.04397954415485578\n",
      "Training loss per 100 training steps: 0.044204027898121986\n",
      "Training loss per 100 training steps: 0.04424862185110931\n",
      "Training loss per 100 training steps: 0.04408117709227143\n",
      "Training loss per 100 training steps: 0.044303878644251164\n",
      "Training loss per 100 training steps: 0.04424233980878473\n",
      "Training loss per 100 training steps: 0.04426908396161579\n",
      "Training loss per 100 training steps: 0.04413089445080944\n",
      "Training loss per 100 training steps: 0.04424507222760994\n",
      "Training loss per 100 training steps: 0.04424104936438784\n",
      "Training loss per 100 training steps: 0.0441957598098085\n",
      "Training loss per 100 training steps: 0.04418136495591916\n",
      "Training loss per 100 training steps: 0.04429250184767334\n",
      "Training loss per 100 training steps: 0.04411728493420789\n",
      "Training loss per 100 training steps: 0.044111827989076716\n",
      "Training loss per 100 training steps: 0.04419768636778146\n",
      "Training loss per 100 training steps: 0.044292344870433036\n",
      "Training loss per 100 training steps: 0.04446703015054437\n",
      "Training loss per 100 training steps: 0.04447150780051992\n",
      "Training loss per 100 training steps: 0.04440688293378097\n",
      "Training loss per 100 training steps: 0.04427744888933715\n",
      "Training loss per 100 training steps: 0.044200381743148734\n",
      "Training loss per 100 training steps: 0.04423805079116082\n",
      "Training loss per 100 training steps: 0.044259151597477175\n",
      "Training loss per 100 training steps: 0.04424810848454717\n",
      "Training loss per 100 training steps: 0.04411247946756178\n",
      "Training loss per 100 training steps: 0.04391755247126073\n",
      "Training loss per 100 training steps: 0.043750392932219134\n",
      "Training loss per 100 training steps: 0.04394567986175672\n",
      "Training loss per 100 training steps: 0.04380104598437969\n",
      "Training loss per 100 training steps: 0.043756231334848066\n",
      "Training loss per 100 training steps: 0.04381529571476032\n",
      "Training loss per 100 training steps: 0.043919610735775466\n",
      "Training loss per 100 training steps: 0.04392656020676954\n",
      "Training loss per 100 training steps: 0.04376051275881415\n",
      "Training loss epoch: 0.04368302195419388\n",
      "Training accuracy epoch: 0.9861535727008907\n",
      "Training epoch: 20\n",
      "Training loss per 100 training steps: 0.015873651951551437\n",
      "Training loss per 100 training steps: 0.0293898645242086\n",
      "Training loss per 100 training steps: 0.03398171177398024\n",
      "Training loss per 100 training steps: 0.036288476550636\n",
      "Training loss per 100 training steps: 0.03467158685807612\n",
      "Training loss per 100 training steps: 0.03361187039288002\n",
      "Training loss per 100 training steps: 0.035046988012681336\n",
      "Training loss per 100 training steps: 0.03460352793475414\n",
      "Training loss per 100 training steps: 0.03491280516568855\n",
      "Training loss per 100 training steps: 0.03594838265217448\n",
      "Training loss per 100 training steps: 0.036222152350531826\n",
      "Training loss per 100 training steps: 0.036482599666883504\n",
      "Training loss per 100 training steps: 0.0366421026671282\n",
      "Training loss per 100 training steps: 0.03644094060486158\n",
      "Training loss per 100 training steps: 0.03645135120774216\n",
      "Training loss per 100 training steps: 0.036550373152930846\n",
      "Training loss per 100 training steps: 0.03645453889514532\n",
      "Training loss per 100 training steps: 0.03639307881571048\n",
      "Training loss per 100 training steps: 0.03645566269790398\n",
      "Training loss per 100 training steps: 0.036044887966922935\n",
      "Training loss per 100 training steps: 0.03610744774737779\n",
      "Training loss per 100 training steps: 0.03597057001032569\n",
      "Training loss per 100 training steps: 0.03575039293847302\n",
      "Training loss per 100 training steps: 0.03552182444613828\n",
      "Training loss per 100 training steps: 0.03555244493763409\n",
      "Training loss per 100 training steps: 0.03553998396037517\n",
      "Training loss per 100 training steps: 0.03542068846333003\n",
      "Training loss per 100 training steps: 0.035592165236513554\n",
      "Training loss per 100 training steps: 0.03537752008751272\n",
      "Training loss per 100 training steps: 0.03549613111596012\n",
      "Training loss per 100 training steps: 0.03553116714148306\n",
      "Training loss per 100 training steps: 0.03566983894323218\n",
      "Training loss per 100 training steps: 0.035687703415857655\n",
      "Training loss per 100 training steps: 0.03588326875069194\n",
      "Training loss per 100 training steps: 0.035810474451239444\n",
      "Training loss per 100 training steps: 0.03571595166597975\n",
      "Training loss per 100 training steps: 0.03573739242368391\n",
      "Training loss per 100 training steps: 0.03583271910358254\n",
      "Training loss per 100 training steps: 0.035851111853771746\n",
      "Training loss per 100 training steps: 0.03594314823649841\n",
      "Training loss per 100 training steps: 0.035972322578263424\n",
      "Training loss per 100 training steps: 0.03609335105632568\n",
      "Training loss per 100 training steps: 0.03628553337431897\n",
      "Training loss per 100 training steps: 0.03621595521162517\n",
      "Training loss per 100 training steps: 0.03620614025635637\n",
      "Training loss per 100 training steps: 0.03625836117528356\n",
      "Training loss per 100 training steps: 0.036227092190405874\n",
      "Training loss per 100 training steps: 0.03614417215130149\n",
      "Training loss per 100 training steps: 0.03622642902959667\n",
      "Training loss per 100 training steps: 0.03621625061784883\n",
      "Training loss per 100 training steps: 0.036244995739725616\n",
      "Training loss per 100 training steps: 0.036349223157297564\n",
      "Training loss per 100 training steps: 0.03630741053106015\n",
      "Training loss per 100 training steps: 0.036301546412819394\n",
      "Training loss per 100 training steps: 0.036390471489191036\n",
      "Training loss per 100 training steps: 0.03654829474164385\n",
      "Training loss per 100 training steps: 0.03661504440059519\n",
      "Training loss per 100 training steps: 0.03662457903291874\n",
      "Training loss per 100 training steps: 0.0365612785823434\n",
      "Training loss per 100 training steps: 0.03651216484268245\n",
      "Training loss per 100 training steps: 0.03648182684753717\n",
      "Training loss per 100 training steps: 0.0364826886970846\n",
      "Training loss per 100 training steps: 0.036476038514482216\n",
      "Training loss epoch: 0.03644234162140426\n",
      "Training accuracy epoch: 0.9883272224968509\n",
      "Training epoch: 21\n",
      "Training loss per 100 training steps: 0.02086571976542473\n",
      "Training loss per 100 training steps: 0.03308717220431507\n",
      "Training loss per 100 training steps: 0.03176268027106124\n",
      "Training loss per 100 training steps: 0.029790448808061747\n",
      "Training loss per 100 training steps: 0.03022103121607539\n",
      "Training loss per 100 training steps: 0.030545633376163325\n",
      "Training loss per 100 training steps: 0.030603032454944366\n",
      "Training loss per 100 training steps: 0.0306200919508105\n",
      "Training loss per 100 training steps: 0.030667004538537206\n",
      "Training loss per 100 training steps: 0.031178624438846506\n",
      "Training loss per 100 training steps: 0.03093525192000904\n",
      "Training loss per 100 training steps: 0.031012581190025413\n",
      "Training loss per 100 training steps: 0.030761556770099244\n",
      "Training loss per 100 training steps: 0.030777741597829707\n",
      "Training loss per 100 training steps: 0.030668853367849996\n",
      "Training loss per 100 training steps: 0.030931212921522316\n",
      "Training loss per 100 training steps: 0.03078100074369543\n",
      "Training loss per 100 training steps: 0.030801784517803337\n",
      "Training loss per 100 training steps: 0.030743264147948715\n",
      "Training loss per 100 training steps: 0.03074975455165603\n",
      "Training loss per 100 training steps: 0.030654075031372616\n",
      "Training loss per 100 training steps: 0.031010925810975153\n",
      "Training loss per 100 training steps: 0.03105178654530483\n",
      "Training loss per 100 training steps: 0.03091456719546877\n",
      "Training loss per 100 training steps: 0.03079578131261782\n",
      "Training loss per 100 training steps: 0.031025083363624063\n",
      "Training loss per 100 training steps: 0.030996447771414182\n",
      "Training loss per 100 training steps: 0.03085887264134483\n",
      "Training loss per 100 training steps: 0.03061586050290923\n",
      "Training loss per 100 training steps: 0.0307802408600967\n",
      "Training loss per 100 training steps: 0.030856392390817163\n",
      "Training loss per 100 training steps: 0.030890112853565778\n",
      "Training loss per 100 training steps: 0.030907286324838548\n",
      "Training loss per 100 training steps: 0.030812241499463447\n",
      "Training loss per 100 training steps: 0.030917745707201984\n",
      "Training loss per 100 training steps: 0.030903331181815204\n",
      "Training loss per 100 training steps: 0.030937697114951812\n",
      "Training loss per 100 training steps: 0.030854550901658468\n",
      "Training loss per 100 training steps: 0.03097389594436631\n",
      "Training loss per 100 training steps: 0.030923868912138223\n",
      "Training loss per 100 training steps: 0.030885107175897927\n",
      "Training loss per 100 training steps: 0.030945636994872853\n",
      "Training loss per 100 training steps: 0.031137434045035526\n",
      "Training loss per 100 training steps: 0.031004474171307618\n",
      "Training loss per 100 training steps: 0.030990099665171123\n",
      "Training loss per 100 training steps: 0.030958181617130154\n",
      "Training loss per 100 training steps: 0.030801912536743404\n",
      "Training loss per 100 training steps: 0.030696958548713685\n",
      "Training loss per 100 training steps: 0.03064560680097317\n",
      "Training loss per 100 training steps: 0.030680258970768842\n",
      "Training loss per 100 training steps: 0.030734050452962094\n",
      "Training loss per 100 training steps: 0.03087726767830623\n",
      "Training loss per 100 training steps: 0.030858329350645017\n",
      "Training loss per 100 training steps: 0.030846367698287358\n",
      "Training loss per 100 training steps: 0.030813724030806084\n",
      "Training loss per 100 training steps: 0.030969216497592125\n",
      "Training loss per 100 training steps: 0.0310778876958946\n",
      "Training loss per 100 training steps: 0.031095167554420294\n",
      "Training loss per 100 training steps: 0.031166310245723465\n",
      "Training loss per 100 training steps: 0.031032939002828802\n",
      "Training loss per 100 training steps: 0.030915497753272847\n",
      "Training loss per 100 training steps: 0.03088683723220948\n",
      "Training loss per 100 training steps: 0.030957099175315854\n",
      "Training loss epoch: 0.030965461330885458\n",
      "Training accuracy epoch: 0.9900454609653574\n",
      "Training epoch: 22\n",
      "Training loss per 100 training steps: 0.028923960402607918\n",
      "Training loss per 100 training steps: 0.028450617001917543\n",
      "Training loss per 100 training steps: 0.027600732475023402\n",
      "Training loss per 100 training steps: 0.02664880356805529\n",
      "Training loss per 100 training steps: 0.02759850436956285\n",
      "Training loss per 100 training steps: 0.02793348340053639\n",
      "Training loss per 100 training steps: 0.027446338105148724\n",
      "Training loss per 100 training steps: 0.027543714406007315\n",
      "Training loss per 100 training steps: 0.027122247853832604\n",
      "Training loss per 100 training steps: 0.026758850473971557\n",
      "Training loss per 100 training steps: 0.026871000377035194\n",
      "Training loss per 100 training steps: 0.026884222928676534\n",
      "Training loss per 100 training steps: 0.027225522119528632\n",
      "Training loss per 100 training steps: 0.027060225656074254\n",
      "Training loss per 100 training steps: 0.02743884037912881\n",
      "Training loss per 100 training steps: 0.02714005464833443\n",
      "Training loss per 100 training steps: 0.02720251262586722\n",
      "Training loss per 100 training steps: 0.026871500724699647\n",
      "Training loss per 100 training steps: 0.027197406599798427\n",
      "Training loss per 100 training steps: 0.02697335872703013\n",
      "Training loss per 100 training steps: 0.026979326822577154\n",
      "Training loss per 100 training steps: 0.02697052522764135\n",
      "Training loss per 100 training steps: 0.026837137685646505\n",
      "Training loss per 100 training steps: 0.02691408691542067\n",
      "Training loss per 100 training steps: 0.026856330114481557\n",
      "Training loss per 100 training steps: 0.026899922615811242\n",
      "Training loss per 100 training steps: 0.026839247808801724\n",
      "Training loss per 100 training steps: 0.02692676220550031\n",
      "Training loss per 100 training steps: 0.026960315072831857\n",
      "Training loss per 100 training steps: 0.026838167054467583\n",
      "Training loss per 100 training steps: 0.026798454827313415\n",
      "Training loss per 100 training steps: 0.026805176074954182\n",
      "Training loss per 100 training steps: 0.026885729158823504\n",
      "Training loss per 100 training steps: 0.0269081965174895\n",
      "Training loss per 100 training steps: 0.02688143931118943\n",
      "Training loss per 100 training steps: 0.02682765465845843\n",
      "Training loss per 100 training steps: 0.026824856784717913\n",
      "Training loss per 100 training steps: 0.026967034817578145\n",
      "Training loss per 100 training steps: 0.026831644287099388\n",
      "Training loss per 100 training steps: 0.02682433131776279\n",
      "Training loss per 100 training steps: 0.026798131213208856\n",
      "Training loss per 100 training steps: 0.02676930160643951\n",
      "Training loss per 100 training steps: 0.02671593571672508\n",
      "Training loss per 100 training steps: 0.026824505399026203\n",
      "Training loss per 100 training steps: 0.026783091739933495\n",
      "Training loss per 100 training steps: 0.02678833781060862\n",
      "Training loss per 100 training steps: 0.026790038958908732\n",
      "Training loss per 100 training steps: 0.02675899323922689\n",
      "Training loss per 100 training steps: 0.02677840823878192\n",
      "Training loss per 100 training steps: 0.026745740155930467\n",
      "Training loss per 100 training steps: 0.026684628722075502\n",
      "Training loss per 100 training steps: 0.02669314543894941\n",
      "Training loss per 100 training steps: 0.026675841160574035\n",
      "Training loss per 100 training steps: 0.026641096556858312\n",
      "Training loss per 100 training steps: 0.02662513404773149\n",
      "Training loss per 100 training steps: 0.026578538040528517\n",
      "Training loss per 100 training steps: 0.026570109044752634\n",
      "Training loss per 100 training steps: 0.026492870971819814\n",
      "Training loss per 100 training steps: 0.026542013144727674\n",
      "Training loss per 100 training steps: 0.026549270787212646\n",
      "Training loss per 100 training steps: 0.026518910007766336\n",
      "Training loss per 100 training steps: 0.026510118959773298\n",
      "Training loss per 100 training steps: 0.0265360465833748\n",
      "Training loss epoch: 0.02652623568362429\n",
      "Training accuracy epoch: 0.9914591407843594\n",
      "Training epoch: 23\n",
      "Training loss per 100 training steps: 0.022657835856080055\n",
      "Training loss per 100 training steps: 0.02212776619558324\n",
      "Training loss per 100 training steps: 0.02491503790546839\n",
      "Training loss per 100 training steps: 0.025613643256467814\n",
      "Training loss per 100 training steps: 0.025032984468205705\n",
      "Training loss per 100 training steps: 0.02542205300031531\n",
      "Training loss per 100 training steps: 0.025030749842420704\n",
      "Training loss per 100 training steps: 0.02494411033026199\n",
      "Training loss per 100 training steps: 0.024471484380018445\n",
      "Training loss per 100 training steps: 0.02438314449903633\n",
      "Training loss per 100 training steps: 0.024771877001253675\n",
      "Training loss per 100 training steps: 0.02468431010723053\n",
      "Training loss per 100 training steps: 0.02458311229685227\n",
      "Training loss per 100 training steps: 0.024189994767806467\n",
      "Training loss per 100 training steps: 0.023795724776457146\n",
      "Training loss per 100 training steps: 0.024091396762103433\n",
      "Training loss per 100 training steps: 0.02387519536673997\n",
      "Training loss per 100 training steps: 0.023894455227691534\n",
      "Training loss per 100 training steps: 0.023744061934867194\n",
      "Training loss per 100 training steps: 0.023537794420997054\n",
      "Training loss per 100 training steps: 0.02336100615262087\n",
      "Training loss per 100 training steps: 0.023278938061346045\n",
      "Training loss per 100 training steps: 0.023112154488555657\n",
      "Training loss per 100 training steps: 0.023143519026765407\n",
      "Training loss per 100 training steps: 0.023154724391326827\n",
      "Training loss per 100 training steps: 0.023227652951698714\n",
      "Training loss per 100 training steps: 0.02317479375082665\n",
      "Training loss per 100 training steps: 0.02318112486704456\n",
      "Training loss per 100 training steps: 0.023345195436171067\n",
      "Training loss per 100 training steps: 0.02322535847634095\n",
      "Training loss per 100 training steps: 0.023277084606728195\n",
      "Training loss per 100 training steps: 0.023334942510311294\n",
      "Training loss per 100 training steps: 0.023187426155199135\n",
      "Training loss per 100 training steps: 0.02314972358959644\n",
      "Training loss per 100 training steps: 0.023116214092320377\n",
      "Training loss per 100 training steps: 0.02302982851625521\n",
      "Training loss per 100 training steps: 0.02299408764489078\n",
      "Training loss per 100 training steps: 0.02296698751004386\n",
      "Training loss per 100 training steps: 0.022897833950818014\n",
      "Training loss per 100 training steps: 0.022828243815875057\n",
      "Training loss per 100 training steps: 0.02276916571774986\n",
      "Training loss per 100 training steps: 0.022838259434587622\n",
      "Training loss per 100 training steps: 0.022766154436726554\n",
      "Training loss per 100 training steps: 0.0226945421795605\n",
      "Training loss per 100 training steps: 0.022648807350561254\n",
      "Training loss per 100 training steps: 0.02259131801882167\n",
      "Training loss per 100 training steps: 0.02262016367129534\n",
      "Training loss per 100 training steps: 0.022587020341013224\n",
      "Training loss per 100 training steps: 0.022615221822082938\n",
      "Training loss per 100 training steps: 0.022609839050277972\n",
      "Training loss per 100 training steps: 0.02267376777180205\n",
      "Training loss per 100 training steps: 0.022630673725528332\n",
      "Training loss per 100 training steps: 0.022738361395154148\n",
      "Training loss per 100 training steps: 0.02281295005341242\n",
      "Training loss per 100 training steps: 0.022851071799768596\n",
      "Training loss per 100 training steps: 0.02284316970549473\n",
      "Training loss per 100 training steps: 0.022857715231464976\n",
      "Training loss per 100 training steps: 0.02284438975849108\n",
      "Training loss per 100 training steps: 0.022800041965400076\n",
      "Training loss per 100 training steps: 0.02277850600016428\n",
      "Training loss per 100 training steps: 0.022727549952949004\n",
      "Training loss per 100 training steps: 0.022798865240265996\n",
      "Training loss per 100 training steps: 0.02283483584403332\n",
      "Training loss epoch: 0.022873486759343087\n",
      "Training accuracy epoch: 0.9926856412583275\n",
      "Training epoch: 24\n",
      "Training loss per 100 training steps: 0.013186394236981869\n",
      "Training loss per 100 training steps: 0.02118525422860564\n",
      "Training loss per 100 training steps: 0.018484279168167368\n",
      "Training loss per 100 training steps: 0.019395718615551623\n",
      "Training loss per 100 training steps: 0.019806026423319783\n",
      "Training loss per 100 training steps: 0.019933036915881712\n",
      "Training loss per 100 training steps: 0.019828833295605742\n",
      "Training loss per 100 training steps: 0.019967410329645618\n",
      "Training loss per 100 training steps: 0.01982465926835374\n",
      "Training loss per 100 training steps: 0.019715587995296754\n",
      "Training loss per 100 training steps: 0.019503304501937296\n",
      "Training loss per 100 training steps: 0.019114525781270434\n",
      "Training loss per 100 training steps: 0.019225020807060598\n",
      "Training loss per 100 training steps: 0.019719506431301\n",
      "Training loss per 100 training steps: 0.019677852511669022\n",
      "Training loss per 100 training steps: 0.01966179931145965\n",
      "Training loss per 100 training steps: 0.019706059194228544\n",
      "Training loss per 100 training steps: 0.019595350979192638\n",
      "Training loss per 100 training steps: 0.019623754923135586\n",
      "Training loss per 100 training steps: 0.01974668879577141\n",
      "Training loss per 100 training steps: 0.019639810949610172\n",
      "Training loss per 100 training steps: 0.01959963498773886\n",
      "Training loss per 100 training steps: 0.019631757820376818\n",
      "Training loss per 100 training steps: 0.019588680919685277\n",
      "Training loss per 100 training steps: 0.01951221285153152\n",
      "Training loss per 100 training steps: 0.01961327272769455\n",
      "Training loss per 100 training steps: 0.01969937473091405\n",
      "Training loss per 100 training steps: 0.019710521294535127\n",
      "Training loss per 100 training steps: 0.019786482505409795\n",
      "Training loss per 100 training steps: 0.01978508985513041\n",
      "Training loss per 100 training steps: 0.01986144585208882\n",
      "Training loss per 100 training steps: 0.01972866488056732\n",
      "Training loss per 100 training steps: 0.019752983501729368\n",
      "Training loss per 100 training steps: 0.01971547731613424\n",
      "Training loss per 100 training steps: 0.01970168241847262\n",
      "Training loss per 100 training steps: 0.019659712750485493\n",
      "Training loss per 100 training steps: 0.01965411501800091\n",
      "Training loss per 100 training steps: 0.019618491318795096\n",
      "Training loss per 100 training steps: 0.019617134448718435\n",
      "Training loss per 100 training steps: 0.019746987757613996\n",
      "Training loss per 100 training steps: 0.019811744565501544\n",
      "Training loss per 100 training steps: 0.019722071396690476\n",
      "Training loss per 100 training steps: 0.019719162317542616\n",
      "Training loss per 100 training steps: 0.019646091446636087\n",
      "Training loss per 100 training steps: 0.019632223835733698\n",
      "Training loss per 100 training steps: 0.019596440316609787\n",
      "Training loss per 100 training steps: 0.019700613532586946\n",
      "Training loss per 100 training steps: 0.019727117204822477\n",
      "Training loss per 100 training steps: 0.019711121590021183\n",
      "Training loss per 100 training steps: 0.019713402850788028\n",
      "Training loss per 100 training steps: 0.019706601132708355\n",
      "Training loss per 100 training steps: 0.019669731828072627\n",
      "Training loss per 100 training steps: 0.01967949087940958\n",
      "Training loss per 100 training steps: 0.019705943287822134\n",
      "Training loss per 100 training steps: 0.019722408337712216\n",
      "Training loss per 100 training steps: 0.01978703627290662\n",
      "Training loss per 100 training steps: 0.01977216095925266\n",
      "Training loss per 100 training steps: 0.019825077580994672\n",
      "Training loss per 100 training steps: 0.019774416711485554\n",
      "Training loss per 100 training steps: 0.019790884677719306\n",
      "Training loss per 100 training steps: 0.019825555471504528\n",
      "Training loss per 100 training steps: 0.01977071006547996\n",
      "Training loss per 100 training steps: 0.019718737929650995\n",
      "Training loss epoch: 0.019681806978008178\n",
      "Training accuracy epoch: 0.9936699516079324\n",
      "Training epoch: 25\n",
      "Training loss per 100 training steps: 0.00568087724968791\n",
      "Training loss per 100 training steps: 0.01592490553745244\n",
      "Training loss per 100 training steps: 0.01614433211802658\n",
      "Training loss per 100 training steps: 0.017073599136654176\n",
      "Training loss per 100 training steps: 0.018294210134345824\n",
      "Training loss per 100 training steps: 0.018913103446936953\n",
      "Training loss per 100 training steps: 0.01875150849441835\n",
      "Training loss per 100 training steps: 0.018628335852237728\n",
      "Training loss per 100 training steps: 0.018513222043120538\n",
      "Training loss per 100 training steps: 0.0183722431185892\n",
      "Training loss per 100 training steps: 0.018104648313621855\n",
      "Training loss per 100 training steps: 0.017929282313683067\n",
      "Training loss per 100 training steps: 0.017838667369223543\n",
      "Training loss per 100 training steps: 0.017791706525641375\n",
      "Training loss per 100 training steps: 0.017793905075815148\n",
      "Training loss per 100 training steps: 0.01780656144070795\n",
      "Training loss per 100 training steps: 0.017675107726679653\n",
      "Training loss per 100 training steps: 0.017603176661282104\n",
      "Training loss per 100 training steps: 0.017554677050185933\n",
      "Training loss per 100 training steps: 0.017555736177158162\n",
      "Training loss per 100 training steps: 0.017445299537926992\n",
      "Training loss per 100 training steps: 0.017571914475258802\n",
      "Training loss per 100 training steps: 0.017585371762240874\n",
      "Training loss per 100 training steps: 0.01761864517276946\n",
      "Training loss per 100 training steps: 0.017468864475571333\n",
      "Training loss per 100 training steps: 0.017426733812812983\n",
      "Training loss per 100 training steps: 0.01743821953593625\n",
      "Training loss per 100 training steps: 0.017419065244209078\n",
      "Training loss per 100 training steps: 0.017460375911093517\n",
      "Training loss per 100 training steps: 0.017440073988386755\n",
      "Training loss per 100 training steps: 0.017298348339438073\n",
      "Training loss per 100 training steps: 0.01728205127198272\n",
      "Training loss per 100 training steps: 0.017329346812525123\n",
      "Training loss per 100 training steps: 0.017316187102164506\n",
      "Training loss per 100 training steps: 0.017398301194333908\n",
      "Training loss per 100 training steps: 0.017356642232284412\n",
      "Training loss per 100 training steps: 0.01736528339545836\n",
      "Training loss per 100 training steps: 0.01733619651993377\n",
      "Training loss per 100 training steps: 0.01733211465648675\n",
      "Training loss per 100 training steps: 0.017298790180210348\n",
      "Training loss per 100 training steps: 0.017271489891158957\n",
      "Training loss per 100 training steps: 0.017323653111266386\n",
      "Training loss per 100 training steps: 0.0172961368235006\n",
      "Training loss per 100 training steps: 0.017288735223971893\n",
      "Training loss per 100 training steps: 0.01727080573209202\n",
      "Training loss per 100 training steps: 0.01723882397808381\n",
      "Training loss per 100 training steps: 0.017204043948820726\n",
      "Training loss per 100 training steps: 0.01720313559607559\n",
      "Training loss per 100 training steps: 0.01721764568377689\n",
      "Training loss per 100 training steps: 0.017199869995071986\n",
      "Training loss per 100 training steps: 0.017257324717023827\n",
      "Training loss per 100 training steps: 0.017217391316605372\n",
      "Training loss per 100 training steps: 0.017217097468889733\n",
      "Training loss per 100 training steps: 0.017161729865007296\n",
      "Training loss per 100 training steps: 0.017107530149589906\n",
      "Training loss per 100 training steps: 0.017137100404629603\n",
      "Training loss per 100 training steps: 0.017205138249701928\n",
      "Training loss per 100 training steps: 0.017231098441523277\n",
      "Training loss per 100 training steps: 0.017215697780777528\n",
      "Training loss per 100 training steps: 0.01718990497984806\n",
      "Training loss per 100 training steps: 0.01712640459292744\n",
      "Training loss per 100 training steps: 0.017176252683159268\n",
      "Training loss per 100 training steps: 0.017133116769454046\n",
      "Training loss epoch: 0.017124012040543757\n",
      "Training accuracy epoch: 0.9945021155176511\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cd9f8c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T02:09:11.559572Z",
     "iopub.status.busy": "2022-03-21T02:09:11.558748Z",
     "iopub.status.idle": "2022-03-21T02:09:11.570531Z",
     "shell.execute_reply": "2022-03-21T02:09:11.571071Z",
     "shell.execute_reply.started": "2022-03-19T11:01:17.309171Z"
    },
    "papermill": {
     "duration": 1.105717,
     "end_time": "2022-03-21T02:09:11.571224",
     "exception": false,
     "start_time": "2022-03-21T02:09:10.465507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            # after using custom collate_fn in DataLoader, need to update the same\n",
    "            ids = batch['input_ids'].unsqueeze(0)\n",
    "            mask = batch['attention_mask'].unsqueeze(0)\n",
    "            labels = batch['labels'].unsqueeze(0)\n",
    "            \n",
    "            ids = ids.to(device, dtype = torch.long)\n",
    "            mask = mask.to(device, dtype = torch.long)\n",
    "            labels = labels.to(device, dtype = torch.long)\n",
    "            \n",
    "            output = model(input_ids=ids[0], attention_mask=mask[0], labels=labels[0])\n",
    "            loss = output[0]\n",
    "            eval_logits = output[1]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "#             eval_labels.append(labels)\n",
    "#             eval_preds.append(predictions)\n",
    "            \n",
    "            eval_labels.append([idx_to_label[id.item()] for id in labels])\n",
    "            eval_preds.append([idx_to_label[id.item()] for id in predictions])\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#     labels = [idx_to_label[id.item()] for id in eval_labels]\n",
    "#     predictions = [idx_to_label[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "#     return labels, predictions\n",
    "    return eval_labels, eval_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a133e1a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-21T02:09:12.799461Z",
     "iopub.status.busy": "2022-03-21T02:09:12.798626Z",
     "iopub.status.idle": "2022-03-21T02:11:53.511983Z",
     "shell.execute_reply": "2022-03-21T02:11:53.511265Z",
     "shell.execute_reply.started": "2022-03-19T11:01:24.119691Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 161.250749,
     "end_time": "2022-03-21T02:11:53.512155",
     "exception": false,
     "start_time": "2022-03-21T02:09:12.261406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 1.6496251821517944\n",
      "Validation loss per 100 evaluation steps: 1.677530144995982\n",
      "Validation loss per 100 evaluation steps: 1.8091944140284808\n",
      "Validation loss per 100 evaluation steps: 1.7915710346445293\n",
      "Validation loss per 100 evaluation steps: 1.8071055493301287\n",
      "Validation loss per 100 evaluation steps: 1.843200724579379\n",
      "Validation loss per 100 evaluation steps: 1.8507944083650179\n",
      "Validation loss per 100 evaluation steps: 1.8372127332973072\n",
      "Validation loss per 100 evaluation steps: 1.8490664685672886\n",
      "Validation loss per 100 evaluation steps: 1.8467704485212924\n",
      "Validation loss per 100 evaluation steps: 1.8401574868035244\n",
      "Validation loss per 100 evaluation steps: 1.8429577501928882\n",
      "Validation loss per 100 evaluation steps: 1.8436938303396366\n",
      "Validation loss per 100 evaluation steps: 1.8402466138081768\n",
      "Validation loss per 100 evaluation steps: 1.8295785128900104\n",
      "Validation loss per 100 evaluation steps: 1.8260407674012782\n",
      "Validation Loss: 1.8236450133272089\n",
      "Validation Accuracy: 0.7458847235069325\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6557a9d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T02:11:54.540705Z",
     "iopub.status.busy": "2022-03-21T02:11:54.540169Z",
     "iopub.status.idle": "2022-03-21T02:12:24.234990Z",
     "shell.execute_reply": "2022-03-21T02:12:24.235516Z",
     "shell.execute_reply.started": "2022-03-19T11:02:06.935282Z"
    },
    "papermill": {
     "duration": 30.211279,
     "end_time": "2022-03-21T02:12:24.235663",
     "exception": false,
     "start_time": "2022-03-21T02:11:54.024384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Claim       0.30      0.40      0.34      9890\n",
      "Concluding Statement       0.32      0.49      0.39      2679\n",
      "        Counterclaim       0.19      0.27      0.22      1168\n",
      "            Evidence       0.17      0.24      0.20      9059\n",
      "                Lead       0.42      0.54      0.47      1837\n",
      "            Position       0.37      0.48      0.42      3080\n",
      "            Rebuttal       0.12      0.18      0.14       866\n",
      "\n",
      "           micro avg       0.26      0.36      0.30     28579\n",
      "           macro avg       0.27      0.37      0.31     28579\n",
      "        weighted avg       0.26      0.36      0.31     28579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b604e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T02:12:25.267138Z",
     "iopub.status.busy": "2022-03-21T02:12:25.266517Z",
     "iopub.status.idle": "2022-03-21T02:12:26.009135Z",
     "shell.execute_reply": "2022-03-21T02:12:26.008154Z",
     "shell.execute_reply.started": "2022-03-19T11:02:06.944784Z"
    },
    "papermill": {
     "duration": 1.263072,
     "end_time": "2022-03-21T02:12:26.009290",
     "exception": false,
     "start_time": "2022-03-21T02:12:24.746218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dirname = \"/kaggle/working/feedback-bert-uncased-model3/\"\n",
    "if not os.path.isdir(dirname):\n",
    "    os.makedirs(dirname)\n",
    "    \n",
    "model.save_pretrained(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01623253",
   "metadata": {
    "papermill": {
     "duration": 0.531309,
     "end_time": "2022-03-21T02:12:27.094188",
     "exception": false,
     "start_time": "2022-03-21T02:12:26.562879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33596.732843,
   "end_time": "2022-03-21T02:12:31.187224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-20T16:52:34.454381",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "040febb2e3914bf999c92cb8f9366c09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_28669b7fe92c410397bf6e4be004c71e",
       "max": 466062.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_820adccb6b3e4123b310bb61144d6b4e",
       "value": 466062.0
      }
     },
     "0c1895e5c8434ea3bfd2f5738e6e7912": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5b0ad59c292e48459413584462068c95",
       "placeholder": "",
       "style": "IPY_MODEL_15df7fbb0cea4d87ae134b7e7495baa7",
       "value": " 226k/226k [00:00&lt;00:00, 364kB/s]"
      }
     },
     "15df7fbb0cea4d87ae134b7e7495baa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "16b9c0022fc941d3b7de187bb5f7874f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "184adddd518d48f4ba37935736b3fe99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_706e2bee70fb4b2dac0a7a37fea29dc7",
        "IPY_MODEL_f148877174ce4e1db468b79f1544384f",
        "IPY_MODEL_0c1895e5c8434ea3bfd2f5738e6e7912"
       ],
       "layout": "IPY_MODEL_1ef0e97d9da244608d0d3d437af771b7"
      }
     },
     "1ef0e97d9da244608d0d3d437af771b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "22c06c5f2a7f435597b1463731327a55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2397772e9ed14360965a17306a8b279d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2600ba87fdd44d2eaf9a5f25d069d0d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "262403ae3e2d4b88a65a439e8285acba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28669b7fe92c410397bf6e4be004c71e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a7d0faf787940dfbc355c9dc539260d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a9a1adfa57d46d492fba567f6ab3e04": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3bdfef0c01bd45dbb1fc7c06d209f364": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4495fe39360246d0bb9992aac56739ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_262403ae3e2d4b88a65a439e8285acba",
       "max": 28.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4e3e294703e04faa9410614f6eac4ae2",
       "value": 28.0
      }
     },
     "457c10f67db643b2a1e3f01c43824ce6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "46706cecf549440480f01e78484df1d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46a4f976ca734a9398a4f9d394737361": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f669d62f8bf147fbae3227394a5f200d",
       "placeholder": "",
       "style": "IPY_MODEL_b1ee09bc253945a99b4e8596821138b5",
       "value": " 28.0/28.0 [00:00&lt;00:00, 1.10kB/s]"
      }
     },
     "4e3e294703e04faa9410614f6eac4ae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4fe27045804849b7b04ae38a753bdd0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54d434500d7e4ef0865789a4e5a16564": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b0ad59c292e48459413584462068c95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6daa7bd2ebf6478694d4ecfa24a39e82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ea51cc0dc28d43928fd3d10aa09c6ec5",
        "IPY_MODEL_040febb2e3914bf999c92cb8f9366c09",
        "IPY_MODEL_cde2bd23f1114511a6df0037ceb6f1f1"
       ],
       "layout": "IPY_MODEL_ffd1037ba2c8436bacef48dd21066066"
      }
     },
     "6ec973508bf7420d879ba0c60eb2e9a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_54d434500d7e4ef0865789a4e5a16564",
       "max": 440473133.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bec7cbd4b27f48ca948771a3bff5caa4",
       "value": 440473133.0
      }
     },
     "706e2bee70fb4b2dac0a7a37fea29dc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a7d0faf787940dfbc355c9dc539260d",
       "placeholder": "",
       "style": "IPY_MODEL_2600ba87fdd44d2eaf9a5f25d069d0d7",
       "value": "Downloading: 100%"
      }
     },
     "73f954150e4c4296b687ef0a79f7e510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "79195ed3a849494e954fb90a0b2ac2bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "820adccb6b3e4123b310bb61144d6b4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "840942657e08478ba87fb22c909da822": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85f509c7f515488d82f21732551dfbe2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8d413091b8624f6d83d69519fc9aa5bb",
        "IPY_MODEL_6ec973508bf7420d879ba0c60eb2e9a9",
        "IPY_MODEL_cd7a0982bd5e465d907c26312f2fd9bc"
       ],
       "layout": "IPY_MODEL_840942657e08478ba87fb22c909da822"
      }
     },
     "865fc4f76416483a87dc04425375f64d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "880c65f61d794abc94e6b3db0709f298": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8d413091b8624f6d83d69519fc9aa5bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2397772e9ed14360965a17306a8b279d",
       "placeholder": "",
       "style": "IPY_MODEL_880c65f61d794abc94e6b3db0709f298",
       "value": "Downloading: 100%"
      }
     },
     "9adcb4bc18fa45a7b5070cab80aa6806": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "afef30be51cc453985281e761db25259": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fcee9f30b0e04f08a89e6781dc98847f",
        "IPY_MODEL_4495fe39360246d0bb9992aac56739ea",
        "IPY_MODEL_46a4f976ca734a9398a4f9d394737361"
       ],
       "layout": "IPY_MODEL_c0ba360c62844725b1dbdedf5450abfc"
      }
     },
     "b1ee09bc253945a99b4e8596821138b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bcf84a7330a04584b3b52eac55e378b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3bdfef0c01bd45dbb1fc7c06d209f364",
       "placeholder": "",
       "style": "IPY_MODEL_457c10f67db643b2a1e3f01c43824ce6",
       "value": " 570/570 [00:00&lt;00:00, 15.3kB/s]"
      }
     },
     "bec7cbd4b27f48ca948771a3bff5caa4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c0ba360c62844725b1dbdedf5450abfc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c4d8131c93384d2a9620ff1fb644c484": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c9e699e9df334394bc88d036debdb3ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cf0b381f8b9a4402900da1d698ef0d07",
        "IPY_MODEL_f3695f255e8248619f3dacd9ca936408",
        "IPY_MODEL_bcf84a7330a04584b3b52eac55e378b0"
       ],
       "layout": "IPY_MODEL_dc71e5bd46f34633aeecad30f6a591cf"
      }
     },
     "cd7a0982bd5e465d907c26312f2fd9bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_46706cecf549440480f01e78484df1d8",
       "placeholder": "",
       "style": "IPY_MODEL_9adcb4bc18fa45a7b5070cab80aa6806",
       "value": " 420M/420M [00:25&lt;00:00, 17.7MB/s]"
      }
     },
     "cde2bd23f1114511a6df0037ceb6f1f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c4d8131c93384d2a9620ff1fb644c484",
       "placeholder": "",
       "style": "IPY_MODEL_dadd3110f582483e8f373c7e79a53d6d",
       "value": " 455k/455k [00:00&lt;00:00, 649kB/s]"
      }
     },
     "cf0b381f8b9a4402900da1d698ef0d07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ffa3b1e707dd4307b47c4517fc915acc",
       "placeholder": "",
       "style": "IPY_MODEL_22c06c5f2a7f435597b1463731327a55",
       "value": "Downloading: 100%"
      }
     },
     "d70c9618f2634922970750e6353a5c61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dadd3110f582483e8f373c7e79a53d6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dc71e5bd46f34633aeecad30f6a591cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea51cc0dc28d43928fd3d10aa09c6ec5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_79195ed3a849494e954fb90a0b2ac2bb",
       "placeholder": "",
       "style": "IPY_MODEL_73f954150e4c4296b687ef0a79f7e510",
       "value": "Downloading: 100%"
      }
     },
     "f148877174ce4e1db468b79f1544384f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16b9c0022fc941d3b7de187bb5f7874f",
       "max": 231508.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fd703390607449fe8bc6c0bc8a6f52de",
       "value": 231508.0
      }
     },
     "f3695f255e8248619f3dacd9ca936408": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4fe27045804849b7b04ae38a753bdd0d",
       "max": 570.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_865fc4f76416483a87dc04425375f64d",
       "value": 570.0
      }
     },
     "f669d62f8bf147fbae3227394a5f200d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fcee9f30b0e04f08a89e6781dc98847f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a9a1adfa57d46d492fba567f6ab3e04",
       "placeholder": "",
       "style": "IPY_MODEL_d70c9618f2634922970750e6353a5c61",
       "value": "Downloading: 100%"
      }
     },
     "fd703390607449fe8bc6c0bc8a6f52de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ffa3b1e707dd4307b47c4517fc915acc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ffd1037ba2c8436bacef48dd21066066": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
