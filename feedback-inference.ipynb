{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-31T07:29:06.671814Z","iopub.execute_input":"2022-01-31T07:29:06.672460Z","iopub.status.idle":"2022-01-31T07:29:06.694708Z","shell.execute_reply.started":"2022-01-31T07:29:06.672367Z","shell.execute_reply":"2022-01-31T07:29:06.694111Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Inference on the Test files**","metadata":{}},{"cell_type":"markdown","source":"## **Load the Model**\n- Load the pre_trained model.\n- Class labels","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast, BertConfig, BertForTokenClassification","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:29:13.438493Z","iopub.execute_input":"2022-01-31T07:29:13.438829Z","iopub.status.idle":"2022-01-31T07:29:21.506489Z","shell.execute_reply.started":"2022-01-31T07:29:13.438789Z","shell.execute_reply":"2022-01-31T07:29:21.505753Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:29:21.508432Z","iopub.execute_input":"2022-01-31T07:29:21.508696Z","iopub.status.idle":"2022-01-31T07:29:21.514831Z","shell.execute_reply.started":"2022-01-31T07:29:21.508664Z","shell.execute_reply":"2022-01-31T07:29:21.514103Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"import pprint as pprint\nlabel_to_idx = {'B-Claim': 0, 'I-Claim': 1,\n                'B-Evidence': 2, 'I-Evidence': 3,\n                'B-Position': 4, 'I-Position': 5,\n                'B-Concluding Statement': 6, 'I-Concluding Statement': 7,\n                'B-Lead': 8, 'I-Lead': 9,\n                'B-Counterclaim': 10, 'I-Counterclaim': 11,\n                'B-Rebuttal': 12, 'I-Rebuttal': 13,\n                'O': 14}\nidx_to_label = {v:k for k,v in label_to_idx.items()}\nidx_to_label","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:29:21.516000Z","iopub.execute_input":"2022-01-31T07:29:21.516514Z","iopub.status.idle":"2022-01-31T07:29:21.540950Z","shell.execute_reply.started":"2022-01-31T07:29:21.516479Z","shell.execute_reply":"2022-01-31T07:29:21.540084Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{0: 'B-Claim',\n 1: 'I-Claim',\n 2: 'B-Evidence',\n 3: 'I-Evidence',\n 4: 'B-Position',\n 5: 'I-Position',\n 6: 'B-Concluding Statement',\n 7: 'I-Concluding Statement',\n 8: 'B-Lead',\n 9: 'I-Lead',\n 10: 'B-Counterclaim',\n 11: 'I-Counterclaim',\n 12: 'B-Rebuttal',\n 13: 'I-Rebuttal',\n 14: 'O'}"},"metadata":{}}]},{"cell_type":"code","source":"MAX_LEN = 512\ntokenizer = BertTokenizerFast.from_pretrained('../input/feedback-nb3/bert-base-uncased-tokenizer')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:29:43.954151Z","iopub.execute_input":"2022-01-31T07:29:43.954408Z","iopub.status.idle":"2022-01-31T07:29:44.034166Z","shell.execute_reply.started":"2022-01-31T07:29:43.954380Z","shell.execute_reply":"2022-01-31T07:29:44.033519Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('../input/feedback-nb3/feedback-bert-uncased-model1')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:30:12.009704Z","iopub.execute_input":"2022-01-31T07:30:12.010196Z","iopub.status.idle":"2022-01-31T07:30:18.168538Z","shell.execute_reply.started":"2022-01-31T07:30:12.010143Z","shell.execute_reply":"2022-01-31T07:30:18.167770Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=15, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def model_pred(content):\n    # given a content, get the predictions from the model\n    # return encoding and pred_logits\n    sentence = content.strip().split()\n    encoding = tokenizer(sentence,\n                         is_split_into_words=True,\n                         return_offsets_mapping=True, \n                         padding='max_length', \n                         truncation=True,\n                         max_length=MAX_LEN,\n                         return_tensors=\"pt\")\n\n    input_ids = encoding[\"input_ids\"].to(device)\n    attention_mask = encoding[\"attention_mask\"].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask)\n    tr_logits = outputs[0]\n    tr_logits_mod = torch.argmax(tr_logits, axis=2)\n    \n    # map sub_words to words\n    tokens_words = [-1] * MAX_LEN\n\n    w_i = 0\n    off = encoding['offset_mapping'][0]\n    for itr, off_map in enumerate(off):\n        if off_map[0] == 0 and off_map[1] != 0:\n            w_i += 1\n            tokens_words[itr] = w_i\n        elif off_map[0] != 0 and off_map[1] != 0:\n            tokens_words[itr] = w_i\n            \n\n    return tr_logits_mod, tokens_words","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:30:36.648737Z","iopub.execute_input":"2022-01-31T07:30:36.649011Z","iopub.status.idle":"2022-01-31T07:30:36.657617Z","shell.execute_reply.started":"2022-01-31T07:30:36.648982Z","shell.execute_reply":"2022-01-31T07:30:36.657006Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_predictions(tr_logits_mod, tokens_words):\n\n    all_predictions = {}\n    pred = tr_logits_mod[0]\n    i = 0\n    while i<MAX_LEN:\n        prediction = []\n        start = pred[i]\n        if start in [0,2,4,6,8,10,12,14]:\n            label = idx_to_label[start.item()]\n            label_mod = label.replace(\"B-\", \"\")\n            prediction.append(tokens_words[i])\n            i += 1\n            if i>=MAX_LEN: break\n            while pred[i]==start+1:\n                # if we have \"I-\" after \"B-\" \n                if not tokens_words[i] in prediction:\n                    prediction.append(tokens_words[i])\n                i += 1\n                if i>=MAX_LEN: break\n        else:\n            i += 1\n        prediction = [x for x in prediction if x!=-1]\n        if len(prediction)>4:\n            if label_mod not in all_predictions:\n                all_predictions[label_mod] = [' '.join([str(x) for x in prediction])]\n            else:\n                all_predictions[label_mod].append(' '.join([str(x) for x in prediction]))\n                \n    return all_predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:30:37.541354Z","iopub.execute_input":"2022-01-31T07:30:37.541639Z","iopub.status.idle":"2022-01-31T07:30:37.552358Z","shell.execute_reply.started":"2022-01-31T07:30:37.541606Z","shell.execute_reply":"2022-01-31T07:30:37.551125Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"row_df = [] # list will have the rows for submission_df\nfolder = \"../input/feedback-prize-2021/test\"\nfor file_name in os.listdir(folder):\n    if file_name.endswith(\".txt\"):\n        with open(os.path.join(folder, file_name), \"r\") as fin:\n            content = fin.read()\n            \n    # get the predictions from the model\n    tr_logits_mod, tokens_words = model_pred(content)\n    \n    # convert the predictions into the desired format\n    all_predictions = get_predictions(tr_logits_mod, tokens_words)\n    \n    # store the predicitons in the list\n    for label in all_predictions:\n        for pred_str in all_predictions[label]:\n            ind_dct = {}\n            ind_dct['id'] = file_name.replace(\".txt\", \"\")\n            ind_dct['class'] = label\n            ind_dct['predictionstring'] = pred_str\n            row_df.append(ind_dct)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:30:41.045601Z","iopub.execute_input":"2022-01-31T07:30:41.045906Z","iopub.status.idle":"2022-01-31T07:30:46.662589Z","shell.execute_reply.started":"2022-01-31T07:30:41.045870Z","shell.execute_reply":"2022-01-31T07:30:46.661789Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.DataFrame(row_df)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:30:50.597484Z","iopub.execute_input":"2022-01-31T07:30:50.598000Z","iopub.status.idle":"2022-01-31T07:30:50.621344Z","shell.execute_reply.started":"2022-01-31T07:30:50.597965Z","shell.execute_reply":"2022-01-31T07:30:50.620662Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"             id     class                                   predictionstring\n0  0FB0700DAF44      Lead  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...\n1  0FB0700DAF44  Position                               42 43 44 45 46 47 48\n2  0FB0700DAF44     Claim    109 110 111 112 113 114 115 116 117 118 119 120\n3  0FB0700DAF44     Claim  121 122 123 124 125 126 127 128 129 130 131 13...\n4  0FB0700DAF44     Claim  315 316 317 318 319 320 321 322 323 324 325 32...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>class</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0FB0700DAF44</td>\n      <td>Lead</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0FB0700DAF44</td>\n      <td>Position</td>\n      <td>42 43 44 45 46 47 48</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0FB0700DAF44</td>\n      <td>Claim</td>\n      <td>109 110 111 112 113 114 115 116 117 118 119 120</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0FB0700DAF44</td>\n      <td>Claim</td>\n      <td>121 122 123 124 125 126 127 128 129 130 131 13...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0FB0700DAF44</td>\n      <td>Claim</td>\n      <td>315 316 317 318 319 320 321 322 323 324 325 32...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:30:54.668881Z","iopub.execute_input":"2022-01-31T07:30:54.669164Z","iopub.status.idle":"2022-01-31T07:30:54.678881Z","shell.execute_reply.started":"2022-01-31T07:30:54.669131Z","shell.execute_reply":"2022-01-31T07:30:54.678278Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sub_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-31T07:31:04.520286Z","iopub.execute_input":"2022-01-31T07:31:04.520911Z","iopub.status.idle":"2022-01-31T07:31:04.527209Z","shell.execute_reply.started":"2022-01-31T07:31:04.520875Z","shell.execute_reply":"2022-01-31T07:31:04.526560Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(43, 3)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}